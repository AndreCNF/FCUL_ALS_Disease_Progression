{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FCUL ALS Model Interpretability\n",
    "---\n",
    "\n",
    "Exploring the ALS dataset from Faculdade de CiÃªncias da Universidade de Lisboa (FCUL) with the data from over 1000 patients collected in Portugal.\n",
    "\n",
    "Using different interpretability approaches so as to understand the outputs of the models trained on FCUL's ALS dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KOdmFzXqF7nq"
   },
   "source": [
    "## Importing the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G5RrWE9R_Nkl"
   },
   "outputs": [],
   "source": [
    "import pandas as pd              # Pandas to handle the data in dataframes\n",
    "import re                        # re to do regex searches in string data\n",
    "import plotly                    # Plotly for interactive and pretty plots\n",
    "import plotly.graph_objs as go\n",
    "import os                        # os handles directory/workspace changes\n",
    "import numpy as np               # NumPy to handle numeric and NaN operations\n",
    "from tqdm import tqdm_notebook   # tqdm allows to track code execution progress\n",
    "import torch                     # PyTorch to create and apply deep learning models\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import shap                      # Model-agnostic interpretability package inspired on Shapley values\n",
    "import pickle                    # Save python objects in files\n",
    "from datetime import datetime    # datetime to use proper date and time formats\n",
    "import utils                     # Contains auxiliary functions\n",
    "from Time_Series_Dataset import Time_Series_Dataset # Dataset subclass which allows the creation of Dataset objects\n",
    "from ModelInterpreter import ModelInterpreter # Class that enables the interpretation of models that handle variable sequence length input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debugging packages\n",
    "import pixiedust                 # Debugging in Jupyter Notebook cells\n",
    "import numpy as np               # Math operations with NumPy to confirm model's behaviour\n",
    "import time                      # Calculate code execution time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to parent directory (presumably \"Documents\")\n",
    "os.chdir(\"../..\")\n",
    "\n",
    "# Path to the CSV dataset files\n",
    "data_path = 'Datasets/Thesis/FCUL_ALS/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bEqFkmlYCGOz"
   },
   "source": [
    "**Important:** Use the following two lines to be able to do plotly plots offline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fZCUmUOzCPeI"
   },
   "outputs": [],
   "source": [
    "import plotly.offline as py\n",
    "plotly.offline.init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Yrzi8YbzDVTH"
   },
   "source": [
    "**Important:** The following function is needed in every Google Colab cell that contains a Plotly chart:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wxyGCedgC6bX"
   },
   "outputs": [],
   "source": [
    "def configure_plotly_browser_state():\n",
    "    import IPython\n",
    "    display(IPython.core.display.HTML('''\n",
    "        <script src=\"/static/components/requirejs/require.js\"></script>\n",
    "        <script>\n",
    "          requirejs.config({\n",
    "            paths: {\n",
    "              base: '/static/base',\n",
    "              plotly: 'https://cdn.plot.ly/plotly-latest.min.js?noext',\n",
    "            },\n",
    "          });\n",
    "        </script>\n",
    "        '''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed to the specified value\n",
    "np.random.seed(utils.random_seed)\n",
    "torch.manual_seed(utils.random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data (already processed, just like the model trained on)\n",
    "ALS_df = pd.read_csv(f'{data_path}cleaned/FCUL_ALS_cleaned.csv')\n",
    "ALS_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the original data (before normalization)\n",
    "orig_ALS_df = pd.read_csv(f'{data_path}cleaned/FCUL_ALS_cleaned_denorm.csv')\n",
    "orig_ALS_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the unnamed index column\n",
    "ALS_df.drop(columns=['Unnamed: 0', 'niv'], inplace=True)\n",
    "ALS_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_ALS_df.niv_label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the unnamed index and label columns in the original dataframe\n",
    "orig_ALS_df.drop(columns=['Unnamed: 0', 'niv_label', 'niv'], inplace=True)\n",
    "orig_ALS_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALS_df.describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of used features\n",
    "ALS_cols = list(ALS_df.columns)\n",
    "\n",
    "# Remove features that aren't used by the model to predict the label\n",
    "for unused_feature in ['subject_id', 'ts', 'niv_label']:\n",
    "    ALS_cols.remove(unused_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALS_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model with the best validation performance\n",
    "# model = utils.load_checkpoint('GitHub/FCUL_ALS_Disease_Progression/models/checkpoint_no_NIV_10_05_2019_03_03.pth')\n",
    "model = utils.load_checkpoint('GitHub/FCUL_ALS_Disease_Progression/models/checkpoint_07_06_2019_23_14.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting train and test sets, in tensor format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary containing the sequence length (number of temporal events) of each sequence (patient)\n",
    "seq_len_df = ALS_df.groupby('subject_id').ts.count().to_frame().sort_values(by='ts', ascending=False)\n",
    "seq_len_dict = dict([(idx, val[0]) for idx, val in list(zip(seq_len_df.index, seq_len_df.values))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_patients = ALS_df.subject_id.nunique()     # Total number of patients\n",
    "n_inputs = len(ALS_df.columns)               # Number of input features\n",
    "padding_value = 0                            # Value to be used in the padding\n",
    "\n",
    "# Pad data (to have fixed sequence length) and convert into a PyTorch tensor\n",
    "data = utils.dataframe_to_padded_tensor(ALS_df, seq_len_dict, n_patients, n_inputs, padding_value=padding_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Dataset object from the data tensor\n",
    "dataset = Time_Series_Dataset(data, ALS_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the train, validation and test sets data loaders and indices\n",
    "train_dataloader, val_dataloader, test_dataloader, \\\n",
    "train_indices, val_indices, test_indices            = utils.create_train_sets(dataset, test_train_ratio=0.2, \n",
    "                                                                              validation_ratio=0.1, \n",
    "                                                                              batch_size=1000, get_indeces=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the tensor data of the training and test sets\n",
    "train_features, train_labels = next(iter(train_dataloader))\n",
    "test_features, test_labels = next(iter(test_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confirm performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output, metrics_vals = utils.model_inference(model, seq_len_dict, dataloader=test_dataloader, \n",
    "                       metrics=['loss', 'accuracy', 'AUC', 'precision', 'recall', 'F1'], output_rounded=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the original lengths of the sequences, for the test data\n",
    "x_lengths_test = [seq_len_dict[patient] for patient in list(test_features[:, 0, 0].numpy())]\n",
    "\n",
    "# Sorted indeces to get the data sorted by sequence length\n",
    "data_sorted_idx = list(np.argsort(x_lengths_test)[::-1])\n",
    "\n",
    "# Sort the x_lengths array by descending sequence length\n",
    "x_lengths_test = [x_lengths_test[idx] for idx in data_sorted_idx]\n",
    "\n",
    "# Sort the features and labels by descending sequence length\n",
    "test_data_exp = test_features[data_sorted_idx, :, :]\n",
    "test_labels_ = test_labels[data_sorted_idx, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust the labels so that it gets the exact same shape as the predictions\n",
    "# (i.e. sequence length = max sequence length of the current batch, not the max of all the data)\n",
    "labels = torch.nn.utils.rnn.pack_padded_sequence(test_labels_, x_lengths_test, batch_first=True)\n",
    "labels, _ = torch.nn.utils.rnn.pad_packed_sequence(labels, batch_first=True, padding_value=999999)\n",
    "\n",
    "mask = (labels <= 1).view(-1, 1).float()                    # Create a mask by filtering out all labels that are not a padding value\n",
    "unpadded_labels = torch.masked_select(labels.contiguous().view(-1, 1), mask.byte()) # Completely remove the padded values from the labels using the mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[tensor.item() for tensor in list(unpadded_labels.int())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[tensor.item() for tensor in list(output)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(np.diff(unpadded_labels.int().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[i for i, x in enumerate(list(np.diff(unpadded_labels.int().numpy()))) if x==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[i for i, x in enumerate(list(np.diff(output.int().numpy()))) if x==1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment:** [Before removing NIV from the features] Most times, the model only predicts NIV use after the patient already started that treatment. This means that it usely only predicts the continuation of the treatment, which isn't so useful. Need to experiment training a model without giving any information regarding current NIV usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the original lengths of the sequences and sort the data\n",
    "train_features, train_labels, x_lengths_train = utils.sort_by_seq_len(train_features, seq_len_dict, labels=train_labels)\n",
    "test_features, test_labels, x_lengths_test = utils.sort_by_seq_len(test_features, seq_len_dict, labels=test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features[0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Denormalize the feature values so that the plots are easier to understand\n",
    "test_features_denorm = utils.denormalize_data(orig_ALS_df, test_features, see_progress=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features[0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features_denorm[0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [],
   "source": [
    "# Use the first n_bkgnd_samples training examples as our background dataset to integrate over\n",
    "# (Ignoring the first 2 features, as they constitute the identifiers 'subject_id' and 'ts')\n",
    "n_bkgnd_samples = 200\n",
    "explainer = shap.DeepExplainer(model, train_features[:n_bkgnd_samples, :, 2:].float(), feedforward_args=[x_lengths_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "# Explain the predictions of the first 10 patients in the test set\n",
    "n_samples = 1\n",
    "shap_values = explainer.shap_values(test_features[:n_samples, :, 2:].float(), \n",
    "                                    feedforward_args=[x_lengths_train, x_lengths_test[:n_samples]],\n",
    "                                    var_seq_len=True)\n",
    "print(f'Calculation of SHAP values took {time.time() - start_time} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer.expected_value[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init the JS visualization code\n",
    "shap.initjs()\n",
    "\n",
    "# Choosing which example to use\n",
    "patient = 0\n",
    "ts = 1\n",
    "\n",
    "# Plot the explanation of one prediction\n",
    "shap.force_plot(explainer.expected_value[0], shap_values[patient][ts], features=test_features[patient, ts, 2:].numpy(), feature_names=ALS_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features_denorm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(orig_ALS_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Init the JS visualization code\n",
    "shap.initjs()\n",
    "\n",
    "# Choosing which example to use\n",
    "patient = 0\n",
    "ts = 1\n",
    "\n",
    "# Plot the explanation of one prediction\n",
    "shap.force_plot(explainer.expected_value[0], shap_values[patient][ts], features=test_features_denorm[patient, ts, 2:].numpy(), feature_names=ALS_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Init the JS visualization code\n",
    "shap.initjs()\n",
    "\n",
    "# Choosing which example to use\n",
    "patient = 0\n",
    "\n",
    "# True sequence length of the current patient's data\n",
    "seq_len = seq_len_dict[test_features_denorm[patient, 0, 0].item()]\n",
    "\n",
    "# Plot the explanation of the predictions for one patient\n",
    "shap.force_plot(explainer.expected_value[0], shap_values[patient, :seq_len], features=test_features_denorm[patient, :seq_len, 2:].numpy(), feature_names=ALS_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize the effects of all the features\n",
    "shap.summary_plot(shap_values.reshape(-1, model.lstm.input_size), features=test_features_denorm[:n_samples, :, 2:].contiguous().view(-1, model.lstm.input_size).numpy(), feature_names=ALS_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize the effects of all the features\n",
    "shap.summary_plot(shap_values.reshape(-1, model.lstm.input_size), features=test_features_denorm[:, :, 2:].view(-1, model.lstm.input_size).numpy(), feature_names=ALS_cols, plot_type='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize the effects of all the features\n",
    "shap.summary_plot(shap_values.reshape(-1, model.lstm.input_size), features=test_features_denorm[:n_samples, :, 2:].contiguous().view(-1, model.lstm.input_size).numpy(), feature_names=ALS_cols, plot_type='violin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comments:**\n",
    "\n",
    "[Before removing padings from data]\n",
    "* The SHAP values are significantly higher than what I usually see (tends to be between -1 and 1, not between -100000 and 250000). It seems to be because of the padding (the padding value is 999999).\n",
    "* ~The output values also seem to be wrong in the patients' force plot, as it goes above 1.~ It doesn't seem to be a problem after all, it's just a SHAP indicator of whether the prediction will be 0 (if the value is negative) or 1 (if the value is positive).\n",
    "\n",
    "[After removing padings from data]\n",
    "* The SHAP values now seem to have normal values (between -1 and 1) and the plots also look good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Interpreter\n",
    "\n",
    "Using my custom class for model interpretability through instance and feature importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [],
   "source": [
    "interpreter = ModelInterpreter(model, ALS_df, seq_len_dict, fast_calc=True, SHAP_bkgnd_samples=200, padding_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [],
   "source": [
    "# %%pixie_debugger\n",
    "# Number of patients to analyse\n",
    "n_patients = 1\n",
    "\n",
    "_, loss_mtx = interpreter.interpret_model(bkgnd_data=train_features, test_data=test_features[:n_patients], test_labels=test_labels[:n_patients], instance_importance=False, feature_importance=True, debug_loss=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_mtx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter.feat_scores[0][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.max(interpreter.feat_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[ALS_cols[idx] for idx in [t.item() for t in list((interpreter.feat_scores[0][5] == 1).nonzero())]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current day and time to attach to the saved model's name\n",
    "current_datetime = datetime.now().strftime('%d_%m_%Y_%H_%M')\n",
    "\n",
    "# Path where the model interpreter will be saved\n",
    "interpreter_path = 'GitHub/FCUL_ALS_Disease_Progression/interpreters/'\n",
    "\n",
    "# Filename and path where the model will be saved\n",
    "interpreter_filename = f'{interpreter_path}checkpoint_{current_datetime}.pickle'\n",
    "\n",
    "# Save model interpreter object, with the instance and feature importance scores, in a pickle file\n",
    "with open(interpreter_filename, 'wb') as file:\n",
    "    pickle.dump(interpreter, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved model interpreter object\n",
    "# with open(interpreter_filename, 'rb') as file:\n",
    "with open('GitHub/FCUL_ALS_Disease_Progression/interpreters/checkpoint_15_06_2019_19_04.pickle', 'rb') as file:\n",
    "    interpreter_loaded = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if np.array_equal(interpreter_loaded.feat_scores, interpreter.feat_scores):\n",
    "    print('The model interpreter object was correctly saved.')\n",
    "    interpreter = interpreter_loaded\n",
    "else:\n",
    "    print('ERROR: There was a problem saving the model interpreter object.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only to use when analysing a model interpreter, after having already been saved\n",
    "interpreter = interpreter_loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter.feat_scores[0, :x_lengths_test[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature importance plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize the effects of all the features\n",
    "shap.summary_plot(interpreter.feat_scores.reshape(-1, interpreter.model.lstm.input_size), \n",
    "                  features=test_features_denorm[:, :, 2:].view(-1, interpreter.model.lstm.input_size).numpy(), \n",
    "                  feature_names=ALS_cols, plot_type='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize the effects of all the features\n",
    "shap.summary_plot(interpreter.feat_scores.reshape(-1, model.lstm.input_size), \n",
    "                  features=test_features_denorm[:, :, 2:].contiguous().view(-1, interpreter.model.lstm.input_size).numpy(), \n",
    "                  feature_names=ALS_cols, plot_type='violin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comments:**\n",
    "\n",
    "[Using padding value of 999999]\n",
    "\n",
    "* The above summary plot isn't clear, as there doesn't seem to be any distinction between feature's high and low values impact on the output. However, it might be doe to padding values being used in the background data (same reason for the predictions bellow also being wrong).\n",
    "\n",
    "[Using padding value of 0]\n",
    "\n",
    "* The plot now makes perfect sense, showing the different effects of low or high values of each feature, with apparently realistic reasoning. This also confirms that the paddings are still messing with the SHAP values calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choosing which example to use\n",
    "subject_id = 125\n",
    "patient = utils.find_subject_idx(test_features_denorm, subject_id=subject_id)\n",
    "patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Init the JS visualization code\n",
    "shap.initjs()\n",
    "\n",
    "# True sequence length of the current patient's data\n",
    "seq_len = seq_len_dict[test_features_denorm[patient, 0, 0].item()]\n",
    "\n",
    "# Plot the explanation of the predictions for one patient\n",
    "shap.force_plot(interpreter.explainer.expected_value[0], \n",
    "                interpreter.feat_scores[patient, :seq_len], \n",
    "                features=test_features_denorm[patient, :seq_len, 2:].numpy(), \n",
    "                feature_names=ALS_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Init the JS visualization code\n",
    "shap.initjs()\n",
    "\n",
    "# Choosing which timestamp to use\n",
    "ts = 10\n",
    "\n",
    "# Plot the explanation of one prediction\n",
    "shap.force_plot(interpreter.explainer.expected_value[0], \n",
    "                interpreter.feat_scores[patient][ts], \n",
    "                features=test_features_denorm[patient, ts, 2:].numpy(), \n",
    "                feature_names=ALS_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comments:**\n",
    "\n",
    "[Using padding value of 999999]\n",
    "\n",
    "* It seems as if the SHAP values weren't indexed in the same way as the instance importance scores. Patients such as 125 shouldn't be highly probable of NIV use, with high influence of timestamps 9 and 10, and then have such a low estimated output value in the feature importance part.\n",
    "\n",
    "* On the other hand, the dataframe query bellow shows that the data is indeed as it's shown in the force plot. There might be some critical problem in the way SHAP is calculating this values or at least the expected output value.\n",
    "\n",
    "[Using padding value of 0]\n",
    "\n",
    "* Although the summary plot improved, the force plots still show incorrect prediction values, most likely due to the interference of the padding values.\n",
    "\n",
    "* I might need to make my own version of a Shapley values estimator, adapted for multivariate sequential data and PyTorch, making it only select non-padding values from the background data and use the current hidden state in the model's output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_output = interpreter.model(test_features[patient, :, 2:].float().unsqueeze(0), [x_lengths_test[patient]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_output_s = pd.Series([float(x) for x in list(ref_output.detach().numpy())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get an overview of the important features and model output for the current patient\n",
    "orig_ALS_df[orig_ALS_df.subject_id == subject_id][['ts', 'p0.1', 'p1', 'p2', 'p3', 'p4', 'p5', 'p6', 'p7', 'p8', 'p9',\n",
    "                                                   'p10', '1r', '2r', '3r', 'phrenmeanampl', 'mip']] \\\n",
    "                                                 .reset_index().drop(columns='index').assign(output=ref_output_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instance importance plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interpreter_loaded.inst_scores[interpreter_loaded.inst_scores == interpreter_loaded.padding_value]\n",
    "interpreter.inst_scores[interpreter.inst_scores == 999999] = np.nan\n",
    "interpreter.inst_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inst_scores_avg = np.nanmean(interpreter.inst_scores, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(range(1, len(inst_scores_avg)+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [go.Bar(\n",
    "                x=list(range(1, len(inst_scores_avg[:20])+1)),\n",
    "                y=list(inst_scores_avg[:20])\n",
    "              )]\n",
    "layout = go.Layout(\n",
    "                    title='Average instance importance scores',\n",
    "                    xaxis=dict(title='Instance'),\n",
    "                    yaxis=dict(title='Importance scores')\n",
    "                  )\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "py.iplot(fig, filename='basic-bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# True sequence length of the current patient's data\n",
    "seq_len = seq_len_dict[test_features[patient, 0, 0].item()]\n",
    "\n",
    "# Plot the instance importance of one sequence\n",
    "interpreter.instance_importance_plot(test_features, interpreter.inst_scores, patient, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_output = interpreter.model(test_features[patient, :, 2:].float().unsqueeze(0), [x_lengths_test[patient]])\n",
    "ref_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ref_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_lengths_test[patient]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_output[-1].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_output2, _ = utils.model_inference(interpreter.model, interpreter.seq_len_dict, data=(test_features[patient].unsqueeze(0), test_labels[patient].unsqueeze(0)),\n",
    "                                       metrics=[''], seq_final_outputs=True)\n",
    "ref_output2.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance = 0\n",
    "sequence_data = test_features[patient, :, 2:].float()\n",
    "x_length = x_lengths_test[patient]\n",
    "\n",
    "# Indeces without the instance that is being analyzed\n",
    "instances_idx = list(range(sequence_data.shape[0]))\n",
    "instances_idx.remove(instance)\n",
    "\n",
    "# Sequence data without the instance that is being analyzed\n",
    "sequence_data = sequence_data[instances_idx, :]\n",
    "\n",
    "# Add a third dimension for the data to be readable by the model\n",
    "sequence_data = sequence_data.unsqueeze(0)\n",
    "\n",
    "# Calculate the output without the instance that is being analyzed\n",
    "new_output = interpreter.model(sequence_data, [x_length-1])\n",
    "new_output[-1].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features[patient, :, 2:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_output[-1].item() - new_output[-1].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_lengths_test_cumsum = np.cumsum(x_lengths_test[:5])\n",
    "x_lengths_test_cumsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output[x_lengths_test_cumsum-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_prob, _ = utils.model_inference(interpreter.model, interpreter.seq_len_dict, data=(test_features[:n_patients], test_labels[:n_patients]),\n",
    "                                     metrics=[''], seq_final_outputs=True)\n",
    "pred_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_prob.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features[:n_patients].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter.inst_scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter.instance_importance_plot(test_features[:n_patients], interpreter.inst_scores, pred_prob=pred_prob)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "fcul-als-python",
   "language": "python",
   "name": "fcul-als-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
