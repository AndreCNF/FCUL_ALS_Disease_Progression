{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training\n",
    "---\n",
    "\n",
    "Training models on the preprocessed ALS dataset from Faculdade de Ciências da Universidade de Lisboa (FCUL) with the data from over 1000 patients collected in Portugal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os                                  # os handles directory/workspace changes\n",
    "import comet_ml                            # Comet.ml can log training metrics, parameters, do version control and parameter optimization\n",
    "import torch                               # PyTorch to create and apply deep learning models\n",
    "import xgboost as xgb                      # Gradient boosting trees models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, f1_score, log_loss, roc_auc_score\n",
    "import joblib                              # Save and load scikit-learn models in disk\n",
    "from datetime import datetime              # datetime to use proper date and time formats\n",
    "import yaml                                # Save and load YAML files\n",
    "import getpass                             # Get password or similar private inputs\n",
    "from ipywidgets import interact            # Display selectors and sliders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pixiedust database opened successfully\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div style=\"margin:10px\">\n",
       "            <a href=\"https://github.com/ibm-watson-data-lab/pixiedust\" target=\"_new\">\n",
       "                <img src=\"https://github.com/ibm-watson-data-lab/pixiedust/raw/master/docs/_static/pd_icon32.png\" style=\"float:left;margin-right:10px\"/>\n",
       "            </a>\n",
       "            <span>Pixiedust version 1.1.18</span>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Debugging packages\n",
    "import pixiedust                           # Debugging in Jupyter Notebook cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the parquet dataset files\n",
    "data_path = 'Datasets/Thesis/FCUL_ALS/cleaned/'\n",
    "# Path to the code files\n",
    "project_path = 'GitHub/FCUL_ALS_Disease_Progression/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to the scripts directory\n",
    "os.chdir(\"../scripts/\")\n",
    "import utils                               # Context specific (in this case, for the ALS data) methods\n",
    "import Models                              # Deep learning models\n",
    "# Change to parent directory (presumably \"Documents\")\n",
    "os.chdir(\"../../..\")\n",
    "import pandas as pd                        # Pandas to load and handle the data\n",
    "import data_utils as du                    # Data science and machine learning relevant methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "du.set_pandas_library(lib='pandas')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Allow pandas to show more columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 3000)\n",
    "pd.set_option('display.max_rows', 3000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the random seed for reproducibility:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "du.set_random_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comet ML settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comet_ml_project_name = input('Comet ML project name:')\n",
    "comet_ml_workspace = input('Comet ML workspace:')\n",
    "comet_ml_api_key = getpass.getpass('Comet ML API key')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "656c45f1e4774b8bb4193f6d87708441",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='data_mode', options=('one hot encoded', 'learn embedding', 'pre-em…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_mode = None                        # The mode in which we'll use the data, either one hot encoded or pre-embedded\n",
    "ml_core = None                             # The core machine learning type we'll use; either traditional ML or DL\n",
    "use_delta_ts = None                        # Indicates if we'll use time variation info\n",
    "time_window_days = None                    # Number of days on which we want to predict NIV\n",
    "already_embedded = None                    # Indicates if categorical features are already embedded when fetching a batch\n",
    "@interact\n",
    "def get_dataset_mode(data_mode=['one hot encoded', 'learn embedding', 'pre-embedded'],\n",
    "                     ml_or_dl=['deep learning', 'machine learning'],\n",
    "                     use_delta=[False, 'normalized', 'raw'], window_d=(0, 90, 30)):\n",
    "    global dataset_mode, ml_core, use_delta_ts, time_window_days, already_embedded\n",
    "    dataset_mode, ml_core, use_delta_ts, time_window_days = data_mode, ml_or_dl, use_delta, window_d\n",
    "    already_embedded = dataset_mode == 'embedded'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_column = 'subject_id'                   # Name of the sequence ID column\n",
    "ts_column = 'ts'                           # Name of the timestamp column\n",
    "label_column = 'niv_label'                 # Name of the label column\n",
    "n_inputs = 46                              # Number of input features\n",
    "n_outputs = 1                              # Number of outputs\n",
    "padding_value = 999999                     # Padding value used to fill in sequences up to the maximum sequence length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stream_dtypes = open(f'{data_path}eICU_dtype_dict.yml', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dtype_dict = yaml.load(stream_dtypes, Loader=yaml.FullLoader)\n",
    "# dtype_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One hot encoding columns categorization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_categ_feat_ohe = open(f'{data_path}categ_feat_ohe.yml', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'el_escorial_reviewed_criteria': ['el_escorial_reviewed_criteria_def',\n",
       "  'el_escorial_reviewed_criteria_missing_value',\n",
       "  'el_escorial_reviewed_criteria_pbp',\n",
       "  'el_escorial_reviewed_criteria_pma',\n",
       "  'el_escorial_reviewed_criteria_poss',\n",
       "  'el_escorial_reviewed_criteria_pro',\n",
       "  'el_escorial_reviewed_criteria_pro_lab_sup',\n",
       "  'el_escorial_reviewed_criteria_sus']}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categ_feat_ohe = yaml.load(stream_categ_feat_ohe, Loader=yaml.FullLoader)\n",
    "categ_feat_ohe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['el_escorial_reviewed_criteria']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(categ_feat_ohe.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Training parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_train_ratio = 0.25                    # Percentage of the data which will be used as a test set\n",
    "validation_ratio = 0.1                     # Percentage of the data from the training set which is used for validation purposes\n",
    "batch_size = 32                            # Number of unit stays in a mini batch\n",
    "n_epochs = 20                              # Number of epochs\n",
    "lr = 0.001                                 # Learning rate\n",
    "num_workers = 0                            # How many subprocesses to use for data loading\n",
    "embedding_dim = None                       # List of embedding dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stream_tvt_sets = open(f'{data_path}eICU_tvt_sets.yml', 'r')\n",
    "# eICU_tvt_sets = yaml.load(stream_tvt_sets, Loader=yaml.FullLoader)\n",
    "# eICU_tvt_sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ['loss', 'accuracy', 'AUC']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>subject_id</th>\n",
       "      <th>ts</th>\n",
       "      <th>gender</th>\n",
       "      <th>bmi</th>\n",
       "      <th>mnd_familiar_history</th>\n",
       "      <th>age_at_onset</th>\n",
       "      <th>disease_duration</th>\n",
       "      <th>r</th>\n",
       "      <th>p1</th>\n",
       "      <th>p2</th>\n",
       "      <th>p3</th>\n",
       "      <th>p4</th>\n",
       "      <th>p5</th>\n",
       "      <th>p6</th>\n",
       "      <th>p7</th>\n",
       "      <th>p8</th>\n",
       "      <th>p9</th>\n",
       "      <th>p10</th>\n",
       "      <th>1r</th>\n",
       "      <th>2r</th>\n",
       "      <th>3r</th>\n",
       "      <th>vc</th>\n",
       "      <th>fvc</th>\n",
       "      <th>mip</th>\n",
       "      <th>mep</th>\n",
       "      <th>p0.1</th>\n",
       "      <th>phrenmeanlat</th>\n",
       "      <th>phrenmeanampl</th>\n",
       "      <th>niv</th>\n",
       "      <th>el_escorial_reviewed_criteria_def</th>\n",
       "      <th>el_escorial_reviewed_criteria_missing_value</th>\n",
       "      <th>el_escorial_reviewed_criteria_pbp</th>\n",
       "      <th>el_escorial_reviewed_criteria_pma</th>\n",
       "      <th>el_escorial_reviewed_criteria_poss</th>\n",
       "      <th>el_escorial_reviewed_criteria_pro</th>\n",
       "      <th>el_escorial_reviewed_criteria_pro_lab_sup</th>\n",
       "      <th>el_escorial_reviewed_criteria_sus</th>\n",
       "      <th>onset_form_1</th>\n",
       "      <th>onset_form_2</th>\n",
       "      <th>onset_form_3</th>\n",
       "      <th>onset_form_4</th>\n",
       "      <th>onset_form_5</th>\n",
       "      <th>onset_form_ftd</th>\n",
       "      <th>onset_form_missing_value</th>\n",
       "      <th>lmn</th>\n",
       "      <th>umn_vs_lmn_missing_value</th>\n",
       "      <th>umn</th>\n",
       "      <th>c9orf72_missing_value</th>\n",
       "      <th>c9orf72</th>\n",
       "      <th>niv_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.172714</td>\n",
       "      <td>0.008184</td>\n",
       "      <td>-0.403247</td>\n",
       "      <td>-0.560198</td>\n",
       "      <td>0.456156</td>\n",
       "      <td>0.645667</td>\n",
       "      <td>0.477898</td>\n",
       "      <td>0.570617</td>\n",
       "      <td>0.899139</td>\n",
       "      <td>1.150113</td>\n",
       "      <td>0.625532</td>\n",
       "      <td>0.360715</td>\n",
       "      <td>-0.243449</td>\n",
       "      <td>0.153474</td>\n",
       "      <td>0.690144</td>\n",
       "      <td>0.518483</td>\n",
       "      <td>0.246144</td>\n",
       "      <td>0.44108</td>\n",
       "      <td>0.002577</td>\n",
       "      <td>0.108659</td>\n",
       "      <td>0.070135</td>\n",
       "      <td>-0.414252</td>\n",
       "      <td>-1.245742</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.172714</td>\n",
       "      <td>0.008184</td>\n",
       "      <td>-0.403247</td>\n",
       "      <td>-0.560198</td>\n",
       "      <td>0.456156</td>\n",
       "      <td>0.645667</td>\n",
       "      <td>0.477898</td>\n",
       "      <td>0.570617</td>\n",
       "      <td>0.899139</td>\n",
       "      <td>1.150113</td>\n",
       "      <td>0.625532</td>\n",
       "      <td>0.360715</td>\n",
       "      <td>-0.243449</td>\n",
       "      <td>0.153474</td>\n",
       "      <td>0.690144</td>\n",
       "      <td>0.518483</td>\n",
       "      <td>0.246144</td>\n",
       "      <td>0.44108</td>\n",
       "      <td>0.002577</td>\n",
       "      <td>0.108659</td>\n",
       "      <td>0.070135</td>\n",
       "      <td>-0.414252</td>\n",
       "      <td>-1.245742</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>36.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.172714</td>\n",
       "      <td>0.008184</td>\n",
       "      <td>-0.403247</td>\n",
       "      <td>-0.560198</td>\n",
       "      <td>0.456156</td>\n",
       "      <td>0.645667</td>\n",
       "      <td>0.477898</td>\n",
       "      <td>0.570617</td>\n",
       "      <td>0.899139</td>\n",
       "      <td>1.150113</td>\n",
       "      <td>0.625532</td>\n",
       "      <td>0.360715</td>\n",
       "      <td>-0.243449</td>\n",
       "      <td>-0.502715</td>\n",
       "      <td>0.690144</td>\n",
       "      <td>0.518483</td>\n",
       "      <td>0.246144</td>\n",
       "      <td>0.44108</td>\n",
       "      <td>-0.176048</td>\n",
       "      <td>-0.073405</td>\n",
       "      <td>-0.311736</td>\n",
       "      <td>-0.460193</td>\n",
       "      <td>-0.699619</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>91.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.172714</td>\n",
       "      <td>0.008184</td>\n",
       "      <td>-0.403247</td>\n",
       "      <td>-0.560198</td>\n",
       "      <td>-0.178179</td>\n",
       "      <td>-0.573469</td>\n",
       "      <td>0.477898</td>\n",
       "      <td>-0.161258</td>\n",
       "      <td>-1.687086</td>\n",
       "      <td>-1.423855</td>\n",
       "      <td>-1.359491</td>\n",
       "      <td>-1.629878</td>\n",
       "      <td>-0.974786</td>\n",
       "      <td>-1.158903</td>\n",
       "      <td>0.012940</td>\n",
       "      <td>-1.061011</td>\n",
       "      <td>0.246144</td>\n",
       "      <td>0.44108</td>\n",
       "      <td>-0.176048</td>\n",
       "      <td>-0.073405</td>\n",
       "      <td>-0.311736</td>\n",
       "      <td>-0.460193</td>\n",
       "      <td>-0.699619</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>115.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.172714</td>\n",
       "      <td>0.008184</td>\n",
       "      <td>-0.403247</td>\n",
       "      <td>-0.560198</td>\n",
       "      <td>0.138988</td>\n",
       "      <td>0.036099</td>\n",
       "      <td>0.477898</td>\n",
       "      <td>-0.161258</td>\n",
       "      <td>-1.687086</td>\n",
       "      <td>-1.423855</td>\n",
       "      <td>-1.359491</td>\n",
       "      <td>-1.629878</td>\n",
       "      <td>-1.706122</td>\n",
       "      <td>-1.158903</td>\n",
       "      <td>0.012940</td>\n",
       "      <td>0.518483</td>\n",
       "      <td>-0.693730</td>\n",
       "      <td>0.44108</td>\n",
       "      <td>-0.176048</td>\n",
       "      <td>-0.073405</td>\n",
       "      <td>-0.311736</td>\n",
       "      <td>-0.460193</td>\n",
       "      <td>-0.699619</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  subject_id     ts  gender       bmi  mnd_familiar_history  \\\n",
       "0           0           2    0.0       0 -1.172714              0.008184   \n",
       "1           1           2   27.0       0 -1.172714              0.008184   \n",
       "2           2           2   36.0       0 -1.172714              0.008184   \n",
       "3           3           2   91.0       0 -1.172714              0.008184   \n",
       "4           4           2  115.0       0 -1.172714              0.008184   \n",
       "\n",
       "   age_at_onset  disease_duration         r        p1        p2        p3  \\\n",
       "0     -0.403247         -0.560198  0.456156  0.645667  0.477898  0.570617   \n",
       "1     -0.403247         -0.560198  0.456156  0.645667  0.477898  0.570617   \n",
       "2     -0.403247         -0.560198  0.456156  0.645667  0.477898  0.570617   \n",
       "3     -0.403247         -0.560198 -0.178179 -0.573469  0.477898 -0.161258   \n",
       "4     -0.403247         -0.560198  0.138988  0.036099  0.477898 -0.161258   \n",
       "\n",
       "         p4        p5        p6        p7        p8        p9       p10  \\\n",
       "0  0.899139  1.150113  0.625532  0.360715 -0.243449  0.153474  0.690144   \n",
       "1  0.899139  1.150113  0.625532  0.360715 -0.243449  0.153474  0.690144   \n",
       "2  0.899139  1.150113  0.625532  0.360715 -0.243449 -0.502715  0.690144   \n",
       "3 -1.687086 -1.423855 -1.359491 -1.629878 -0.974786 -1.158903  0.012940   \n",
       "4 -1.687086 -1.423855 -1.359491 -1.629878 -1.706122 -1.158903  0.012940   \n",
       "\n",
       "         1r        2r       3r        vc       fvc       mip       mep  \\\n",
       "0  0.518483  0.246144  0.44108  0.002577  0.108659  0.070135 -0.414252   \n",
       "1  0.518483  0.246144  0.44108  0.002577  0.108659  0.070135 -0.414252   \n",
       "2  0.518483  0.246144  0.44108 -0.176048 -0.073405 -0.311736 -0.460193   \n",
       "3 -1.061011  0.246144  0.44108 -0.176048 -0.073405 -0.311736 -0.460193   \n",
       "4  0.518483 -0.693730  0.44108 -0.176048 -0.073405 -0.311736 -0.460193   \n",
       "\n",
       "       p0.1  phrenmeanlat  phrenmeanampl  niv  \\\n",
       "0 -1.245742           0.0            0.0    0   \n",
       "1 -1.245742           0.0            0.0    0   \n",
       "2 -0.699619           0.0            0.0    0   \n",
       "3 -0.699619           0.0            0.0    1   \n",
       "4 -0.699619           0.0            0.0    1   \n",
       "\n",
       "   el_escorial_reviewed_criteria_def  \\\n",
       "0                                  0   \n",
       "1                                  0   \n",
       "2                                  0   \n",
       "3                                  0   \n",
       "4                                  0   \n",
       "\n",
       "   el_escorial_reviewed_criteria_missing_value  \\\n",
       "0                                            1   \n",
       "1                                            1   \n",
       "2                                            1   \n",
       "3                                            1   \n",
       "4                                            1   \n",
       "\n",
       "   el_escorial_reviewed_criteria_pbp  el_escorial_reviewed_criteria_pma  \\\n",
       "0                                  0                                  0   \n",
       "1                                  0                                  0   \n",
       "2                                  0                                  0   \n",
       "3                                  0                                  0   \n",
       "4                                  0                                  0   \n",
       "\n",
       "   el_escorial_reviewed_criteria_poss  el_escorial_reviewed_criteria_pro  \\\n",
       "0                                   0                                  0   \n",
       "1                                   0                                  0   \n",
       "2                                   0                                  0   \n",
       "3                                   0                                  0   \n",
       "4                                   0                                  0   \n",
       "\n",
       "   el_escorial_reviewed_criteria_pro_lab_sup  \\\n",
       "0                                          0   \n",
       "1                                          0   \n",
       "2                                          0   \n",
       "3                                          0   \n",
       "4                                          0   \n",
       "\n",
       "   el_escorial_reviewed_criteria_sus  onset_form_1  onset_form_2  \\\n",
       "0                                  0             1             0   \n",
       "1                                  0             1             0   \n",
       "2                                  0             1             0   \n",
       "3                                  0             1             0   \n",
       "4                                  0             1             0   \n",
       "\n",
       "   onset_form_3  onset_form_4  onset_form_5  onset_form_ftd  \\\n",
       "0             0             0             0               0   \n",
       "1             0             0             0               0   \n",
       "2             0             0             0               0   \n",
       "3             0             0             0               0   \n",
       "4             0             0             0               0   \n",
       "\n",
       "   onset_form_missing_value  lmn  umn_vs_lmn_missing_value  umn  \\\n",
       "0                         0    0                         1    0   \n",
       "1                         0    0                         1    0   \n",
       "2                         0    0                         1    0   \n",
       "3                         0    0                         1    0   \n",
       "4                         0    0                         1    0   \n",
       "\n",
       "   c9orf72_missing_value  c9orf72  niv_label  \n",
       "0                      1        0      False  \n",
       "1                      1        0       True  \n",
       "2                      1        0       True  \n",
       "3                      1        0       True  \n",
       "4                      1        0      False  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ALS_df = pd.read_csv(f'{data_path}FCUL_ALS_cleaned.csv')\n",
    "ALS_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove the `Unnamed: 0` column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALS_df.drop(columns=['Unnamed: 0'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['subject_id', 'ts', 'gender', 'bmi', 'mnd_familiar_history',\n",
       "       'age_at_onset', 'disease_duration', 'r', 'p1', 'p2', 'p3', 'p4', 'p5',\n",
       "       'p6', 'p7', 'p8', 'p9', 'p10', '1r', '2r', '3r', 'vc', 'fvc', 'mip',\n",
       "       'mep', 'p0.1', 'phrenmeanlat', 'phrenmeanampl', 'niv',\n",
       "       'el_escorial_reviewed_criteria_def',\n",
       "       'el_escorial_reviewed_criteria_missing_value',\n",
       "       'el_escorial_reviewed_criteria_pbp',\n",
       "       'el_escorial_reviewed_criteria_pma',\n",
       "       'el_escorial_reviewed_criteria_poss',\n",
       "       'el_escorial_reviewed_criteria_pro',\n",
       "       'el_escorial_reviewed_criteria_pro_lab_sup',\n",
       "       'el_escorial_reviewed_criteria_sus', 'onset_form_1', 'onset_form_2',\n",
       "       'onset_form_3', 'onset_form_4', 'onset_form_5', 'onset_form_ftd',\n",
       "       'onset_form_missing_value', 'lmn', 'umn_vs_lmn_missing_value', 'umn',\n",
       "       'c9orf72_missing_value', 'c9orf72', 'niv_label'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ALS_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ALS_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the maximum sequence length, so that the ML models and their related methods can handle all sequences, which have varying sequence lengths:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_length = ALS_df.groupby(id_column)[ts_column].count().max()\n",
    "total_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the label column, in case we're using a time window different than 90 days:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "if time_window_days is not 90:\n",
    "    # Recalculate the NIV label, based on the chosen time window\n",
    "    ALS_df[label_column] = utils.set_niv_label(ALS_df, time_window_days)\n",
    "    display(ALS_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove the `niv` column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALS_df.drop(columns=['niv'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the `delta_ts` (time variation between samples) if required:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pixiedust": {
     "displayParams": {
      "handlerId": "tableView"
     }
    }
   },
   "outputs": [],
   "source": [
    "if use_delta_ts is not False:\n",
    "    # Create a time variation column\n",
    "    ALS_df['delta_ts'] = ALS_df.groupby(id_column).ts.diff()\n",
    "    # Fill all the delta_ts missing values (the first value in a time series) with zeros\n",
    "    ALS_df['delta_ts'] = ALS_df['delta_ts'].fillna(0)\n",
    "if use_delta_ts == 'normalized':\n",
    "    # Normalize the time variation data\n",
    "    # NOTE: When using the MF2-LSTM model, since it assumes that the time\n",
    "    # variation is in days, we shouldn't normalize `delta_ts` with this model.\n",
    "    ALS_df['delta_ts'] = (ALS_df['delta_ts'] - ALS_df['delta_ts'].mean()) / ALS_df['delta_ts'].std()\n",
    "if use_delta_ts is not False:\n",
    "    display(ALS_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert into a padded tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[2.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         [2.0000e+00, 2.7000e+01, 0.0000e+00,  ..., 1.0000e+00,\n",
       "          0.0000e+00, 1.0000e+00],\n",
       "         [2.0000e+00, 3.6000e+01, 0.0000e+00,  ..., 1.0000e+00,\n",
       "          0.0000e+00, 1.0000e+00],\n",
       "         ...,\n",
       "         [1.0000e+06, 1.0000e+06, 0.0000e+00,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 1.0000e+06],\n",
       "         [1.0000e+06, 1.0000e+06, 0.0000e+00,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 1.0000e+06],\n",
       "         [1.0000e+06, 1.0000e+06, 0.0000e+00,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 1.0000e+06]],\n",
       "\n",
       "        [[3.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.0000e+00,\n",
       "          0.0000e+00, 1.0000e+00],\n",
       "         [3.0000e+00, 3.4000e+01, 0.0000e+00,  ..., 1.0000e+00,\n",
       "          0.0000e+00, 1.0000e+00],\n",
       "         [3.0000e+00, 9.2000e+01, 0.0000e+00,  ..., 1.0000e+00,\n",
       "          0.0000e+00, 1.0000e+00],\n",
       "         ...,\n",
       "         [1.0000e+06, 1.0000e+06, 0.0000e+00,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 1.0000e+06],\n",
       "         [1.0000e+06, 1.0000e+06, 0.0000e+00,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 1.0000e+06],\n",
       "         [1.0000e+06, 1.0000e+06, 0.0000e+00,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 1.0000e+06]],\n",
       "\n",
       "        [[4.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         [4.0000e+00, 3.7000e+01, 0.0000e+00,  ..., 1.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         [4.0000e+00, 1.4900e+02, 0.0000e+00,  ..., 1.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         ...,\n",
       "         [1.0000e+06, 1.0000e+06, 0.0000e+00,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 1.0000e+06],\n",
       "         [1.0000e+06, 1.0000e+06, 0.0000e+00,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 1.0000e+06],\n",
       "         [1.0000e+06, 1.0000e+06, 0.0000e+00,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 1.0000e+06]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[1.3300e+03, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         [1.3300e+03, 1.3000e+02, 0.0000e+00,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         [1.0000e+06, 1.0000e+06, 0.0000e+00,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 1.0000e+06],\n",
       "         ...,\n",
       "         [1.0000e+06, 1.0000e+06, 0.0000e+00,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 1.0000e+06],\n",
       "         [1.0000e+06, 1.0000e+06, 0.0000e+00,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 1.0000e+06],\n",
       "         [1.0000e+06, 1.0000e+06, 0.0000e+00,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 1.0000e+06]],\n",
       "\n",
       "        [[1.3360e+03, 0.0000e+00, 1.0000e+00,  ..., 1.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         [1.3360e+03, 9.6000e+01, 1.0000e+00,  ..., 1.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         [1.0000e+06, 1.0000e+06, 0.0000e+00,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 1.0000e+06],\n",
       "         ...,\n",
       "         [1.0000e+06, 1.0000e+06, 0.0000e+00,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 1.0000e+06],\n",
       "         [1.0000e+06, 1.0000e+06, 0.0000e+00,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 1.0000e+06],\n",
       "         [1.0000e+06, 1.0000e+06, 0.0000e+00,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 1.0000e+06]],\n",
       "\n",
       "        [[1.3410e+03, 0.0000e+00, 0.0000e+00,  ..., 1.0000e+00,\n",
       "          0.0000e+00, 1.0000e+00],\n",
       "         [1.3410e+03, 3.0400e+02, 0.0000e+00,  ..., 1.0000e+00,\n",
       "          0.0000e+00, 1.0000e+00],\n",
       "         [1.0000e+06, 1.0000e+06, 0.0000e+00,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 1.0000e+06],\n",
       "         ...,\n",
       "         [1.0000e+06, 1.0000e+06, 0.0000e+00,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 1.0000e+06],\n",
       "         [1.0000e+06, 1.0000e+06, 0.0000e+00,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 1.0000e+06],\n",
       "         [1.0000e+06, 1.0000e+06, 0.0000e+00,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 1.0000e+06]]], dtype=torch.float64)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = du.padding.dataframe_to_padded_tensor(ALS_df, padding_value=padding_value, \n",
    "                                             label_column=label_column, inplace=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the embedding configuration, if needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(categ_feat_ohe.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(categ_feat_ohe.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['el_escorial_reviewed_criteria_def',\n",
       " 'el_escorial_reviewed_criteria_missing_value',\n",
       " 'el_escorial_reviewed_criteria_pbp',\n",
       " 'el_escorial_reviewed_criteria_pma',\n",
       " 'el_escorial_reviewed_criteria_poss',\n",
       " 'el_escorial_reviewed_criteria_pro',\n",
       " 'el_escorial_reviewed_criteria_pro_lab_sup',\n",
       " 'el_escorial_reviewed_criteria_sus']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(categ_feat_ohe.values())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n"
     ]
    }
   ],
   "source": [
    "for feature in list(categ_feat_ohe.values())[0]:\n",
    "    print(du.search_explore.find_col_idx(ALS_df, feature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID index: 0\n",
      "Timestamp index: 1\n",
      "Label index: 48\n"
     ]
    }
   ],
   "source": [
    "# Indices of the ID, timestamp and label columns\n",
    "id_column_idx = du.search_explore.find_col_idx(ALS_df, id_column)\n",
    "ts_column_idx = du.search_explore.find_col_idx(ALS_df, ts_column)\n",
    "label_column_idx = du.search_explore.find_col_idx(ALS_df, label_column)\n",
    "print(\n",
    "f'''ID index: {id_column_idx}\n",
    "Timestamp index: {ts_column_idx}\n",
    "Label index: {label_column_idx}'''\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding features: [28, 29, 30, 31, 32, 33, 34, 35]\n",
      "Number of embeddings: 9\n"
     ]
    }
   ],
   "source": [
    "if dataset_mode == 'one hot encoded':\n",
    "    embed_features = None\n",
    "    n_embeddings = None\n",
    "    embedding_dim = None\n",
    "else:\n",
    "    embed_features = list()\n",
    "    if len(categ_feat_ohe.keys()) == 1:\n",
    "        for ohe_feature in list(categ_feat_ohe.values())[0]:\n",
    "            # Find the current feature's index so as to be able to use it as a tensor\n",
    "            feature_idx = du.search_explore.find_col_idx(ALS_df, ohe_feature)\n",
    "            # Decrease the index number if it's larger than the label column (which will be removed)\n",
    "            if feature_idx > label_column_idx:\n",
    "                feature_idx = feature_idx - 1\n",
    "            embed_features.append(feature_idx)\n",
    "        # Each one hot encoded column counts as a category to be embedded + missing values\n",
    "        n_embeddings = len(embed_features)+1\n",
    "    else:\n",
    "        n_embeddings = list()\n",
    "        for i in range(len(categ_feat_ohe.keys())):\n",
    "            tmp_list = list()\n",
    "            for ohe_feature in list(categ_feat_ohe.values())[i]:\n",
    "                # Find the current feature's index so as to be able to use it as a tensor\n",
    "                feature_idx = du.search_explore.find_col_idx(ALS_df, ohe_feature)\n",
    "                # Decrease the index number if it's larger than the label column (which will be removed)\n",
    "                if feature_idx > label_column_idx:\n",
    "                    feature_idx = feature_idx - 1\n",
    "                tmp_list.append(feature_idx)\n",
    "            # Add the current feature's list of one hot encoded columns\n",
    "            embed_features.append(tmp_list)\n",
    "            # Each one hot encoded column counts as a category to be embedded + missing values\n",
    "            n_embeddings.append(len(tmp_list)+1)\n",
    "    embedding_dim = None\n",
    "print(f'Embedding features: {embed_features}')\n",
    "print(f'Number of embeddings: {n_embeddings}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['subject_id', 'ts', 'gender', 'bmi', 'mnd_familiar_history',\n",
       "       'age_at_onset', 'disease_duration', 'r', 'p1', 'p2', 'p3', 'p4', 'p5',\n",
       "       'p6', 'p7', 'p8', 'p9', 'p10', '1r', '2r', '3r', 'vc', 'fvc', 'mip',\n",
       "       'mep', 'p0.1', 'phrenmeanlat', 'phrenmeanampl',\n",
       "       'el_escorial_reviewed_criteria_def',\n",
       "       'el_escorial_reviewed_criteria_missing_value',\n",
       "       'el_escorial_reviewed_criteria_pbp',\n",
       "       'el_escorial_reviewed_criteria_pma',\n",
       "       'el_escorial_reviewed_criteria_poss',\n",
       "       'el_escorial_reviewed_criteria_pro',\n",
       "       'el_escorial_reviewed_criteria_pro_lab_sup',\n",
       "       'el_escorial_reviewed_criteria_sus', 'onset_form_1', 'onset_form_2',\n",
       "       'onset_form_3', 'onset_form_4', 'onset_form_5', 'onset_form_ftd',\n",
       "       'onset_form_missing_value', 'lmn', 'umn_vs_lmn_missing_value', 'umn',\n",
       "       'c9orf72_missing_value', 'c9orf72', 'niv_label'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ALS_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['el_escorial_reviewed_criteria_def',\n",
       "       'el_escorial_reviewed_criteria_missing_value',\n",
       "       'el_escorial_reviewed_criteria_pbp',\n",
       "       'el_escorial_reviewed_criteria_pma',\n",
       "       'el_escorial_reviewed_criteria_poss',\n",
       "       'el_escorial_reviewed_criteria_pro',\n",
       "       'el_escorial_reviewed_criteria_pro_lab_sup',\n",
       "       'el_escorial_reviewed_criteria_sus'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ALS_df.columns[embed_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the pre-trained embedding layer, if required:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset_mode == 'pre-embedded':\n",
    "    pretrained_model_name = input('Name of the model file that has the embedding layer:')\n",
    "    pretrained_model_type = input('Type of the model that has the embedding layer:')\n",
    "    # Load the model with the pre-trained embedding layer\n",
    "    pretrained_model = du.deep_learning.load_checkpoint(filepath=f'{project_path}models/{pretrained_model_name}', \n",
    "                                                        ModelClass=getattr(Models, pretrained_model_type.replace('-', '')))\n",
    "    pretrained_embed_layers = pretrained_model.embed_layers\n",
    "    # Update the embedding dimension\n",
    "    embedding_dim = pretrained_embed_layers.embedding_dim\n",
    "    print(\n",
    "f'''\n",
    "Pre-trained model: {pretrained_model}\n",
    "Pre-trained embedding layer: {pretrained_embed_layers}\n",
    "'''\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the dataset object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = du.datasets.Time_Series_Dataset(ALS_df, data, padding_value=padding_value, \n",
    "                                          label_name=label_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure that we discard the ID, timestamp and label columns\n",
    "# if n_inputs != dataset.n_inputs:\n",
    "#     n_inputs = dataset.n_inputs\n",
    "#     print(f'Changed the number of inputs to {n_inputs}')\n",
    "# else:\n",
    "#     n_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "dataset.__len__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separating into train and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_dataloader, val_dataloader, test_dataloader,\n",
    "train_indeces, val_indeces, test_indeces) = du.machine_learning.create_train_sets(dataset,\n",
    "                                                                                  test_train_ratio=test_train_ratio,\n",
    "                                                                                  validation_ratio=validation_ratio,\n",
    "                                                                                  batch_size=batch_size,\n",
    "                                                                                  num_workers=num_workers,\n",
    "                                                                                  get_indices=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ml_core == 'deep learning':\n",
    "    # Ignore the indeces, we only care about the dataloaders when using neural networks\n",
    "    del train_indeces\n",
    "    del val_indeces\n",
    "    del test_indeces\n",
    "else:\n",
    "    # Get the full arrays of each set\n",
    "    train_features, train_labels = dataset.X[train_indeces], dataset.y[train_indeces]\n",
    "    val_features, val_labels = dataset.X[val_indeces], dataset.y[val_indeces]\n",
    "    test_features, test_labels = dataset.X[test_indeces], dataset.y[test_indeces]\n",
    "    # Remove the ID and timestamp columns from the data arrays\n",
    "    train_features = train_features[:, :, 2:]\n",
    "    val_features = val_features[:, :, 2:]\n",
    "    test_features = test_features[:, :, 2:]\n",
    "    # Reshape the data into a 2D format\n",
    "    train_features = train_features.reshape(-1, train_features.shape[-1])\n",
    "    val_features = val_features.reshape(-1, val_features.shape[-1])\n",
    "    test_features = test_features.reshape(-1, test_features.shape[-1])\n",
    "    train_labels = train_labels.reshape(-1)\n",
    "    val_labels = val_labels.reshape(-1)\n",
    "    test_labels = test_labels.reshape(-1)\n",
    "    # Remove padding samples from the data\n",
    "    train_features = train_features[[padding_value not in row for row in train_features]]\n",
    "    val_features = val_features[[padding_value not in row for row in val_features]]\n",
    "    test_features = test_features[[padding_value not in row for row in test_features]]\n",
    "    train_labels = train_labels[[padding_value not in row for row in train_labels]]\n",
    "    val_labels = val_labels[[padding_value not in row for row in val_labels]]\n",
    "    test_labels = test_labels[[padding_value not in row for row in test_labels]]\n",
    "    # Convert from PyTorch tensor to NumPy array\n",
    "    train_features = train_features.numpy()\n",
    "    val_features = val_features.numpy()\n",
    "    test_features = test_features.numpy()\n",
    "    train_labels = train_labels.numpy()\n",
    "    val_labels = val_labels.numpy()\n",
    "    test_labels = test_labels.numpy()\n",
    "    # Ignore the dataloaders, we only care about the full arrays when using scikit-learn or XGBoost\n",
    "    del train_dataloader\n",
    "    del val_dataloader\n",
    "    del test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [],
   "source": [
    "if ml_core == 'deep learning':\n",
    "    print(next(iter(train_dataloader))[0])\n",
    "else:\n",
    "    print(train_features[:32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "if ml_core == 'deep learning':\n",
    "    print(next(iter(val_dataloader))[0])\n",
    "else:\n",
    "    print(val_features[:32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "if ml_core == 'deep learning':\n",
    "    print(next(iter(test_dataloader))[0])\n",
    "else:\n",
    "    print(test_features[:32])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Vanilla RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Creating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Model parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "n_hidden = 653                             # Number of hidden units\n",
    "n_layers = 2                               # Number of LSTM layers\n",
    "p_dropout = 0.4250806721766345             # Probability of dropout\n",
    "bidir = True                              # Sets if the RNN layer is bidirectional or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_delta_ts == 'normalized':\n",
    "    # Count the delta_ts column as another feature, only ignore ID, timestamp and label columns\n",
    "    n_inputs = n_inputs + 1\n",
    "elif use_delta_ts == 'raw':\n",
    "    raise Exception('ERROR: When using a model of type Vanilla RNN, we can\\'t use raw delta_ts. Please either normalize it (use_delta_ts = \"normalized\") or discard it (use_delta_ts = False).')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Instantiating the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [],
   "source": [
    "model = Models.VanillaRNN(n_inputs, n_hidden, n_outputs, n_layers, p_dropout,\n",
    "                          embed_features=embed_features, n_embeddings=n_embeddings,\n",
    "                          embedding_dim=embedding_dim, bidir=bidir, \n",
    "                          total_length=total_length)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the frozen, pre-trained embedding layer (if required):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset_mode == 'pre-embedded':\n",
    "    # Replace the embedding layer with the pre-trained one\n",
    "    model.embed_layers = pretrained_embed_layers\n",
    "    # Freeze the pre-trained embedding layer (i.e. stop it from being trained / changed)\n",
    "    for param in model.embed_layers.parameters():\n",
    "        param.requires_grad = False\n",
    "    print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the name that will be given to the models that will be saved:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'rnn'\n",
    "if bidir is True:\n",
    "    model_name = model_name + '_bidir'\n",
    "if dataset_mode == 'pre-embedded':\n",
    "    model_name = model_name + '_pre_embedded'\n",
    "elif dataset_mode == 'learn embedding':\n",
    "    model_name = model_name + '_with_embedding'\n",
    "elif dataset_mode == 'one hot encoded':\n",
    "    model_name = model_name + '_one_hot_encoded'\n",
    "if use_delta_ts is not False:\n",
    "    model_name = model_name + '_delta_ts'\n",
    "model_name = model_name + f'_{time_window_days}dayswindow'\n",
    "model_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Training and testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "next(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [],
   "source": [
    "model = du.deep_learning.train(model, train_dataloader, val_dataloader, test_dataloader, dataset=dataset,\n",
    "                               padding_value=padding_value, batch_size=batch_size, n_epochs=n_epochs, lr=lr,\n",
    "                               models_path=f'{project_path}models/', model_name=model_name, ModelClass=Models.VanillaRNN,\n",
    "                               is_custom=False, do_test=True, metrics=metrics, log_comet_ml=True,\n",
    "                               comet_ml_api_key=comet_ml_api_key, comet_ml_project_name=comet_ml_project_name,\n",
    "                               comet_ml_workspace=comet_ml_workspace, comet_ml_save_model=True,\n",
    "                               already_embedded=already_embedded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "next(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_name = input('Hyperparameter optimization configuration file name:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss_min, exp_name_min = du.machine_learning.optimize_hyperparameters(Models.VanillaRNN,\n",
    "                                                                          train_dataloader=train_dataloader,\n",
    "                                                                          val_dataloader=val_dataloader,\n",
    "                                                                          test_dataloader=test_dataloader,\n",
    "                                                                          dataset=dataset,\n",
    "                                                                          config_name=config_name,\n",
    "                                                                          comet_ml_api_key=comet_ml_api_key,\n",
    "                                                                          comet_ml_project_name=comet_ml_project_name,\n",
    "                                                                          comet_ml_workspace=comet_ml_workspace,\n",
    "                                                                          n_inputs=n_inputs, id_column=id_column,\n",
    "                                                                          inst_column=ts_column,\n",
    "                                                                          id_columns_idx=[0, 1],\n",
    "                                                                          n_outputs=n_outputs, model_type='multivariate_rnn',\n",
    "                                                                          is_custom=False, models_path=f'{project_path}models/',\n",
    "                                                                          model_name=model_name,\n",
    "                                                                          metrics=metrics,\n",
    "                                                                          config_path=f'{project_path}hyperparameter_optimization/',\n",
    "                                                                          var_seq=True, clip_value=0.5,\n",
    "                                                                          padding_value=padding_value,\n",
    "                                                                          batch_size=batch_size, n_epochs=n_epochs,\n",
    "                                                                          lr=lr,\n",
    "                                                                          comet_ml_save_model=True,\n",
    "                                                                          embed_features=embed_features,\n",
    "                                                                          n_embeddings=n_embeddings, \n",
    "                                                                          total_length=total_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name_min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Vanilla LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Creating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Model parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "n_hidden = 653                             # Number of hidden units\n",
    "n_layers = 2                               # Number of LSTM layers\n",
    "p_dropout = 0.4250806721766345             # Probability of dropout\n",
    "bidir = True                               # Sets if the RNN layer is bidirectional or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_delta_ts == 'normalized':\n",
    "    # Count the delta_ts column as another feature, only ignore ID, timestamp and label columns\n",
    "    n_inputs = n_inputs + 1\n",
    "elif use_delta_ts == 'raw':\n",
    "    raise Exception('ERROR: When using a model of type Vanilla RNN, we can\\'t use raw delta_ts. Please either normalize it (use_delta_ts = \"normalized\") or discard it (use_delta_ts = False).')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Instantiating the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [],
   "source": [
    "model = Models.VanillaLSTM(n_inputs, n_hidden, n_outputs, n_layers, p_dropout,\n",
    "                           embed_features=embed_features, n_embeddings=n_embeddings,\n",
    "                           embedding_dim=embedding_dim, bidir=bidir, \n",
    "                           total_length=total_length)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the frozen, pre-trained embedding layer (if required):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset_mode == 'pre-embedded':\n",
    "    # Replace the embedding layer with the pre-trained one\n",
    "    model.embed_layers = pretrained_embed_layers\n",
    "    # Freeze the pre-trained embedding layer (i.e. stop it from being trained / changed)\n",
    "    for param in model.embed_layers.parameters():\n",
    "        param.requires_grad = False\n",
    "    print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the name that will be given to the models that will be saved:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'lstm'\n",
    "if bidir is True:\n",
    "    model_name = model_name + '_bidir'\n",
    "if dataset_mode == 'pre-embedded':\n",
    "    model_name = model_name + '_pre_embedded'\n",
    "elif dataset_mode == 'learn embedding':\n",
    "    model_name = model_name + '_with_embedding'\n",
    "elif dataset_mode == 'one hot encoded':\n",
    "    model_name = model_name + '_one_hot_encoded'\n",
    "if use_delta_ts is not False:\n",
    "    model_name = model_name + '_delta_ts'\n",
    "model_name = model_name + f'_{time_window_days}dayswindow'\n",
    "model_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Training and testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "next(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [],
   "source": [
    "model = du.deep_learning.train(model, train_dataloader, val_dataloader, test_dataloader, dataset=dataset,\n",
    "                               padding_value=padding_value, batch_size=batch_size, n_epochs=n_epochs, lr=lr,\n",
    "                               models_path=f'{project_path}models/', model_name=model_name, ModelClass=Models.VanillaLSTM,\n",
    "                               is_custom=False, do_test=True, metrics=metrics, log_comet_ml=True,\n",
    "                               comet_ml_api_key=comet_ml_api_key, comet_ml_project_name=comet_ml_project_name,\n",
    "                               comet_ml_workspace=comet_ml_workspace, comet_ml_save_model=True,\n",
    "                               already_embedded=already_embedded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "next(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_name = input('Hyperparameter optimization configuration file name:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss_min, exp_name_min = du.machine_learning.optimize_hyperparameters(Models.VanillaLSTM,\n",
    "                                                                          train_dataloader=train_dataloader,\n",
    "                                                                          val_dataloader=val_dataloader,\n",
    "                                                                          test_dataloader=test_dataloader,\n",
    "                                                                          dataset=dataset,\n",
    "                                                                          config_name=config_name,\n",
    "                                                                          comet_ml_api_key=comet_ml_api_key,\n",
    "                                                                          comet_ml_project_name=comet_ml_project_name,\n",
    "                                                                          comet_ml_workspace=comet_ml_workspace,\n",
    "                                                                          n_inputs=n_inputs, id_column=id_column,\n",
    "                                                                          inst_column=ts_column,\n",
    "                                                                          id_columns_idx=[0, 1],\n",
    "                                                                          n_outputs=n_outputs, model_type='multivariate_rnn',\n",
    "                                                                          is_custom=False, models_path=f'{project_path}models/',\n",
    "                                                                          model_name=model_name,\n",
    "                                                                          array_param='embedding_dim',\n",
    "                                                                          metrics=metrics,\n",
    "                                                                          config_path=f'{project_path}hyperparameter_optimization/',\n",
    "                                                                          var_seq=True, clip_value=0.5,\n",
    "                                                                          padding_value=padding_value,\n",
    "                                                                          batch_size=batch_size, n_epochs=n_epochs,\n",
    "                                                                          lr=lr,\n",
    "                                                                          comet_ml_save_model=True,\n",
    "                                                                          embed_features=embed_features,\n",
    "                                                                          n_embeddings=n_embeddings, \n",
    "                                                                          total_length=total_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name_min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T-LSTM\n",
    "\n",
    "Implementation of the [_Patient Subtyping via Time-Aware LSTM Networks_](http://biometrics.cse.msu.edu/Publications/MachineLearning/Baytasetal_PatientSubtypingViaTimeAwareLSTMNetworks.pdf) paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Creating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Model parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "n_hidden = 653                             # Number of hidden units\n",
    "n_layers = 2                               # Number of LSTM layers\n",
    "p_dropout = 0.4250806721766345             # Probability of dropout\n",
    "bidir = False                              # Sets if the RNN layer is bidirectional or not\n",
    "elapsed_time = 'small'                     # Indicates if the elapsed time between events is small or long; influences how to discount elapsed time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_delta_ts == 'raw':\n",
    "    raise Exception('ERROR: When using a model of type TLSTM, we can\\'t use raw delta_ts. Please normalize it (use_delta_ts = \"normalized\").')\n",
    "elif use_delta_ts is False:\n",
    "    raise Exception('ERROR: When using a model of type TLSTM, we must use delta_ts. Please use it, in a normalized version (use_delta_ts = \"normalized\").')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Instantiating the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [],
   "source": [
    "model = Models.TLSTM(n_inputs, n_hidden, n_outputs, n_layers, p_dropout,\n",
    "                     embed_features=embed_features, n_embeddings=n_embeddings,\n",
    "                     embedding_dim=embedding_dim, elapsed_time=elapsed_time,\n",
    "                     bidir=bidir)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the frozen, pre-trained embedding layer (if required):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset_mode == 'pre-embedded':\n",
    "    # Replace the embedding layer with the pre-trained one\n",
    "    model.embed_layers = pretrained_embed_layers\n",
    "    # Freeze the pre-trained embedding layer (i.e. stop it from being trained / changed)\n",
    "    for param in model.embed_layers.parameters():\n",
    "        param.requires_grad = False\n",
    "    print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the name that will be given to the models that will be saved:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'tlstm'\n",
    "if bidir is True:\n",
    "    model_name = model_name + '_bidir'\n",
    "if dataset_mode == 'pre-embedded':\n",
    "    model_name = model_name + '_pre_embedded'\n",
    "elif dataset_mode == 'learn embedding':\n",
    "    model_name = model_name + '_with_embedding'\n",
    "elif dataset_mode == 'one hot encoded':\n",
    "    model_name = model_name + '_one_hot_encoded'\n",
    "model_name = model_name + f'_{time_window_days}dayswindow'\n",
    "model_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Training and testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "next(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [],
   "source": [
    "model = du.deep_learning.train(model, train_dataloader, val_dataloader, test_dataloader, dataset=dataset,\n",
    "                               padding_value=padding_value, batch_size=batch_size, n_epochs=n_epochs, lr=lr,\n",
    "                               models_path=f'{project_path}models/', model_name=model_name, ModelClass=Models.TLSTM,\n",
    "                               is_custom=True, do_test=True, metrics=metrics, log_comet_ml=True,\n",
    "                               comet_ml_api_key=comet_ml_api_key, comet_ml_project_name=comet_ml_project_name,\n",
    "                               comet_ml_workspace=comet_ml_workspace, comet_ml_save_model=True,\n",
    "                               already_embedded=already_embedded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "next(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_name = input('Hyperparameter optimization configuration file name:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss_min, exp_name_min = du.machine_learning.optimize_hyperparameters(Models.TLSTM,\n",
    "                                                                          train_dataloader=train_dataloader,\n",
    "                                                                          val_dataloader=val_dataloader,\n",
    "                                                                          test_dataloader=test_dataloader,\n",
    "                                                                          dataset=dataset,\n",
    "                                                                          config_name=config_name,\n",
    "                                                                          comet_ml_api_key=comet_ml_api_key,\n",
    "                                                                          comet_ml_project_name=comet_ml_project_name,\n",
    "                                                                          comet_ml_workspace=comet_ml_workspace,\n",
    "                                                                          n_inputs=n_inputs, id_column=id_column,\n",
    "                                                                          inst_column=ts_column,\n",
    "                                                                          id_columns_idx=[0, 1],\n",
    "                                                                          n_outputs=n_outputs, model_type='multivariate_rnn',\n",
    "                                                                          is_custom=True, models_path=f'{project_path}models/',\n",
    "                                                                          model_name=model_name,\n",
    "                                                                          array_param='embedding_dim',\n",
    "                                                                          metrics=metrics,\n",
    "                                                                          config_path=f'{project_path}hyperparameter_optimization/',\n",
    "                                                                          var_seq=True, clip_value=0.5,\n",
    "                                                                          padding_value=padding_value,\n",
    "                                                                          batch_size=batch_size, n_epochs=n_epochs,\n",
    "                                                                          lr=lr,\n",
    "                                                                          comet_ml_save_model=True,\n",
    "                                                                          embed_features=embed_features,\n",
    "                                                                          n_embeddings=n_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name_min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MF1-LSTM\n",
    "\n",
    "Implementation of the [_Predicting healthcare trajectories from medical records: A deep learning approach_](https://doi.org/10.1016/j.jbi.2017.04.001) paper, time decay version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Creating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Model parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "n_hidden = 653                             # Number of hidden units\n",
    "n_layers = 2                               # Number of LSTM layers\n",
    "p_dropout = 0.4250806721766345             # Probability of dropout\n",
    "bidir = False                              # Sets if the RNN layer is bidirectional or not\n",
    "elapsed_time = 'small'                     # Indicates if the elapsed time between events is small or long; influences how to discount elapsed time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_delta_ts == 'raw':\n",
    "    raise Exception('ERROR: When using a model of type MF1-LSTM, we can\\'t use raw delta_ts. Please normalize it (use_delta_ts = \"normalized\").')\n",
    "elif use_delta_ts is False:\n",
    "    raise Exception('ERROR: When using a model of type MF1-LSTM, we must use delta_ts. Please use it, in a normalized version (use_delta_ts = \"normalized\").')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Instantiating the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [],
   "source": [
    "model = Models.MF1LSTM(n_inputs, n_hidden, n_outputs, n_layers, p_dropout,\n",
    "                       embed_features=embed_features, n_embeddings=n_embeddings,\n",
    "                       embedding_dim=embedding_dim, elapsed_time=elapsed_time)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the frozen, pre-trained embedding layer (if required):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset_mode == 'pre-embedded':\n",
    "    # Replace the embedding layer with the pre-trained one\n",
    "    model.embed_layers = pretrained_embed_layers\n",
    "    # Freeze the pre-trained embedding layer (i.e. stop it from being trained / changed)\n",
    "    for param in model.embed_layers.parameters():\n",
    "        param.requires_grad = False\n",
    "    print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the name that will be given to the models that will be saved:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'mf1lstm'\n",
    "if bidir is True:\n",
    "    model_name = model_name + '_bidir'\n",
    "if dataset_mode == 'pre-embedded':\n",
    "    model_name = model_name + '_pre_embedded'\n",
    "elif dataset_mode == 'learn embedding':\n",
    "    model_name = model_name + '_with_embedding'\n",
    "elif dataset_mode == 'one hot encoded':\n",
    "    model_name = model_name + '_one_hot_encoded'\n",
    "model_name = model_name + f'_{time_window_days}dayswindow'\n",
    "model_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Training and testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "next(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [],
   "source": [
    "model = du.deep_learning.train(model, train_dataloader, val_dataloader, test_dataloader, dataset=dataset,\n",
    "                               padding_value=padding_value, batch_size=batch_size, n_epochs=n_epochs, lr=lr,\n",
    "                               models_path=f'{project_path}models/', model_name=model_name, ModelClass=Models.MF1LSTM,\n",
    "                               is_custom=True, do_test=True, metrics=metrics, log_comet_ml=True,\n",
    "                               comet_ml_api_key=comet_ml_api_key, comet_ml_project_name=comet_ml_project_name,\n",
    "                               comet_ml_workspace=comet_ml_workspace, comet_ml_save_model=True,\n",
    "                               already_embedded=already_embedded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "next(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_name = input('Hyperparameter optimization configuration file name:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss_min, exp_name_min = du.machine_learning.optimize_hyperparameters(Models.MF1LSTM,\n",
    "                                                                          train_dataloader=train_dataloader,\n",
    "                                                                          val_dataloader=val_dataloader,\n",
    "                                                                          test_dataloader=test_dataloader,\n",
    "                                                                          dataset=dataset,\n",
    "                                                                          config_name=config_name,\n",
    "                                                                          comet_ml_api_key=comet_ml_api_key,\n",
    "                                                                          comet_ml_project_name=comet_ml_project_name,\n",
    "                                                                          comet_ml_workspace=comet_ml_workspace,\n",
    "                                                                          n_inputs=n_inputs, id_column=id_column,\n",
    "                                                                          inst_column=ts_column,\n",
    "                                                                          id_columns_idx=[0, 1],\n",
    "                                                                          n_outputs=n_outputs, model_type='multivariate_rnn',\n",
    "                                                                          is_custom=True, models_path=f'{project_path}models/',\n",
    "                                                                          model_name=model_name,\n",
    "                                                                          array_param='embedding_dim',\n",
    "                                                                          metrics=metrics,\n",
    "                                                                          config_path=f'{project_path}hyperparameter_optimization/',\n",
    "                                                                          var_seq=True, clip_value=0.5,\n",
    "                                                                          padding_value=padding_value,\n",
    "                                                                          batch_size=batch_size, n_epochs=n_epochs,\n",
    "                                                                          lr=lr,\n",
    "                                                                          comet_ml_save_model=True,\n",
    "                                                                          embed_features=embed_features,\n",
    "                                                                          n_embeddings=n_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name_min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MF2-LSTM\n",
    "\n",
    "Implementation of the [_Predicting healthcare trajectories from medical records: A deep learning approach_](https://doi.org/10.1016/j.jbi.2017.04.001) paper, parametric time version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Creating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Model parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "n_hidden = 653                             # Number of hidden units\n",
    "n_layers = 2                               # Number of LSTM layers\n",
    "p_dropout = 0.4250806721766345             # Probability of dropout\n",
    "bidir = False                              # Sets if the RNN layer is bidirectional or not\n",
    "elapsed_time = 'small'                     # Indicates if the elapsed time between events is small or long; influences how to discount elapsed time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_delta_ts == 'normalized':\n",
    "    raise Exception('ERROR: When using a model of type MF2-LSTM, we can\\'t use normalized delta_ts. Please use it raw (use_delta_ts = \"raw\").')\n",
    "elif use_delta_ts is False:\n",
    "    raise Exception('ERROR: When using a model of type MF2-LSTM, we must use delta_ts. Please use it, in a raw version (use_delta_ts = \"raw\").')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Instantiating the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [],
   "source": [
    "model = Models.MF2LSTM(n_inputs, n_hidden, n_outputs, n_layers, p_dropout,\n",
    "                       embed_features=embed_features, n_embeddings=n_embeddings,\n",
    "                       embedding_dim=embedding_dim, elapsed_time=elapsed_time)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the frozen, pre-trained embedding layer (if required):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset_mode == 'pre-embedded':\n",
    "    # Replace the embedding layer with the pre-trained one\n",
    "    model.embed_layers = pretrained_embed_layers\n",
    "    # Freeze the pre-trained embedding layer (i.e. stop it from being trained / changed)\n",
    "    for param in model.embed_layers.parameters():\n",
    "        param.requires_grad = False\n",
    "    print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the name that will be given to the models that will be saved:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'mf2lstm'\n",
    "if bidir is True:\n",
    "    model_name = model_name + '_bidir'\n",
    "if dataset_mode == 'pre-embedded':\n",
    "    model_name = model_name + '_pre_embedded'\n",
    "elif dataset_mode == 'learn embedding':\n",
    "    model_name = model_name + '_with_embedding'\n",
    "elif dataset_mode == 'one hot encoded':\n",
    "    model_name = model_name + '_one_hot_encoded'\n",
    "model_name = model_name + f'_{time_window_days}dayswindow'\n",
    "model_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Training and testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "next(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [],
   "source": [
    "model = du.deep_learning.train(model, train_dataloader, val_dataloader, test_dataloader, dataset=dataset,\n",
    "                               padding_value=padding_value, batch_size=batch_size, n_epochs=n_epochs, lr=lr,\n",
    "                               models_path=f'{project_path}models/', model_name=model_name, ModelClass=Models.MF2LSTM,\n",
    "                               is_custom=True, do_test=True, metrics=metrics, log_comet_ml=True,\n",
    "                               comet_ml_api_key=comet_ml_api_key, comet_ml_project_name=comet_ml_project_name,\n",
    "                               comet_ml_workspace=comet_ml_workspace, comet_ml_save_model=True,\n",
    "                               already_embedded=already_embedded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "next(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_name = input('Hyperparameter optimization configuration file name:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss_min, exp_name_min = du.machine_learning.optimize_hyperparameters(Models.MF2LSTM,\n",
    "                                                                          train_dataloader=train_dataloader,\n",
    "                                                                          val_dataloader=val_dataloader,\n",
    "                                                                          test_dataloader=test_dataloader,\n",
    "                                                                          dataset=dataset,\n",
    "                                                                          config_name=config_name,\n",
    "                                                                          comet_ml_api_key=comet_ml_api_key,\n",
    "                                                                          comet_ml_project_name=comet_ml_project_name,\n",
    "                                                                          comet_ml_workspace=comet_ml_workspace,\n",
    "                                                                          n_inputs=n_inputs, id_column=id_column,\n",
    "                                                                          inst_column=ts_column,\n",
    "                                                                          id_columns_idx=[0, 1],\n",
    "                                                                          n_outputs=n_outputs, model_type='multivariate_rnn',\n",
    "                                                                          is_custom=True, models_path=f'{project_path}models/',\n",
    "                                                                          model_name=model_name,\n",
    "                                                                          array_param='embedding_dim',\n",
    "                                                                          metrics=metrics,\n",
    "                                                                          config_path=f'{project_path}hyperparameter_optimization/',\n",
    "                                                                          var_seq=True, clip_value=0.5,\n",
    "                                                                          padding_value=padding_value,\n",
    "                                                                          batch_size=batch_size, n_epochs=n_epochs,\n",
    "                                                                          lr=lr,\n",
    "                                                                          comet_ml_save_model=True,\n",
    "                                                                          embed_features=embed_features,\n",
    "                                                                          n_embeddings=n_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name_min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objective = 'binary:logistic'              # Objective function to minimize (in this case, logistic)\n",
    "eval_metric = 'logloss'                    # Metric to analyze (in this case, negative log likelihood loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = xgb.XGBClassifier(objective=objective, eval_metric=eval_metric, learning_rate=lr,\n",
    "                              num_class=1, n_estimators=50000, \n",
    "                              random_state=du.random_seed, seed=du.random_seed)\n",
    "xgb_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training with early stopping (stops training if the evaluation metric doesn't improve on 5 consequetive iterations):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model.fit(train_features, train_labels, early_stopping_rounds=5, eval_set=[(val_features, val_labels)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the validation loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_pred_proba = xgb_model.predict_proba(val_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss = log_loss(val_labels, val_pred_proba)\n",
    "val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current day and time to attach to the saved model's name\n",
    "current_datetime = datetime.now().strftime('%d_%m_%Y_%H_%M')\n",
    "# Filename and path where the model will be saved\n",
    "model_filename = f'{project_path}models/xgb_{val_loss:.4f}valloss_{current_datetime}.pth'\n",
    "# Save the model\n",
    "xgb_model.save_model(model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = xgb.XGBClassifier()\n",
    "xgb_model.load_model(model_filename)\n",
    "xgb_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train until the best iteration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_xgb_model = xgb_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_xgb_model = xgb.XGBClassifier(objective=objective, eval_metric=eval_metric, learning_rate=lr,\n",
    "#                                    num_class=1, n_estimators=xgb_model.best_iteration, \n",
    "#                                    random_state=du.random_seed, seed=du.random_seed)\n",
    "# best_xgb_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_xgb_model.fit(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the validation loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_pred_proba = best_xgb_model.predict_proba(val_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_loss = log_loss(val_labels, val_pred_proba)\n",
    "# val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get the current day and time to attach to the saved model's name\n",
    "# current_datetime = datetime.now().strftime('%d_%m_%Y_%H_%M')\n",
    "# # Filename and path where the model will be saved\n",
    "# model_filename = f'{models_path}xgb_{val_loss:.4f}valloss_{current_datetime}.pth'\n",
    "# # Save the model\n",
    "# best_xgb_model.save_model(model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_xgb_model = xgb.XGBClassifier()\n",
    "# best_xgb_model.load_model(model_filename)\n",
    "# best_xgb_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate on the validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_proba = best_xgb_model.predict_proba(val_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc = roc_auc_score(val_labels, pred_proba[:, 1])\n",
    "auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = best_xgb_model.predict(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = accuracy_score(test_labels, pred)\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = f1_score(test_labels, pred, average='weighted')\n",
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_proba = best_xgb_model.predict_proba(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = log_loss(test_labels, pred_proba)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc = roc_auc_score(test_labels, pred_proba[:, 1])\n",
    "auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solver = 'lbfgs'\n",
    "penalty = 'l2'\n",
    "C = 1\n",
    "max_iter = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_model = LogisticRegression(solver=solver, penalty=penalty, C=C, max_iter=max_iter, random_state=du.random_seed)\n",
    "logreg_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_model.fit(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the validation loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_pred_proba = logreg_model.predict_proba(val_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss = log_loss(val_labels, val_pred_proba)\n",
    "val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current day and time to attach to the saved model's name\n",
    "current_datetime = datetime.now().strftime('%d_%m_%Y_%H_%M')\n",
    "# Filename and path where the model will be saved\n",
    "model_filename = f'{project_path}models/logreg_{val_loss:.4f}valloss_{current_datetime}.pth'\n",
    "# Save the model\n",
    "joblib.dump(logreg_model, model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logreg_model = joblib.load(f'{models_path}logreg/checkpoint_16_12_2019_02_27.model')\n",
    "logreg_model = joblib.load(model_filename)\n",
    "logreg_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate on the validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_proba = logreg_model.predict_proba(val_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc = roc_auc_score(val_labels, pred_proba[:, 1])\n",
    "auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = logreg_model.score(test_features, test_labels)\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = logreg_model.predict(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = f1_score(test_labels, pred, average='weighted')\n",
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_proba = logreg_model.predict_proba(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = log_loss(test_labels, pred_proba)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc = roc_auc_score(test_labels, pred_proba[:, 1])\n",
    "auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_function_shape = 'ovo'\n",
    "C = 1\n",
    "kernel = 'rbf'\n",
    "max_iter = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_model = SVC(kernel=kernel, decision_function_shape=decision_function_shape, C=C,\n",
    "                max_iter=max_iter, probability=True, random_state=du.random_seed)\n",
    "svm_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_model.fit(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the validation loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_pred_proba = svm_model.predict_proba(val_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss = log_loss(val_labels, val_pred_proba)\n",
    "val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current day and time to attach to the saved model's name\n",
    "current_datetime = datetime.now().strftime('%d_%m_%Y_%H_%M')\n",
    "# Filename and path where the model will be saved\n",
    "model_filename = f'{project_path}models/svm_{val_loss:.4f}valloss_{current_datetime}.pth'\n",
    "# Save the model\n",
    "joblib.dump(svm_model, model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svm_model = joblib.load(f'{models_path}svm/checkpoint_16_12_2019_05_51.model')\n",
    "svm_model = joblib.load(model_filename)\n",
    "svm_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate on the validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_proba = svm_model.predict_proba(val_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc = roc_auc_score(val_labels, pred_proba[:, 1])\n",
    "auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = svm_model.score(test_features, test_labels)\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = svm_model.predict(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = f1_score(test_labels, pred, average='weighted')\n",
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_proba = svm_model.predict_proba(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = log_loss(test_labels, pred_proba)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc = roc_auc_score(test_labels, pred_proba[:, 1])\n",
    "auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter optimization"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-4.m49",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-4:m49"
  },
  "jupytext": {
   "cell_metadata_filter": "-all",
   "cell_metadata_json": true,
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "ipynb,py:light",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "fcul_als_disease_progression",
   "language": "python",
   "name": "fcul_als_disease_progression"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc-showcode": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
