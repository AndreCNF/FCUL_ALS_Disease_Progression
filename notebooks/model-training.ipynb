{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# eICU Model training\n",
    "---\n",
    "\n",
    "Training models on the preprocessed the eICU dataset from MIT, which has data from over 139k patients collected in the US."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os                                  # os handles directory/workspace changes\n",
    "import comet_ml                            # Comet.ml can log training metrics, parameters, do version control and parameter optimization\n",
    "import torch                               # PyTorch to create and apply deep learning models\n",
    "import xgboost as xgb                      # Gradient boosting trees models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, f1_score, log_loss, roc_auc_score\n",
    "import joblib                              # Save scikit-learn models in disk\n",
    "from datetime import datetime              # datetime to use proper date and time formats\n",
    "import yaml                                # Save and load YAML files\n",
    "import getpass                             # Get password or similar private inputs\n",
    "from ipywidgets import interact            # Display selectors and sliders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debugging packages\n",
    "import pixiedust                           # Debugging in Jupyter Notebook cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the parquet dataset files\n",
    "data_path = 'data/eICU/cleaned/'\n",
    "# Path to the code files\n",
    "project_path = 'code/eICU-mortality-prediction/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to the scripts directory\n",
    "os.chdir(\"../scripts/\")\n",
    "import utils                               # Context specific (in this case, for the eICU data) methods\n",
    "import Models                              # Deep learning models\n",
    "# Change to parent directory (presumably \"Documents\")\n",
    "os.chdir(\"../../..\")\n",
    "# import modin.pandas as pd                  # Optimized distributed version of Pandas\n",
    "import pandas as pd                        # Pandas to load and handle the data\n",
    "import data_utils as du                    # Data science and machine learning relevant methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "du.set_pandas_library(lib='pandas')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Allow pandas to show more columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 3000)\n",
    "pd.set_option('display.max_rows', 3000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the random seed for reproducibility:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "du.set_random_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comet ML settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comet_ml_project_name = input('Comet ML project name:')\n",
    "comet_ml_workspace = input('Comet ML workspace:')\n",
    "comet_ml_api_key = getpass.getpass('Comet ML API key')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_mode = None                        # The mode in which we'll use the data, either one hot encoded or pre-embedded\n",
    "ml_core = None                             # The core machine learning type we'll use; either traditional ML or DL\n",
    "use_delta_ts = None                        # Indicates if we'll use time variation info\n",
    "time_window_h = None                       # Number of hours on which we want to predict mortality\n",
    "already_embedded = None                    # Indicates if categorical features are already embedded when fetching a batch\n",
    "@interact\n",
    "def get_dataset_mode(data_mode=['one hot encoded', 'learn embedding', 'pre-embedded'],\n",
    "                     ml_or_dl=['deep learning', 'machine learning'],\n",
    "                     use_delta=[False, 'normalized', 'raw'], window_h=(0, 96, 24)):\n",
    "    global dataset_mode, ml_core, use_delta_ts, time_window_h, already_embedded\n",
    "    dataset_mode, ml_core, use_delta_ts, time_window_h = data_mode, ml_or_dl, use_delta, window_h\n",
    "    already_embedded = dataset_mode == 'embedded'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_column = 'patientunitstayid'            # Name of the sequence ID column\n",
    "ts_column = 'ts'                           # Name of the timestamp column\n",
    "label_column = 'label'                     # Name of the label column\n",
    "n_inputs = 2090                            # Number of input features\n",
    "n_outputs = 1                              # Number of outputs\n",
    "padding_value = 999999                     # Padding value used to fill in sequences up to the maximum sequence length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "stream_dtypes = open(f'{data_path}eICU_dtype_dict.yml', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "dtype_dict = yaml.load(stream_dtypes, Loader=yaml.FullLoader)\n",
    "dtype_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One hot encoding columns categorization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "stream_cat_feat_ohe = open(f'{data_path}eICU_cat_feat_ohe.yml', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "cat_feat_ohe = yaml.load(stream_cat_feat_ohe, Loader=yaml.FullLoader)\n",
    "cat_feat_ohe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(cat_feat_ohe.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Training parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_train_ratio = 0.25                    # Percentage of the data which will be used as a test set\n",
    "# validation_ratio = 0.1                     # Percentage of the data from the training set which is used for validation purposes\n",
    "batch_size = 32                            # Number of unit stays in a mini batch\n",
    "n_epochs = 10                              # Number of epochs\n",
    "lr = 0.001                                 # Learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_tvt_sets = open(f'{data_path}eICU_tvt_sets.yml', 'r')\n",
    "eICU_tvt_sets = yaml.load(stream_tvt_sets, Loader=yaml.FullLoader)\n",
    "eICU_tvt_sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ['loss', 'accuracy', 'AUC', 'AUC_weighted']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the dataset object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cat_feat_ohe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[feat_list for feat_list in cat_feat_ohe.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[[col for col in feat_list] for feat_list in [feat_list for feat_list in cat_feat_ohe.values()]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [],
   "source": [
    "dataset = du.datasets.Large_Dataset(files_name='eICU', process_pipeline=utils.eICU_process_pipeline,\n",
    "                                    id_column=id_column, initial_analysis=utils.eICU_initial_analysis,\n",
    "                                    files_path=data_path, dataset_mode=dataset_mode, ml_core=ml_core,\n",
    "                                    use_delta_ts=use_delta_ts, time_window_h=time_window_h,\n",
    "                                    padding_value=padding_value, cat_feat_ohe=cat_feat_ohe, dtype_dict=dtype_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure that we discard the ID, timestamp and label columns\n",
    "if n_inputs != dataset.n_inputs:\n",
    "    n_inputs = dataset.n_inputs\n",
    "    print(f'Changed the number of inputs to {n_inputs}')\n",
    "else:\n",
    "    n_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset_mode == 'learn embedding':\n",
    "    embed_features = dataset.embed_features\n",
    "    n_embeddings = dataset.n_embeddings\n",
    "else:\n",
    "    embed_features = None\n",
    "    n_embeddings = None\n",
    "print(f'Embedding features: {embed_features}')\n",
    "print(f'Number of embeddings: {n_embeddings}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "dataset.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.bool_feat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separating into train and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "(train_dataloader, val_dataloader, test_dataloader,\n",
    "train_indeces, val_indeces, test_indeces) = du.machine_learning.create_train_sets(dataset,\n",
    "#                                                                                   test_train_ratio=test_train_ratio,\n",
    "#                                                                                   validation_ratio=validation_ratio,\n",
    "                                                                                  train_indices=eICU_tvt_sets['train_indices'],\n",
    "                                                                                  val_indices=eICU_tvt_sets['val_indices'],\n",
    "                                                                                  test_indices=eICU_tvt_sets['test_indices'],\n",
    "                                                                                  batch_size=batch_size,\n",
    "                                                                                  get_indeces=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ml_core == 'deep learning':\n",
    "    # Ignore the indeces, we only care about the dataloaders when using neural networks\n",
    "    del train_indeces\n",
    "    del val_indeces\n",
    "    del test_indeces\n",
    "else:\n",
    "    # Get the full arrays of each set\n",
    "    train_features, train_labels = dataset.X[train_indeces], dataset.y[train_indeces]\n",
    "    val_features, val_labels = dataset.X[val_indeces], dataset.y[val_indeces]\n",
    "    test_features, test_labels = dataset.X[test_indeces], dataset.y[test_indeces]\n",
    "    # Ignore the dataloaders, we only care about the full arrays when using scikit-learn or XGBoost\n",
    "    del train_dataloaders\n",
    "    del val_dataloaders\n",
    "    del test_dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [],
   "source": [
    "if ml_core == 'deep learning':\n",
    "    print(next(iter(train_dataloader))[0])\n",
    "else:\n",
    "    print(train_features[:32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "if ml_core == 'deep learning':\n",
    "    print(next(iter(val_dataloader))[0])\n",
    "else:\n",
    "    print(val_features[:32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "if ml_core == 'deep learning':\n",
    "    print(next(iter(test_dataloader))[0])\n",
    "else:\n",
    "    print(test_features[:32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(test_dataloader))[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Vanilla RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Creating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Model parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "n_hidden = 100                             # Number of hidden units\n",
    "n_layers = 2                               # Number of LSTM layers\n",
    "p_dropout = 0.2                            # Probability of dropout\n",
    "bidir = False                              # Sets if the RNN layer is bidirectional or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_delta_ts == 'normalized':\n",
    "    # Count the delta_ts column as another feature, only ignore ID, timestamp and label columns\n",
    "    n_inputs = dataset.n_inputs + 1\n",
    "elif use_delta_ts == 'raw':\n",
    "    raise Exception('ERROR: When using a model of type Vanilla RNN, we can\\'t use raw delta_ts. Please either normalize it (use_delta_ts = \"normalized\") or discard it (use_delta_ts = False).')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Instantiating the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [],
   "source": [
    "model = Models.VanillaRNN(n_inputs, n_hidden, n_outputs, n_layers, p_dropout,\n",
    "                          embed_features=embed_features, n_embeddings=n_embeddings,\n",
    "                          embedding_dim=embedding_dim, bidir=bidir)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the name that will be given to the models that will be saved:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'rnn'\n",
    "if dataset_mode == 'pre-embedded':\n",
    "    model_name = model_name + '_pre_embedded'\n",
    "elif dataset_mode == 'learn embedding':\n",
    "    model_name = model_name + '_with_embedding'\n",
    "elif dataset_mode == 'one hot encoded':\n",
    "    model_name = model_name + '_one_hot_encoded'\n",
    "if use_delta_ts is not False:\n",
    "    model_name = model_name + '_delta_ts'\n",
    "model_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Training and testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "next(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [],
   "source": [
    "model = du.deep_learning.train(model, train_dataloader, val_dataloader, test_dataloader, dataset=dataset,\n",
    "                               padding_value=padding_value, batch_size=batch_size, n_epochs=n_epochs, lr=lr,\n",
    "                               models_path=f'{project_path}models/', model_name=model_name, ModelClass=Models.VanillaRNN,\n",
    "                               is_custom=False, do_test=True, metrics=metrics, log_comet_ml=True,\n",
    "                               comet_ml_api_key=comet_ml_api_key, comet_ml_project_name=comet_ml_project_name,\n",
    "                               comet_ml_workspace=comet_ml_workspace, comet_ml_save_model=True,\n",
    "                               already_embedded=already_embedded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "next(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_name = input('Hyperparameter optimization configuration file name:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss_min, exp_name_min = du.machine_learning.optimize_hyperparameters(Models.VanillaRNN,\n",
    "                                                                          train_dataloader=train_dataloader,\n",
    "                                                                          val_dataloader=val_dataloader,\n",
    "                                                                          test_dataloader=test_dataloader,\n",
    "                                                                          dataset=dataset,\n",
    "                                                                          config_name=config_name,\n",
    "                                                                          comet_ml_api_key=comet_ml_api_key,\n",
    "                                                                          comet_ml_project_name=comet_ml_project_name,\n",
    "                                                                          comet_ml_workspace=comet_ml_workspace,\n",
    "                                                                          n_inputs=n_inputs, id_column=id_column,\n",
    "                                                                          inst_column=ts_column,\n",
    "                                                                          id_columns_idx=[0, 1],\n",
    "                                                                          n_outputs=n_outputs, model_type='multivariate_rnn',\n",
    "                                                                          is_custom=False, models_path='models/',\n",
    "                                                                          model_name=model_name,\n",
    "                                                                          array_param='embedding_dim',\n",
    "                                                                          metrics=metrics,\n",
    "                                                                          config_path=f'{project_path}hyperparameter_optimization/',\n",
    "                                                                          var_seq=True, clip_value=0.5,\n",
    "                                                                          padding_value=padding_value,\n",
    "                                                                          batch_size=batch_size, n_epochs=n_epochs,\n",
    "                                                                          lr=lr,\n",
    "                                                                          comet_ml_save_model=True,\n",
    "                                                                          embed_features=embed_features,\n",
    "                                                                          n_embeddings=n_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name_min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Vanilla LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Creating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Model parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "n_hidden = 100                             # Number of hidden units\n",
    "n_layers = 2                               # Number of LSTM layers\n",
    "p_dropout = 0.2                            # Probability of dropout\n",
    "bidir = False                              # Sets if the RNN layer is bidirectional or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_delta_ts == 'normalized':\n",
    "    # Count the delta_ts column as another feature, only ignore ID, timestamp and label columns\n",
    "    n_inputs = dataset.n_inputs + 1\n",
    "elif use_delta_ts == 'raw':\n",
    "    raise Exception('ERROR: When using a model of type Vanilla RNN, we can\\'t use raw delta_ts. Please either normalize it (use_delta_ts = \"normalized\") or discard it (use_delta_ts = False).')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Instantiating the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [],
   "source": [
    "model = Models.VanillaLSTM(n_inputs, n_hidden, n_outputs, n_layers, p_dropout,\n",
    "                           embed_features=embed_features, n_embeddings=n_embeddings,\n",
    "                           embedding_dim=embedding_dim, bidir=bidir)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the name that will be given to the models that will be saved:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'lstm'\n",
    "if dataset_mode == 'pre-embedded':\n",
    "    model_name = model_name + '_pre_embedded'\n",
    "elif dataset_mode == 'learn embedding':\n",
    "    model_name = model_name + '_with_embedding'\n",
    "elif dataset_mode == 'one hot encoded':\n",
    "    model_name = model_name + '_one_hot_encoded'\n",
    "if use_delta_ts is not False:\n",
    "    model_name = model_name + '_delta_ts'\n",
    "model_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Training and testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "next(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [],
   "source": [
    "model = du.deep_learning.train(model, train_dataloader, val_dataloader, test_dataloader, dataset=dataset,\n",
    "                               padding_value=padding_value, batch_size=batch_size, n_epochs=n_epochs, lr=lr,\n",
    "                               models_path=f'{project_path}models/', model_name=model_name, ModelClass=Models.VanillaLSTM,\n",
    "                               is_custom=False, do_test=True, metrics=metrics, log_comet_ml=True,\n",
    "                               comet_ml_api_key=comet_ml_api_key, comet_ml_project_name=comet_ml_project_name,\n",
    "                               comet_ml_workspace=comet_ml_workspace, comet_ml_save_model=True,\n",
    "                               already_embedded=already_embedded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "next(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_name = input('Hyperparameter optimization configuration file name:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss_min, exp_name_min = du.machine_learning.optimize_hyperparameters(Models.VanillaLSTM,\n",
    "                                                                          train_dataloader=train_dataloader,\n",
    "                                                                          val_dataloader=val_dataloader,\n",
    "                                                                          test_dataloader=test_dataloader,\n",
    "                                                                          dataset=dataset,\n",
    "                                                                          config_name=config_name,\n",
    "                                                                          comet_ml_api_key=comet_ml_api_key,\n",
    "                                                                          comet_ml_project_name=comet_ml_project_name,\n",
    "                                                                          comet_ml_workspace=comet_ml_workspace,\n",
    "                                                                          n_inputs=n_inputs, id_column=id_column,\n",
    "                                                                          inst_column=ts_column,\n",
    "                                                                          id_columns_idx=[0, 1],\n",
    "                                                                          n_outputs=n_outputs, model_type='multivariate_rnn',\n",
    "                                                                          is_custom=False, models_path='models/',\n",
    "                                                                          model_name=model_name,\n",
    "                                                                          array_param='embedding_dim',\n",
    "                                                                          metrics=metrics,\n",
    "                                                                          config_path=f'{project_path}hyperparameter_optimization/',\n",
    "                                                                          var_seq=True, clip_value=0.5,\n",
    "                                                                          padding_value=padding_value,\n",
    "                                                                          batch_size=batch_size, n_epochs=n_epochs,\n",
    "                                                                          lr=lr,\n",
    "                                                                          comet_ml_save_model=True,\n",
    "                                                                          embed_features=embed_features,\n",
    "                                                                          n_embeddings=n_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name_min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T-LSTM\n",
    "\n",
    "Implementation of the [_Patient Subtyping via Time-Aware LSTM Networks_](http://biometrics.cse.msu.edu/Publications/MachineLearning/Baytasetal_PatientSubtypingViaTimeAwareLSTMNetworks.pdf) paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Creating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Model parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "n_hidden = 100                             # Number of hidden units\n",
    "n_rnn_layers = 2                           # Number of TLSTM layers\n",
    "p_dropout = 0.2                            # Probability of dropout\n",
    "elapsed_time = 'small'                     # Indicates if the elapsed time between events is small or long; influences how to discount elapsed time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_delta_ts == 'raw':\n",
    "    raise Exception('ERROR: When using a model of type TLSTM, we can\\'t use raw delta_ts. Please normalize it (use_delta_ts = \"normalized\").')\n",
    "elif use_delta_ts is False:\n",
    "    raise Exception('ERROR: When using a model of type TLSTM, we must use delta_ts. Please use it, in a normalized version (use_delta_ts = \"normalized\").')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Instantiating the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [],
   "source": [
    "model = Models.TLSTM(n_inputs, n_hidden, n_outputs, n_rnn_layers, p_dropout,\n",
    "                     embed_features=embed_features, n_embeddings=n_embeddings,\n",
    "                     embedding_dim=embedding_dim, elapsed_time=elapsed_time)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the name that will be given to the models that will be saved:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'tlstm'\n",
    "if dataset_mode == 'pre-embedded':\n",
    "    model_name = model_name + '_pre_embedded'\n",
    "elif dataset_mode == 'learn embedding':\n",
    "    model_name = model_name + '_with_embedding'\n",
    "elif dataset_mode == 'one hot encoded':\n",
    "    model_name = model_name + '_one_hot_encoded'\n",
    "if use_delta_ts is not False:\n",
    "    model_name = model_name + '_delta_ts'\n",
    "model_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Training and testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "next(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [],
   "source": [
    "model = du.deep_learning.train(model, train_dataloader, val_dataloader, test_dataloader, dataset=dataset,\n",
    "                               padding_value=padding_value, batch_size=batch_size, n_epochs=n_epochs, lr=lr,\n",
    "                               models_path=f'{project_path}models/', model_name=model_name, ModelClass=Models.TLSTM,\n",
    "                               is_custom=True, do_test=True, metrics=metrics, log_comet_ml=True,\n",
    "                               comet_ml_api_key=comet_ml_api_key, comet_ml_project_name=comet_ml_project_name,\n",
    "                               comet_ml_workspace=comet_ml_workspace, comet_ml_save_model=True,\n",
    "                               already_embedded=already_embedded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "next(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_name = input('Hyperparameter optimization configuration file name:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss_min, exp_name_min = du.machine_learning.optimize_hyperparameters(Models.TLSTM,\n",
    "                                                                          train_dataloader=train_dataloader,\n",
    "                                                                          val_dataloader=val_dataloader,\n",
    "                                                                          test_dataloader=test_dataloader,\n",
    "                                                                          dataset=dataset,\n",
    "                                                                          config_name=config_name,\n",
    "                                                                          comet_ml_api_key=comet_ml_api_key,\n",
    "                                                                          comet_ml_project_name=comet_ml_project_name,\n",
    "                                                                          comet_ml_workspace=comet_ml_workspace,\n",
    "                                                                          n_inputs=n_inputs, id_column=id_column,\n",
    "                                                                          inst_column=ts_column,\n",
    "                                                                          id_columns_idx=[0, 1],\n",
    "                                                                          n_outputs=n_outputs, model_type='multivariate_rnn',\n",
    "                                                                          is_custom=True, models_path='models/',\n",
    "                                                                          model_name=model_name,\n",
    "                                                                          array_param='embedding_dim',\n",
    "                                                                          metrics=metrics,\n",
    "                                                                          config_path=f'{project_path}hyperparameter_optimization/',\n",
    "                                                                          var_seq=True, clip_value=0.5,\n",
    "                                                                          padding_value=padding_value,\n",
    "                                                                          batch_size=batch_size, n_epochs=n_epochs,\n",
    "                                                                          lr=lr,\n",
    "                                                                          comet_ml_save_model=True,\n",
    "                                                                          embed_features=embed_features,\n",
    "                                                                          n_embeddings=n_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name_min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MF1-LSTM\n",
    "\n",
    "Implementation of the [_Predicting healthcare trajectories from medical records: A deep learning approach_](https://doi.org/10.1016/j.jbi.2017.04.001) paper, time decay version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Creating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Model parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "n_hidden = 100                             # Number of hidden units\n",
    "n_rnn_layers = 2                           # Number of MF1-LSTM layers\n",
    "p_dropout = 0.2                            # Probability of dropout\n",
    "elapsed_time = 'small'                     # Indicates if the elapsed time between events is small or long; influences how to discount elapsed time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_delta_ts == 'raw':\n",
    "    raise Exception('ERROR: When using a model of type MF1-LSTM, we can\\'t use raw delta_ts. Please normalize it (use_delta_ts = \"normalized\").')\n",
    "elif use_delta_ts is False:\n",
    "    raise Exception('ERROR: When using a model of type MF1-LSTM, we must use delta_ts. Please use it, in a normalized version (use_delta_ts = \"normalized\").')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Instantiating the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [],
   "source": [
    "model = Models.MF1LSTM(n_inputs, n_hidden, n_outputs, n_rnn_layers, p_dropout,\n",
    "                       embed_features=embed_features, n_embeddings=n_embeddings,\n",
    "                       embedding_dim=embedding_dim, elapsed_time=elapsed_time)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the name that will be given to the models that will be saved:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'mf1lstm'\n",
    "if dataset_mode == 'pre-embedded':\n",
    "    model_name = model_name + '_pre_embedded'\n",
    "elif dataset_mode == 'learn embedding':\n",
    "    model_name = model_name + '_with_embedding'\n",
    "elif dataset_mode == 'one hot encoded':\n",
    "    model_name = model_name + '_one_hot_encoded'\n",
    "if use_delta_ts is not False:\n",
    "    model_name = model_name + '_delta_ts'\n",
    "model_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Training and testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "next(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [],
   "source": [
    "model = du.deep_learning.train(model, train_dataloader, val_dataloader, test_dataloader, dataset=dataset,\n",
    "                               padding_value=padding_value, batch_size=batch_size, n_epochs=n_epochs, lr=lr,\n",
    "                               models_path=f'{project_path}models/', model_name=model_name, ModelClass=Models.MF1LSTM,\n",
    "                               is_custom=True, do_test=True, metrics=metrics, log_comet_ml=True,\n",
    "                               comet_ml_api_key=comet_ml_api_key, comet_ml_project_name=comet_ml_project_name,\n",
    "                               comet_ml_workspace=comet_ml_workspace, comet_ml_save_model=True,\n",
    "                               already_embedded=already_embedded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "next(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_name = input('Hyperparameter optimization configuration file name:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss_min, exp_name_min = du.machine_learning.optimize_hyperparameters(Models.MF1LSTM,\n",
    "                                                                          train_dataloader=train_dataloader,\n",
    "                                                                          val_dataloader=val_dataloader,\n",
    "                                                                          test_dataloader=test_dataloader,\n",
    "                                                                          dataset=dataset,\n",
    "                                                                          config_name=config_name,\n",
    "                                                                          comet_ml_api_key=comet_ml_api_key,\n",
    "                                                                          comet_ml_project_name=comet_ml_project_name,\n",
    "                                                                          comet_ml_workspace=comet_ml_workspace,\n",
    "                                                                          n_inputs=n_inputs, id_column=id_column,\n",
    "                                                                          inst_column=ts_column,\n",
    "                                                                          id_columns_idx=[0, 1],\n",
    "                                                                          n_outputs=n_outputs, model_type='multivariate_rnn',\n",
    "                                                                          is_custom=True, models_path='models/',\n",
    "                                                                          model_name=model_name,\n",
    "                                                                          array_param='embedding_dim',\n",
    "                                                                          metrics=metrics,\n",
    "                                                                          config_path=f'{project_path}hyperparameter_optimization/',\n",
    "                                                                          var_seq=True, clip_value=0.5,\n",
    "                                                                          padding_value=padding_value,\n",
    "                                                                          batch_size=batch_size, n_epochs=n_epochs,\n",
    "                                                                          lr=lr,\n",
    "                                                                          comet_ml_save_model=True,\n",
    "                                                                          embed_features=embed_features,\n",
    "                                                                          n_embeddings=n_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name_min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MF2-LSTM\n",
    "\n",
    "Implementation of the [_Predicting healthcare trajectories from medical records: A deep learning approach_](https://doi.org/10.1016/j.jbi.2017.04.001) paper, parametric time version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Creating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Model parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "n_hidden = 100                             # Number of hidden units\n",
    "n_rnn_layers = 2                           # Number of MF2-LSTM layers\n",
    "p_dropout = 0.2                            # Probability of dropout\n",
    "elapsed_time = 'small'                     # Indicates if the elapsed time between events is small or long; influences how to discount elapsed time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_delta_ts == 'normalized':\n",
    "    raise Exception('ERROR: When using a model of type MF2-LSTM, we can\\'t use normalized delta_ts. Please use it raw (use_delta_ts = \"raw\").')\n",
    "elif use_delta_ts is False:\n",
    "    raise Exception('ERROR: When using a model of type MF2-LSTM, we must use delta_ts. Please use it, in a raw version (use_delta_ts = \"raw\").')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Instantiating the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [],
   "source": [
    "model = Models.MF2LSTM(n_inputs, n_hidden, n_outputs, n_rnn_layers, p_dropout,\n",
    "                       embed_features=embed_features, n_embeddings=n_embeddings,\n",
    "                       embedding_dim=embedding_dim, elapsed_time=elapsed_time)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the name that will be given to the models that will be saved:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'mf2lstm'\n",
    "if dataset_mode == 'pre-embedded':\n",
    "    model_name = model_name + '_pre_embedded'\n",
    "elif dataset_mode == 'learn embedding':\n",
    "    model_name = model_name + '_with_embedding'\n",
    "elif dataset_mode == 'one hot encoded':\n",
    "    model_name = model_name + '_one_hot_encoded'\n",
    "if use_delta_ts is not False:\n",
    "    model_name = model_name + '_delta_ts'\n",
    "model_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Training and testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "next(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [],
   "source": [
    "model = du.deep_learning.train(model, train_dataloader, val_dataloader, test_dataloader, dataset=dataset,\n",
    "                               padding_value=padding_value, batch_size=batch_size, n_epochs=n_epochs, lr=lr,\n",
    "                               models_path=f'{project_path}models/', model_name=model_name, ModelClass=Models.MF2LSTM,\n",
    "                               is_custom=True, do_test=True, metrics=metrics, log_comet_ml=True,\n",
    "                               comet_ml_api_key=comet_ml_api_key, comet_ml_project_name=comet_ml_project_name,\n",
    "                               comet_ml_workspace=comet_ml_workspace, comet_ml_save_model=True,\n",
    "                               already_embedded=already_embedded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "next(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_name = input('Hyperparameter optimization configuration file name:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss_min, exp_name_min = du.machine_learning.optimize_hyperparameters(Models.MF2LSTM,\n",
    "                                                                          train_dataloader=train_dataloader,\n",
    "                                                                          val_dataloader=val_dataloader,\n",
    "                                                                          test_dataloader=test_dataloader,\n",
    "                                                                          dataset=dataset,\n",
    "                                                                          config_name=config_name,\n",
    "                                                                          comet_ml_api_key=comet_ml_api_key,\n",
    "                                                                          comet_ml_project_name=comet_ml_project_name,\n",
    "                                                                          comet_ml_workspace=comet_ml_workspace,\n",
    "                                                                          n_inputs=n_inputs, id_column=id_column,\n",
    "                                                                          inst_column=ts_column,\n",
    "                                                                          id_columns_idx=[0, 1],\n",
    "                                                                          n_outputs=n_outputs, model_type='multivariate_rnn',\n",
    "                                                                          is_custom=True, models_path='models/',\n",
    "                                                                          model_name=model_name,\n",
    "                                                                          array_param='embedding_dim',\n",
    "                                                                          metrics=metrics,\n",
    "                                                                          config_path=f'{project_path}hyperparameter_optimization/',\n",
    "                                                                          var_seq=True, clip_value=0.5,\n",
    "                                                                          padding_value=padding_value,\n",
    "                                                                          batch_size=batch_size, n_epochs=n_epochs,\n",
    "                                                                          lr=lr,\n",
    "                                                                          comet_ml_save_model=True,\n",
    "                                                                          embed_features=embed_features,\n",
    "                                                                          n_embeddings=n_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name_min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objective = 'multi:softmax'                # Objective function to minimize (in this case, softmax)\n",
    "eval_metric = 'mlogloss'                   # Metric to analyze (in this case, multioutput negative log likelihood loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = xgb.XGBClassifier(objective=objective, eval_metric=eval_metric, learning_rate=lr,\n",
    "                              num_class=n_output, random_state=du.random_seed, seed=du.random_seed)\n",
    "xgb_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training with early stopping (stops training if the evaluation metric doesn't improve on 5 consequetive iterations):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model.fit(train_features, train_labels, early_stopping_rounds=5, eval_set=[(val_features, val_labels)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the validation loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_pred_proba = xgb_model.predict_proba(val_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss = log_loss(val_labels, val_pred_proba)\n",
    "val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current day and time to attach to the saved model's name\n",
    "current_datetime = datetime.now().strftime('%d_%m_%Y_%H_%M')\n",
    "# Filename and path where the model will be saved\n",
    "model_filename = f'{models_path}xgb_{val_loss:.4f}valloss_{current_datetime}.pth'\n",
    "# Save the model\n",
    "joblib.dump(xgb_model, model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgb_model = joblib.load(f'{models_path}xgb/checkpoint_16_12_2019_11_39.model')\n",
    "xgb_model = joblib.load(model_filename)\n",
    "xgb_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train until the best iteration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = xgb.XGBClassifier(objective=objective, eval_metric='mlogloss', learning_rate=lr,\n",
    "                              num_class=n_class, random_state=du.random_seed, seed=du.random_seed)\n",
    "xgb_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model.fit(train_features, train_labels, early_stopping_rounds=5, num_boost_round=xgb_model.best_iteration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = xgb_model.predict(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = accuracy_score(test_labels, pred)\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = f1_score(test_labels, pred, average='weighted')\n",
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_proba = xgb_model.predict_proba(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = log_loss(test_labels, pred_proba)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc = roc_auc_score(test_labels, pred_proba, multi_class='ovr', average='weighted')\n",
    "auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solver = 'lbfgs'\n",
    "penalty = 'l2'\n",
    "C = 1\n",
    "max_iter = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_model = LogisticRegression(solver=solver, penalty=penalty, C=C, max_iter=max_iter, random_state=du.random_seed)\n",
    "logreg_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_model.fit(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the validation loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_pred_proba = logreg_model.predict_proba(val_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss = log_loss(val_labels, val_pred_proba)\n",
    "val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current day and time to attach to the saved model's name\n",
    "current_datetime = datetime.now().strftime('%d_%m_%Y_%H_%M')\n",
    "# Filename and path where the model will be saved\n",
    "model_filename = f'{models_path}logreg_{val_loss:.4f}valloss_{current_datetime}.pth'\n",
    "# Save the model\n",
    "joblib.dump(logreg_model, model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logreg_model = joblib.load(f'{models_path}logreg/checkpoint_16_12_2019_02_27.model')\n",
    "logreg_model = joblib.load(model_filename)\n",
    "logreg_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = logreg_model.score(test_features, test_labels)\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = logreg_model.predict(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = f1_score(test_labels, pred, average='weighted')\n",
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_proba = logreg_model.predict_proba(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = log_loss(test_labels, pred_proba)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc = roc_auc_score(test_labels, pred_proba, multi_class='ovr', average='weighted')\n",
    "auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_function_shape = 'ovo'\n",
    "C = 1\n",
    "kernel = 'rbf'\n",
    "max_iter = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_model = SVC(kernel=kernel, decision_function_shape=decision_function_shape, C=C,\n",
    "                max_iter=max_iter, probability=True, random_state=du.random_seed)\n",
    "svm_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_model.fit(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the validation loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_pred_proba = svm_model.predict_proba(val_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss = log_loss(val_labels, val_pred_proba)\n",
    "val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current day and time to attach to the saved model's name\n",
    "current_datetime = datetime.now().strftime('%d_%m_%Y_%H_%M')\n",
    "# Filename and path where the model will be saved\n",
    "model_filename = f'{models_path}svm_{val_loss:.4f}valloss_{current_datetime}.pth'\n",
    "# Save the model\n",
    "joblib.dump(svm_model, model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svm_model = joblib.load(f'{models_path}svm/checkpoint_16_12_2019_05_51.model')\n",
    "svm_model = joblib.load(model_filename)\n",
    "svm_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = logreg_model.score(test_features, test_labels)\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = logreg_model.predict(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = f1_score(test_labels, pred, average='weighted')\n",
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_proba = logreg_model.predict_proba(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = log_loss(test_labels, pred_proba)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc = roc_auc_score(test_labels, pred_proba, multi_class='ovr', average='weighted')\n",
    "auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter optimization"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "cell_metadata_json": true,
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "ipynb,py:light",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "eICU-mortality-prediction",
   "language": "python",
   "name": "eicu-mortality-prediction"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc-showcode": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
