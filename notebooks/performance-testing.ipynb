{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FCUL ALS Performance Testing\n",
    "---\n",
    "\n",
    "Testing the models trained on the ALS dataset from Faculdade de CiÃªncias da Universidade de Lisboa (FCUL) with the data from over 1000 patients collected in Portugal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KOdmFzXqF7nq"
   },
   "source": [
    "## Importing the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G5RrWE9R_Nkl"
   },
   "outputs": [],
   "source": [
    "import os                                  # os handles directory/workspace changes\n",
    "import numpy as np                         # NumPy to handle numeric and NaN operations\n",
    "import torch                               # PyTorch to create and apply deep learning models\n",
    "import pickle                              # Save python objects in files\n",
    "import yaml                                # Save and load YAML files\n",
    "from ipywidgets import interact            # Display selectors and sliders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pixiedust                           # Debugging in Jupyter Notebook cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the parquet dataset files\n",
    "data_path = 'Datasets/Thesis/FCUL_ALS/cleaned/'\n",
    "# Path to the code files\n",
    "project_path = 'GitHub/FCUL_ALS_Disease_Progression/'\n",
    "# Path to the models\n",
    "models_path = f'{project_path}models/'\n",
    "# Path to the metrics\n",
    "metrics_path = f'{project_path}metrics/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to the scripts directory\n",
    "os.chdir(\"../scripts/\")\n",
    "import utils                               # Context specific (in this case, for the ALS data) methods\n",
    "import Models                              # Deep learning models\n",
    "# Change to parent directory (presumably \"Documents\")\n",
    "os.chdir(\"../../..\")\n",
    "import pandas as pd                        # Pandas to load and handle the data\n",
    "import data_utils as du                    # Data science and machine learning relevant methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "du.set_pandas_library(lib='pandas')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Allow pandas to show more columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 3000)\n",
    "pd.set_option('display.max_rows', 3000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the random seed for reproducibility:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "du.set_random_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Allow Jupyter Lab to display all outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_column = 'subject_id'                   # Name of the sequence ID column\n",
    "ts_column = 'ts'                           # Name of the timestamp column\n",
    "label_column = 'niv_label'                 # Name of the label column\n",
    "padding_value = 999999                     # Padding value used to fill in sequences up to the maximum sequence length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One hot encoding columns categorization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_categ_feat_ohe = open(f'{data_path}categ_feat_ohe.yml', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categ_feat_ohe = yaml.load(stream_categ_feat_ohe, Loader=yaml.FullLoader)\n",
    "categ_feat_ohe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(categ_feat_ohe.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Dataloaders parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_train_ratio = 0.25                    # Percentage of the data which will be used as a test set\n",
    "validation_ratio = 0.1                     # Percentage of the data from the training set which is used for validation purposes\n",
    "batch_size = 32                            # Number of unit stays in a mini batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model to test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_filename = None                      # Name of the file containing the model that will be loaded\n",
    "model_class = None                         # Python class name that corresponds to the chosen model's type\n",
    "model = None                               # Machine learning model object\n",
    "dataset_mode = 'one hot encoded'           # The mode in which we'll use the data, either one hot encoded or pre-embedded\n",
    "ml_core = 'deep learning'                  # The core machine learning type we'll use; either traditional ML or DL\n",
    "use_delta_ts = False                       # Indicates if we'll use time variation info\n",
    "time_window_days = 90                      # Number of days on which we want to predict NIV\n",
    "is_custom = False                          # Indicates if the model being used is a custom built one\n",
    "@interact\n",
    "def get_dataset_mode(model_name=['Bidirectional LSTM with embedding layer',\n",
    "                                 'Bidirectional RNN with embedding layer and delta_ts',\n",
    "                                 'Bidirectional LSTM with delta_ts',\n",
    "                                 'Regular LSTM',\n",
    "                                 'Bidirectional LSTM with embedding layer and delta_ts',\n",
    "                                 'MF1-LSTM',\n",
    "                                 'XGBoost',\n",
    "                                 'Logistic regression']):\n",
    "    global model_filename, model_class, model, dataset_mode, ml_core, use_delta_ts, time_window_days, is_custom\n",
    "    if model_name == 'Bidirectional LSTM with embedding layer':\n",
    "        # Set the model file and class names, then load the model\n",
    "        model_filename = 'lstm_bidir_pre_embedded_90dayswindow_0.2490valloss_06_07_2020_03_47.pth'\n",
    "        model_class = 'VanillaLSTM'\n",
    "        model = du.deep_learning.load_checkpoint(f'{models_path}{model_filename}', getattr(Models, model_class))\n",
    "        # Set the use of an embedding layer\n",
    "        dataset_mode = 'pre-embedded'\n",
    "        # Set it as a custom model\n",
    "        is_custom = True\n",
    "    elif model_name == 'Bidirectional RNN with embedding layer and delta_ts':\n",
    "        # Set the model file and class names, then load the model\n",
    "        model_filename = 'rnn_bidir_pre_embedded_delta_ts_90dayswindow_0.3059valloss_06_07_2020_03_10.pth'\n",
    "        model_class = 'VanillaRNN'\n",
    "        model = du.deep_learning.load_checkpoint(f'{models_path}{model_filename}', getattr(Models, model_class))\n",
    "        # Set the use of an embedding layer\n",
    "        dataset_mode = 'pre-embedded'\n",
    "        # Set the use of delta_ts\n",
    "        use_delta_ts = 'normalized'\n",
    "        # Set it as a custom model\n",
    "        is_custom = True\n",
    "    elif model_name == 'Bidirectional LSTM with delta_ts':\n",
    "        # Set the model file and class names, then load the model\n",
    "        model_filename = 'lstm_bidir_one_hot_encoded_delta_ts_90dayswindow_0.3809valloss_06_07_2020_04_08.pth'\n",
    "        model_class = 'VanillaLSTM'\n",
    "        model = du.deep_learning.load_checkpoint(f'{models_path}{model_filename}', getattr(Models, model_class))\n",
    "        # Set the use of delta_ts\n",
    "        use_delta_ts = 'normalized'\n",
    "        # Set it as a custom model\n",
    "        is_custom = True\n",
    "    elif model_name == 'Regular LSTM':\n",
    "        # Set the model file and class names, then load the model\n",
    "        model_filename = 'lstm_one_hot_encoded_90dayswindow_0.4363valloss_06_07_2020_03_28.pth'\n",
    "        model_class = 'VanillaLSTM'\n",
    "        model = du.deep_learning.load_checkpoint(f'{models_path}{model_filename}', getattr(Models, model_class))\n",
    "        # Set it as a custom model\n",
    "        is_custom = True\n",
    "    elif model_name == 'Bidirectional LSTM with embedding layer and delta_ts':\n",
    "        # Set the model file and class names, then load the model\n",
    "        model_filename = 'lstm_bidir_pre_embedded_delta_ts_90dayswindow_0.3481valloss_06_07_2020_04_15.pth'\n",
    "        model_class = 'VanillaLSTM'\n",
    "        model = du.deep_learning.load_checkpoint(f'{models_path}{model_filename}', getattr(Models, model_class))\n",
    "        # Set the use of an embedding layer\n",
    "        dataset_mode = 'pre-embedded'\n",
    "        # Set the use of delta_ts\n",
    "        use_delta_ts = 'normalized'\n",
    "        # Set it as a custom model\n",
    "        is_custom = True\n",
    "    elif model_name == 'MF1-LSTM':\n",
    "        # Set the model file and class names, then load the model\n",
    "        model_filename = 'mf1lstm_one_hot_encoded_90dayswindow_0.6009valloss_07_07_2020_03_46.pth'\n",
    "        model_class = 'MF1LSTM'\n",
    "        model = du.deep_learning.load_checkpoint(f'{models_path}{model_filename}', getattr(Models, model_class))\n",
    "        # Set the use of delta_ts\n",
    "        use_delta_ts = 'normalized'\n",
    "        # Set it as a custom model\n",
    "        is_custom = True\n",
    "    elif model_name == 'XGBoost':\n",
    "        # Set the model file and class names, then load the model\n",
    "        model_filename = 'xgb_0.5926valloss_09_07_2020_02_40.pth'\n",
    "        model_class = 'XGBoost'\n",
    "        model = xgb.XGBClassifier()\n",
    "        model.load_model(f'{models_path}{model_filename}')\n",
    "        # Set as a traditional ML model\n",
    "        ml_core = 'machine learning'\n",
    "    elif model_name == 'Logistic regression':\n",
    "        # Set the model file and class names, then load the model\n",
    "        model_filename = 'logreg_0.6210valloss_09_07_2020_02_54.pth'\n",
    "        model_class = 'logreg'\n",
    "        model = joblib.load(f'{models_path}{model_filename}')\n",
    "        # Set as a traditional ML model\n",
    "        ml_core = 'machine learning'\n",
    "    print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessed data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALS_df = pd.read_csv(f'{data_path}FCUL_ALS_cleaned.csv')\n",
    "ALS_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove the `Unnamed: 0` column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALS_df.drop(columns=['Unnamed: 0'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALS_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ALS_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the maximum sequence length, so that the ML models and their related methods can handle all sequences, which have varying sequence lengths:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_length = ALS_df.groupby(id_column)[ts_column].count().max()\n",
    "total_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the label column, in case we're using a time window different than 90 days:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "if time_window_days is not 90:\n",
    "    # Recalculate the NIV label, based on the chosen time window\n",
    "    ALS_df[label_column] = utils.set_niv_label(ALS_df, time_window_days)\n",
    "    display(ALS_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove the `niv` column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALS_df.drop(columns=['niv'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the `delta_ts` (time variation between samples) if required:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pixiedust": {
     "displayParams": {
      "handlerId": "tableView"
     }
    }
   },
   "outputs": [],
   "source": [
    "if use_delta_ts is not False:\n",
    "    # Create a time variation column\n",
    "    ALS_df['delta_ts'] = ALS_df.groupby(id_column).ts.diff()\n",
    "    # Fill all the delta_ts missing values (the first value in a time series) with zeros\n",
    "    ALS_df['delta_ts'] = ALS_df['delta_ts'].fillna(0)\n",
    "if use_delta_ts == 'normalized':\n",
    "    # Normalize the time variation data\n",
    "    # NOTE: When using the MF2-LSTM model, since it assumes that the time\n",
    "    # variation is in days, we shouldn't normalize `delta_ts` with this model.\n",
    "    ALS_df['delta_ts'] = (ALS_df['delta_ts'] - ALS_df['delta_ts'].mean()) / ALS_df['delta_ts'].std()\n",
    "if use_delta_ts is not False:\n",
    "    ALS_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert into a padded tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = du.padding.dataframe_to_padded_tensor(ALS_df, padding_value=padding_value,\n",
    "                                             label_column=label_column, inplace=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the embedding configuration, if needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indices of the ID, timestamp and label columns\n",
    "id_column_idx = du.search_explore.find_col_idx(ALS_df, id_column)\n",
    "ts_column_idx = du.search_explore.find_col_idx(ALS_df, ts_column)\n",
    "label_column_idx = du.search_explore.find_col_idx(ALS_df, label_column)\n",
    "print(\n",
    "f'''ID index: {id_column_idx}\n",
    "Timestamp index: {ts_column_idx}\n",
    "Label index: {label_column_idx}'''\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset_mode == 'one hot encoded':\n",
    "    embed_features = None\n",
    "else:\n",
    "    embed_features = list()\n",
    "    if len(categ_feat_ohe.keys()) == 1:\n",
    "        for ohe_feature in list(categ_feat_ohe.values())[0]:\n",
    "            # Find the current feature's index so as to be able to use it as a tensor\n",
    "            feature_idx = du.search_explore.find_col_idx(ALS_df, ohe_feature)\n",
    "            # Decrease the index number if it's larger than the label column (which will be removed)\n",
    "            if feature_idx > label_column_idx:\n",
    "                feature_idx = feature_idx - 1\n",
    "            embed_features.append(feature_idx)\n",
    "    else:\n",
    "        for i in range(len(categ_feat_ohe.keys())):\n",
    "            tmp_list = list()\n",
    "            for ohe_feature in list(categ_feat_ohe.values())[i]:\n",
    "                # Find the current feature's index so as to be able to use it as a tensor\n",
    "                feature_idx = du.search_explore.find_col_idx(ALS_df, ohe_feature)\n",
    "                # Decrease the index number if it's larger than the label column (which will be removed)\n",
    "                if feature_idx > label_column_idx:\n",
    "                    feature_idx = feature_idx - 1\n",
    "                tmp_list.append(feature_idx)\n",
    "            # Add the current feature's list of one hot encoded columns\n",
    "            embed_features.append(tmp_list)\n",
    "print(f'Embedding features: {embed_features}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gather the feature names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = list(ALS_df.columns)\n",
    "feature_columns.remove('niv_label')\n",
    "if ml_core == 'machine learning':\n",
    "    feature_columns.remove('subject_id')\n",
    "    feature_columns.remove('ts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the dataset object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = du.datasets.Time_Series_Dataset(ALS_df, data, padding_value=padding_value,\n",
    "                                          label_name=label_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "dataset.__len__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separating into train and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_dataloader, val_dataloader, test_dataloader,\n",
    "train_indeces, val_indeces, test_indeces) = du.machine_learning.create_train_sets(dataset,\n",
    "                                                                                  test_train_ratio=test_train_ratio,\n",
    "                                                                                  validation_ratio=validation_ratio,\n",
    "                                                                                  batch_size=batch_size,\n",
    "                                                                                  get_indices=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the full arrays of each set\n",
    "train_features, train_labels = dataset.X[train_indeces], dataset.y[train_indeces]\n",
    "val_features, val_labels = dataset.X[val_indeces], dataset.y[val_indeces]\n",
    "test_features, test_labels = dataset.X[test_indeces], dataset.y[test_indeces]\n",
    "all_features, all_labels = dataset.X, dataset.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore the dataloaders, we only care about the full arrays when using scikit-learn or XGBoost\n",
    "del train_dataloader\n",
    "del val_dataloader\n",
    "del test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ml_core == 'machine learning':\n",
    "    # Reshape the data into a 2D format\n",
    "    train_features = train_features.reshape(-1, train_features.shape[-1])\n",
    "    val_features = val_features.reshape(-1, val_features.shape[-1])\n",
    "    test_features = test_features.reshape(-1, test_features.shape[-1])\n",
    "    all_features = all_features.reshape(-1, all_features.shape[-1])\n",
    "    train_labels = train_labels.reshape(-1)\n",
    "    val_labels = val_labels.reshape(-1)\n",
    "    test_labels = test_labels.reshape(-1)\n",
    "    all_labels = all_labels.reshape(-1)\n",
    "    # Remove padding samples from the data\n",
    "    train_features = train_features[[padding_value not in row for row in train_features]]\n",
    "    val_features = val_features[[padding_value not in row for row in val_features]]\n",
    "    test_features = test_features[[padding_value not in row for row in test_features]]\n",
    "    all_features = all_features[[padding_value not in row for row in all_features]]\n",
    "    train_labels = train_labels[[padding_value not in row for row in train_labels]]\n",
    "    val_labels = val_labels[[padding_value not in row for row in val_labels]]\n",
    "    test_labels = test_labels[[padding_value not in row for row in test_labels]]\n",
    "    all_labels = all_labels[[padding_value not in row for row in all_labels]]\n",
    "    # Convert from PyTorch tensor to NumPy array\n",
    "    train_features = train_features.numpy()\n",
    "    val_features = val_features.numpy()\n",
    "    test_features = test_features.numpy()\n",
    "    all_features = all_features.numpy()\n",
    "    train_labels = train_labels.numpy()\n",
    "    val_labels = val_labels.numpy()\n",
    "    test_labels = test_labels.numpy()\n",
    "    all_labels = all_labels.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "_, train_metrics = du.deep_learning.model_inference(model, data=(train_features, train_labels), \n",
    "                                                    metrics=['loss', 'accuracy', 'precision',\n",
    "                                                             'recall', 'F1', 'AUC'], \n",
    "                                                    model_type='multivariate_rnn', is_custom=is_custom, \n",
    "                                                    padding_value=padding_value)\n",
    "train_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, val_metrics = du.deep_learning.model_inference(model, data=(val_features, val_labels), \n",
    "                                                  metrics=['loss', 'accuracy', 'precision',\n",
    "                                                           'recall', 'F1', 'AUC'], \n",
    "                                                  model_type='multivariate_rnn', is_custom=is_custom, \n",
    "                                                  padding_value=padding_value)\n",
    "val_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, test_metrics = du.deep_learning.model_inference(model, data=(test_features, test_labels), \n",
    "                                                   metrics=['loss', 'accuracy', 'precision',\n",
    "                                                            'recall', 'F1', 'AUC'], \n",
    "                                                   model_type='multivariate_rnn', is_custom=is_custom, \n",
    "                                                   padding_value=padding_value)\n",
    "test_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving a dictionary with all the metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join all the sets' metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = dict()\n",
    "metrics['train'] = train_metrics\n",
    "metrics['val'] = val_metrics\n",
    "metrics['test'] = test_metrics\n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save in a YAML file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file_name_no_ext = model_filename.split('.pth')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_stream = open(f'{metrics_path}{model_file_name_no_ext}.yml', 'w')\n",
    "yaml.dump(metrics, metrics_stream, default_flow_style=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-4.m49",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-4:m49"
  },
  "jupytext": {
   "cell_metadata_filter": "-all",
   "cell_metadata_json": true,
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "ipynb,py:light",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "fcul_als_disease_progression",
   "language": "python",
   "name": "fcul_als_disease_progression"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
