{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FCUL ALS Performance Testing\n",
    "---\n",
    "\n",
    "Testing the models trained on the ALS dataset from Faculdade de CiÃªncias da Universidade de Lisboa (FCUL) with the data from over 1000 patients collected in Portugal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KOdmFzXqF7nq"
   },
   "source": [
    "## Importing the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G5RrWE9R_Nkl"
   },
   "outputs": [],
   "source": [
    "import os                                  # os handles directory/workspace changes\n",
    "import numpy as np                         # NumPy to handle numeric and NaN operations\n",
    "import torch                               # PyTorch to create and apply deep learning models\n",
    "import xgboost as xgb                      # Gradient boosting trees models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "import joblib                              # Save and load scikit-learn models in disk\n",
    "import pickle                              # Save python objects in files\n",
    "import yaml                                # Save and load YAML files\n",
    "from ipywidgets import interact            # Display selectors and sliders\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, log_loss, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pixiedust                           # Debugging in Jupyter Notebook cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the parquet dataset files\n",
    "data_path = 'Datasets/Thesis/FCUL_ALS/cleaned/'\n",
    "# Path to the code files\n",
    "project_path = 'GitHub/FCUL_ALS_Disease_Progression/'\n",
    "# Path to the models\n",
    "models_path = f'{project_path}models/'\n",
    "# Path to the metrics\n",
    "metrics_path = f'{project_path}metrics/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to the scripts directory\n",
    "os.chdir(\"../scripts/\")\n",
    "import utils                               # Context specific (in this case, for the ALS data) methods\n",
    "import Models                              # Deep learning models\n",
    "# Change to parent directory (presumably \"Documents\")\n",
    "os.chdir(\"../../..\")\n",
    "import pandas as pd                        # Pandas to load and handle the data\n",
    "import data_utils as du                    # Data science and machine learning relevant methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "du.set_pandas_library(lib='pandas')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Allow pandas to show more columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 3000)\n",
    "pd.set_option('display.max_rows', 3000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the random seed for reproducibility:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "du.set_random_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Allow Jupyter Lab to display all outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_column = 'subject_id'                   # Name of the sequence ID column\n",
    "ts_column = 'ts'                           # Name of the timestamp column\n",
    "label_column = 'niv_label'                 # Name of the label column\n",
    "padding_value = 999999                     # Padding value used to fill in sequences up to the maximum sequence length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One hot encoding columns categorization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_categ_feat_ohe = open(f'{data_path}categ_feat_ohe.yml', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categ_feat_ohe = yaml.load(stream_categ_feat_ohe, Loader=yaml.FullLoader)\n",
    "categ_feat_ohe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(categ_feat_ohe.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Dataloaders parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_train_ratio = 0.25                    # Percentage of the data which will be used as a test set\n",
    "validation_ratio = 0.1                     # Percentage of the data from the training set which is used for validation purposes\n",
    "batch_size = 32                            # Number of unit stays in a mini batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decide if we are just going to test the best model or do a comparison with all similar trained models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mode = None                           # Sets if testing an individual model or multiple, similar ones\n",
    "@interact\n",
    "def set_test_mode(test=['one', 'aggregate']):\n",
    "    global test_mode\n",
    "    test_mode = test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model to test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_filename = None                      # Name of the file containing the model that will be loaded\n",
    "model_class = None                         # Python class name that corresponds to the chosen model's type\n",
    "model = None                               # Machine learning model object\n",
    "model2 = None                              # Other machine learning model object\n",
    "model3 = None                              # Other machine learning model object\n",
    "dataset_mode = 'one hot encoded'           # The mode in which we'll use the data, either one hot encoded or pre-embedded\n",
    "ml_core = 'deep learning'                  # The core machine learning type we'll use; either traditional ML or DL\n",
    "use_delta_ts = False                       # Indicates if we'll use time variation info\n",
    "time_window_days = 90                      # Number of days on which we want to predict NIV\n",
    "is_custom = False                          # Indicates if the model being used is a custom built one\n",
    "random_seed_1 = None                       # Model random seeds\n",
    "random_seed_2 = None\n",
    "random_seed_3 = None\n",
    "@interact\n",
    "def get_dataset_mode(model_name=['Bidirectional LSTM with embedding layer and delta_ts',\n",
    "                                 'Bidirectional LSTM with embedding layer',\n",
    "                                 'Bidirectional LSTM with delta_ts',\n",
    "                                 'Bidirectional LSTM',\n",
    "                                 'LSTM with embedding layer and delta_ts',\n",
    "                                 'LSTM with embedding layer',\n",
    "                                 'LSTM with delta_ts',\n",
    "                                 'LSTM',\n",
    "                                 'Bidirectional RNN with embedding layer and delta_ts',\n",
    "                                 'Bidirectional RNN with embedding layer',\n",
    "                                 'Bidirectional RNN with delta_ts',\n",
    "                                 'Bidirectional RNN',\n",
    "                                 'RNN with embedding layer and delta_ts',\n",
    "                                 'RNN with embedding layer',\n",
    "                                 'RNN with delta_ts',\n",
    "                                 'RNN',\n",
    "                                 'MF1-LSTM with embedding layer',\n",
    "                                 'MF1-LSTM',\n",
    "                                 'MF2-LSTM with embedding layer',\n",
    "                                 'MF2-LSTM',\n",
    "                                 'TLSTM with embedding layer',\n",
    "                                 'TLSTM',\n",
    "                                 'XGBoost',\n",
    "                                 'Logistic regression',\n",
    "                                 'SVM']):\n",
    "    global model_filename, model_class, model, model2, model3, dataset_mode, ml_core, use_delta_ts \n",
    "    global time_window_days, is_custom, test_mode, random_seed_1, random_seed_2, random_seed_3\n",
    "    if model_name == 'Bidirectional LSTM with embedding layer and delta_ts':\n",
    "        # Set the model file and class names, then load the model\n",
    "        model_filename = 'lstm_bidir_pre_embedded_delta_ts_90dayswindow_0.3705valloss_08_07_2020_04_04.pth'\n",
    "        model_class = 'VanillaLSTM'\n",
    "        model = du.deep_learning.load_checkpoint(f'{models_path}{model_filename}', getattr(Models, model_class))\n",
    "        if test_mode == 'aggregate':\n",
    "            model_filename2 = 'lstm_bidir_pre_embedded_delta_ts_90dayswindow_0.3674valloss_08_07_2020_03_59.pth'\n",
    "            model_filename3 = 'lstm_bidir_pre_embedded_delta_ts_90dayswindow_0.3481valloss_06_07_2020_04_15.pth'\n",
    "            model2 = du.deep_learning.load_checkpoint(f'{models_path}{model_filename2}', getattr(Models, model_class))\n",
    "            model3 = du.deep_learning.load_checkpoint(f'{models_path}{model_filename3}', getattr(Models, model_class))\n",
    "            # Set the secondary random seed\n",
    "            random_seed_2 = 100\n",
    "            random_seed_3 = 42\n",
    "        # Set the main random seed\n",
    "        random_seed_1 = 0\n",
    "        # Set the use of an embedding layer\n",
    "        dataset_mode = 'pre-embedded'\n",
    "        # Set the use of delta_ts\n",
    "        use_delta_ts = 'normalized'\n",
    "        # Set it as not a custom model\n",
    "        is_custom = False\n",
    "        # Set as a traditional ML model\n",
    "        ml_core = 'deep learning'\n",
    "    elif model_name == 'Bidirectional LSTM with embedding layer':\n",
    "        # Set the model file and class names, then load the model\n",
    "        model_filename = 'lstm_bidir_pre_embedded_90dayswindow_0.2490valloss_06_07_2020_03_47.pth'\n",
    "        model_class = 'VanillaLSTM'\n",
    "        model = du.deep_learning.load_checkpoint(f'{models_path}{model_filename}', getattr(Models, model_class))\n",
    "        if test_mode == 'aggregate':\n",
    "            model_filename2 = 'lstm_bidir_pre_embedded_90dayswindow_0.3652valloss_08_07_2020_03_27.pth'\n",
    "            model_filename3 = 'lstm_bidir_pre_embedded_90dayswindow_0.3994valloss_08_07_2020_03_33.pth'\n",
    "            model2 = du.deep_learning.load_checkpoint(f'{models_path}{model_filename2}', getattr(Models, model_class))\n",
    "            model3 = du.deep_learning.load_checkpoint(f'{models_path}{model_filename3}', getattr(Models, model_class))\n",
    "            # Set the secondary random seed\n",
    "            random_seed_2 = 0\n",
    "            random_seed_3 = 100\n",
    "        # Set the main random seed\n",
    "        random_seed_1 = 42\n",
    "        # Set the use of an embedding layer\n",
    "        dataset_mode = 'pre-embedded'\n",
    "        # Set the use of delta_ts\n",
    "        use_delta_ts = False\n",
    "        # Set it as not a custom model\n",
    "        is_custom = False\n",
    "        # Set as a traditional ML model\n",
    "        ml_core = 'deep learning'\n",
    "    elif model_name == 'Bidirectional LSTM with delta_ts':\n",
    "        # Set the model file and class names, then load the model\n",
    "        model_filename = 'lstm_bidir_one_hot_encoded_delta_ts_90dayswindow_0.3784valloss_08_07_2020_04_14.pth'\n",
    "        model_class = 'VanillaLSTM'\n",
    "        model = du.deep_learning.load_checkpoint(f'{models_path}{model_filename}', getattr(Models, model_class))\n",
    "        if test_mode == 'aggregate':\n",
    "            model_filename2 = 'lstm_bidir_one_hot_encoded_delta_ts_90dayswindow_0.3809valloss_06_07_2020_04_08.pth'\n",
    "            model_filename3 = 'lstm_bidir_one_hot_encoded_delta_ts_90dayswindow_0.3603valloss_08_07_2020_04_17.pth'\n",
    "            model2 = du.deep_learning.load_checkpoint(f'{models_path}{model_filename2}', getattr(Models, model_class))\n",
    "            model3 = du.deep_learning.load_checkpoint(f'{models_path}{model_filename3}', getattr(Models, model_class))\n",
    "            # Set the secondary random seed\n",
    "            random_seed_2 = 42\n",
    "            random_seed_3 = 100\n",
    "        # Set the main random seed\n",
    "        random_seed_1 = 0\n",
    "        # Set the use of an embedding layer\n",
    "        dataset_mode = 'one hot encoded'\n",
    "        # Set the use of delta_ts\n",
    "        use_delta_ts = 'normalized'\n",
    "        # Set it as not a custom model\n",
    "        is_custom = False\n",
    "        # Set as a traditional ML model\n",
    "        ml_core = 'deep learning'\n",
    "    elif model_name == 'Bidirectional LSTM':\n",
    "        # Set the model file and class names, then load the model\n",
    "        model_filename = 'lstm_bidir_one_hot_encoded_90dayswindow_0.4497valloss_08_07_2020_04_31.pth'\n",
    "        model_class = 'VanillaLSTM'\n",
    "        model = du.deep_learning.load_checkpoint(f'{models_path}{model_filename}', getattr(Models, model_class))\n",
    "        if test_mode == 'aggregate':\n",
    "            model_filename2 = 'lstm_bidir_one_hot_encoded_90dayswindow_0.4598valloss_06_07_2020_03_39.pth'\n",
    "            model_filename3 = 'lstm_bidir_one_hot_encoded_90dayswindow_0.3688valloss_08_07_2020_04_36.pth'\n",
    "            model2 = du.deep_learning.load_checkpoint(f'{models_path}{model_filename2}', getattr(Models, model_class))\n",
    "            model3 = du.deep_learning.load_checkpoint(f'{models_path}{model_filename3}', getattr(Models, model_class))\n",
    "            # Set the secondary random seed\n",
    "            random_seed_2 = 42\n",
    "            random_seed_3 = 100\n",
    "        # Set the main random seed\n",
    "        random_seed_1 = 0\n",
    "        # Set the use of an embedding layer\n",
    "        dataset_mode = 'one hot encoded'\n",
    "        # Set the use of delta_ts\n",
    "        use_delta_ts = False\n",
    "        # Set it as not a custom model\n",
    "        is_custom = False\n",
    "        # Set as a traditional ML model\n",
    "        ml_core = 'deep learning'\n",
    "    elif model_name == 'LSTM with embedding layer and delta_ts':\n",
    "        # Set the model file and class names, then load the model\n",
    "        model_filename = 'lstm_pre_embedded_delta_ts_90dayswindow_0.5071valloss_21_08_2020_05_03.pth'\n",
    "        model_class = 'VanillaLSTM'\n",
    "        model = du.deep_learning.load_checkpoint(f'{models_path}{model_filename}', getattr(Models, model_class))\n",
    "        if test_mode == 'aggregate':\n",
    "            model_filename2 = 'lstm_pre_embedded_delta_ts_90dayswindow_0.4771valloss_06_07_2020_03_55.pth'\n",
    "            model_filename3 = 'lstm_pre_embedded_delta_ts_90dayswindow_0.5712valloss_21_08_2020_05_00.pth'\n",
    "            model2 = du.deep_learning.load_checkpoint(f'{models_path}{model_filename2}', getattr(Models, model_class))\n",
    "            model3 = du.deep_learning.load_checkpoint(f'{models_path}{model_filename3}', getattr(Models, model_class))\n",
    "            # Set the secondary random seed\n",
    "            random_seed_2 = 0\n",
    "            random_seed_3 = 42\n",
    "        # Set the main random seed\n",
    "        random_seed_1 = 100\n",
    "        # Set the use of an embedding layer\n",
    "        dataset_mode = 'pre-embedded'\n",
    "        # Set the use of delta_ts\n",
    "        use_delta_ts = 'normalized'\n",
    "        # Set it as not a custom model\n",
    "        is_custom = False\n",
    "        # Set as a traditional ML model\n",
    "        ml_core = 'deep learning'\n",
    "    elif model_name == 'LSTM with embedding layer':\n",
    "        # Set the model file and class names, then load the model\n",
    "        model_filename = 'lstm_pre_embedded_90dayswindow_0.5898valloss_06_07_2020_03_21.pth'\n",
    "        model_class = 'VanillaLSTM'\n",
    "        model = du.deep_learning.load_checkpoint(f'{models_path}{model_filename}', getattr(Models, model_class))\n",
    "        if test_mode == 'aggregate':\n",
    "            model_filename2 = 'lstm_pre_embedded_90dayswindow_0.5205valloss_21_08_2020_04_47.pth'\n",
    "            model_filename3 = 'lstm_pre_embedded_90dayswindow_0.5186valloss_21_08_2020_04_44.pth'\n",
    "            model2 = du.deep_learning.load_checkpoint(f'{models_path}{model_filename2}', getattr(Models, model_class))\n",
    "            model3 = du.deep_learning.load_checkpoint(f'{models_path}{model_filename3}', getattr(Models, model_class))\n",
    "            # Set the secondary random seed\n",
    "            random_seed_2 = 0\n",
    "            random_seed_3 = 100\n",
    "        # Set the main random seed\n",
    "        random_seed_1 = 42\n",
    "        # Set the use of an embedding layer\n",
    "        dataset_mode = 'pre-embedded'\n",
    "        # Set the use of delta_ts\n",
    "        use_delta_ts = False\n",
    "        # Set it as not a custom model\n",
    "        is_custom = False\n",
    "        # Set as a traditional ML model\n",
    "        ml_core = 'deep learning'\n",
    "    elif model_name == 'LSTM with delta_ts':\n",
    "        # Set the model file and class names, then load the model\n",
    "        model_filename = 'lstm_one_hot_encoded_delta_ts_90dayswindow_0.5178valloss_06_07_2020_04_02.pth'\n",
    "        model_class = 'VanillaLSTM'\n",
    "        model = du.deep_learning.load_checkpoint(f'{models_path}{model_filename}', getattr(Models, model_class))\n",
    "        if test_mode == 'aggregate':\n",
    "            model_filename2 = 'lstm_one_hot_encoded_delta_ts_90dayswindow_0.5106valloss_21_08_2020_04_53.pth'\n",
    "            model_filename3 = 'lstm_one_hot_encoded_delta_ts_90dayswindow_0.5139valloss_21_08_2020_04_56.pth'\n",
    "            model2 = du.deep_learning.load_checkpoint(f'{models_path}{model_filename2}', getattr(Models, model_class))\n",
    "            model3 = du.deep_learning.load_checkpoint(f'{models_path}{model_filename3}', getattr(Models, model_class))\n",
    "            # Set the secondary random seed\n",
    "            random_seed_2 = 0\n",
    "            random_seed_3 = 100\n",
    "        # Set the main random seed\n",
    "        random_seed_1 = 42\n",
    "        # Set the use of an embedding layer\n",
    "        dataset_mode = 'one hot encoded'\n",
    "        # Set the use of delta_ts\n",
    "        use_delta_ts = 'normalized'\n",
    "        # Set it as not a custom model\n",
    "        is_custom = False\n",
    "        # Set as a traditional ML model\n",
    "        ml_core = 'deep learning'\n",
    "    elif model_name == 'LSTM':\n",
    "        # Set the model file and class names, then load the model\n",
    "        model_filename = 'lstm_one_hot_encoded_90dayswindow_0.5125valloss_08_07_2020_04_41.pth'\n",
    "        model_class = 'VanillaLSTM'\n",
    "        model = du.deep_learning.load_checkpoint(f'{models_path}{model_filename}', getattr(Models, model_class))\n",
    "        if test_mode == 'aggregate':\n",
    "            model_filename2 = 'lstm_one_hot_encoded_90dayswindow_0.4363valloss_06_07_2020_03_28.pth'\n",
    "            model_filename3 = 'lstm_one_hot_encoded_90dayswindow_0.5232valloss_08_07_2020_04_44.pth'\n",
    "            model2 = du.deep_learning.load_checkpoint(f'{models_path}{model_filename2}', getattr(Models, model_class))\n",
    "            model3 = du.deep_learning.load_checkpoint(f'{models_path}{model_filename3}', getattr(Models, model_class))\n",
    "            # Set the secondary random seed\n",
    "            random_seed_2 = 42\n",
    "            random_seed_3 = 0\n",
    "        # Set the main random seed\n",
    "        random_seed_1 = 100\n",
    "        # Set the use of an embedding layer\n",
    "        dataset_mode = 'one hot encoded'\n",
    "        # Set the use of delta_ts\n",
    "        use_delta_ts = False\n",
    "        # Set it as not a custom model\n",
    "        is_custom = False\n",
    "        # Set as a traditional ML model\n",
    "        ml_core = 'deep learning'\n",
    "    elif model_name == 'Bidirectional RNN with embedding layer and delta_ts':\n",
    "        # Set the model file and class names, then load the model\n",
    "        model_filename = 'rnn_bidir_pre_embedded_delta_ts_90dayswindow_0.3579valloss_08_07_2020_03_55.pth'\n",
    "        model_class = 'VanillaRNN'\n",
    "        model = du.deep_learning.load_checkpoint(f'{models_path}{model_filename}', getattr(Models, model_class))\n",
    "        if test_mode == 'aggregate':\n",
    "            model_filename2 = 'rnn_bidir_pre_embedded_delta_ts_90dayswindow_0.3059valloss_06_07_2020_03_10.pth'\n",
    "            model_filename3 = 'rnn_bidir_pre_embedded_delta_ts_90dayswindow_0.4249valloss_08_07_2020_03_49.pth'\n",
    "            model2 = du.deep_learning.load_checkpoint(f'{models_path}{model_filename2}', getattr(Models, model_class))\n",
    "            model3 = du.deep_learning.load_checkpoint(f'{models_path}{model_filename3}', getattr(Models, model_class))\n",
    "            # Set the secondary random seed\n",
    "            random_seed_2 = 42\n",
    "            random_seed_3 = 0\n",
    "        # Set the main random seed\n",
    "        random_seed_1 = 100\n",
    "        # Set the use of an embedding layer\n",
    "        dataset_mode = 'pre-embedded'\n",
    "        # Set the use of delta_ts\n",
    "        use_delta_ts = 'normalized'\n",
    "        # Set it as not a custom model\n",
    "        is_custom = False\n",
    "        # Set as a traditional ML model\n",
    "        ml_core = 'deep learning'\n",
    "    elif model_name == 'Bidirectional RNN with embedding layer':\n",
    "        # Set the model file and class names, then load the model\n",
    "        model_filename = 'rnn_bidir_pre_embedded_90dayswindow_0.4005valloss_08_07_2020_03_43.pth'\n",
    "        model_class = 'VanillaRNN'\n",
    "        model = du.deep_learning.load_checkpoint(f'{models_path}{model_filename}', getattr(Models, model_class))\n",
    "        if test_mode == 'aggregate':\n",
    "            model_filename2 = 'rnn_bidir_pre_embedded_90dayswindow_0.4241valloss_05_07_2020_13_05.pth'\n",
    "            model_filename3 = 'rnn_bidir_pre_embedded_90dayswindow_0.4020valloss_08_07_2020_03_40.pth'\n",
    "            model2 = du.deep_learning.load_checkpoint(f'{models_path}{model_filename2}', getattr(Models, model_class))\n",
    "            model3 = du.deep_learning.load_checkpoint(f'{models_path}{model_filename3}', getattr(Models, model_class))\n",
    "            # Set the secondary random seed\n",
    "            random_seed_2 = 42\n",
    "            random_seed_3 = 100\n",
    "        # Set the main random seed\n",
    "        random_seed_1 = 0\n",
    "        # Set the use of an embedding layer\n",
    "        dataset_mode = 'pre-embedded'\n",
    "        # Set the use of delta_ts\n",
    "        use_delta_ts = False\n",
    "        # Set it as not a custom model\n",
    "        is_custom = False\n",
    "        # Set as a traditional ML model\n",
    "        ml_core = 'deep learning'\n",
    "    elif model_name == 'Bidirectional RNN with delta_ts':\n",
    "        # Set the model file and class names, then load the model\n",
    "        model_filename = 'rnn_bidir_one_hot_encoded_delta_ts_90dayswindow_0.3631valloss_08_07_2020_04_21.pth'\n",
    "        model_class = 'VanillaRNN'\n",
    "        model = du.deep_learning.load_checkpoint(f'{models_path}{model_filename}', getattr(Models, model_class))\n",
    "        if test_mode == 'aggregate':\n",
    "            model_filename2 = 'rnn_bidir_one_hot_encoded_delta_ts_90dayswindow_0.3907valloss_08_07_2020_04_24.pth'\n",
    "            model_filename3 = 'rnn_bidir_one_hot_encoded_delta_ts_90dayswindow_0.3510valloss_06_07_2020_02_59.pth'\n",
    "            model2 = du.deep_learning.load_checkpoint(f'{models_path}{model_filename2}', getattr(Models, model_class))\n",
    "            model3 = du.deep_learning.load_checkpoint(f'{models_path}{model_filename3}', getattr(Models, model_class))\n",
    "            # Set the secondary random seed\n",
    "            random_seed_2 = 0\n",
    "            random_seed_3 = 42\n",
    "        # Set the main random seed\n",
    "        random_seed_1 = 100\n",
    "        # Set the use of an embedding layer\n",
    "        dataset_mode = 'one hot encoded'\n",
    "        # Set the use of delta_ts\n",
    "        use_delta_ts = 'normalized'\n",
    "        # Set it as not a custom model\n",
    "        is_custom = False\n",
    "        # Set as a traditional ML model\n",
    "        ml_core = 'deep learning'\n",
    "    elif model_name == 'Bidirectional RNN':\n",
    "        # Set the model file and class names, then load the model\n",
    "        model_filename = 'rnn_bidir_one_hot_encoded_90dayswindow_0.3713valloss_08_07_2020_04_49.pth'\n",
    "        model_class = 'VanillaRNN'\n",
    "        model = du.deep_learning.load_checkpoint(f'{models_path}{model_filename}', getattr(Models, model_class))\n",
    "        if test_mode == 'aggregate':\n",
    "            model_filename2 = 'rnn_bidir_one_hot_encoded_90dayswindow_0.3610valloss_08_07_2020_04_46.pth'\n",
    "            model_filename3 = 'rnn_bidir_one_hot_encoded_90dayswindow_0.4241valloss_03_07_2020_17_40.pth'\n",
    "            model2 = du.deep_learning.load_checkpoint(f'{models_path}{model_filename2}', getattr(Models, model_class))\n",
    "            model3 = du.deep_learning.load_checkpoint(f'{models_path}{model_filename3}', getattr(Models, model_class))\n",
    "            # Set the secondary random seed\n",
    "            random_seed_2 = 0\n",
    "            random_seed_3 = 42\n",
    "        # Set the main random seed\n",
    "        random_seed_1 = 100\n",
    "        # Set the use of an embedding layer\n",
    "        dataset_mode = 'one hot encoded'\n",
    "        # Set the use of delta_ts\n",
    "        use_delta_ts = False\n",
    "        # Set it as not a custom model\n",
    "        is_custom = False\n",
    "        # Set as a traditional ML model\n",
    "        ml_core = 'deep learning'\n",
    "    elif model_name == 'RNN with embedding layer and delta_ts':\n",
    "        # Set the model file and class names, then load the model\n",
    "        model_filename = 'rnn_pre_embedded_delta_ts_90dayswindow_0.5267valloss_21_08_2020_04_19.pth'\n",
    "        model_class = 'VanillaRNN'\n",
    "        model = du.deep_learning.load_checkpoint(f'{models_path}{model_filename}', getattr(Models, model_class))\n",
    "        if test_mode == 'aggregate':\n",
    "            model_filename2 = 'rnn_pre_embedded_delta_ts_90dayswindow_0.5602valloss_06_07_2020_02_50.pth'\n",
    "            model_filename3 = 'rnn_pre_embedded_delta_ts_90dayswindow_0.5393valloss_21_08_2020_04_15.pth'\n",
    "            model2 = du.deep_learning.load_checkpoint(f'{models_path}{model_filename2}', getattr(Models, model_class))\n",
    "            model3 = du.deep_learning.load_checkpoint(f'{models_path}{model_filename3}', getattr(Models, model_class))\n",
    "            # Set the secondary random seed\n",
    "            random_seed_2 = 42\n",
    "            random_seed_3 = 100\n",
    "        # Set the main random seed\n",
    "        random_seed_1 = 0\n",
    "        # Set the use of an embedding layer\n",
    "        dataset_mode = 'pre-embedded'\n",
    "        # Set the use of delta_ts\n",
    "        use_delta_ts = 'normalized'\n",
    "        # Set it as not a custom model\n",
    "        is_custom = False\n",
    "        # Set as a traditional ML model\n",
    "        ml_core = 'deep learning'\n",
    "    elif model_name == 'RNN with embedding layer':\n",
    "        # Set the model file and class names, then load the model\n",
    "        model_filename = 'rnn_pre_embedded_90dayswindow_0.5238valloss_21_08_2020_04_39.pth'\n",
    "        model_class = 'VanillaRNN'\n",
    "        model = du.deep_learning.load_checkpoint(f'{models_path}{model_filename}', getattr(Models, model_class))\n",
    "        if test_mode == 'aggregate':\n",
    "            model_filename2 = 'rnn_with_embedding_90dayswindow_0.5569valloss_30_06_2020_17_04.pth'\n",
    "            model_filename3 = 'rnn_pre_embedded_90dayswindow_0.5335valloss_21_08_2020_04_41.pth'\n",
    "            model2 = du.deep_learning.load_checkpoint(f'{models_path}{model_filename2}', getattr(Models, model_class))\n",
    "            model3 = du.deep_learning.load_checkpoint(f'{models_path}{model_filename3}', getattr(Models, model_class))\n",
    "            # Set the secondary random seed\n",
    "            random_seed_2 = 42\n",
    "            random_seed_3 = 100\n",
    "        # Set the main random seed\n",
    "        random_seed_1 = 0\n",
    "        # Set the use of an embedding layer\n",
    "        dataset_mode = 'pre-embedded'\n",
    "        # Set the use of delta_ts\n",
    "        use_delta_ts = False\n",
    "        # Set it as not a custom model\n",
    "        is_custom = False\n",
    "        # Set as a traditional ML model\n",
    "        ml_core = 'deep learning'\n",
    "    elif model_name == 'RNN with delta_ts':\n",
    "        # Set the model file and class names, then load the model\n",
    "        model_filename = 'rnn_one_hot_encoded_delta_ts_90dayswindow_0.5354valloss_21_08_2020_04_24.pth'\n",
    "        model_class = 'VanillaRNN'\n",
    "        model = du.deep_learning.load_checkpoint(f'{models_path}{model_filename}', getattr(Models, model_class))\n",
    "        if test_mode == 'aggregate':\n",
    "            model_filename2 = 'rnn_one_hot_encoded_delta_ts_90dayswindow_0.4275valloss_06_07_2020_02_55.pth'\n",
    "            model_filename3 = 'rnn_one_hot_encoded_delta_ts_90dayswindow_0.5364valloss_21_08_2020_04_28.pth'\n",
    "            model2 = du.deep_learning.load_checkpoint(f'{models_path}{model_filename2}', getattr(Models, model_class))\n",
    "            model3 = du.deep_learning.load_checkpoint(f'{models_path}{model_filename3}', getattr(Models, model_class))\n",
    "            # Set the secondary random seed\n",
    "            random_seed_2 = 42\n",
    "            random_seed_3 = 100\n",
    "        # Set the main random seed\n",
    "        random_seed_1 = 0\n",
    "        # Set the use of an embedding layer\n",
    "        dataset_mode = 'one hot encoded'\n",
    "        # Set the use of delta_ts\n",
    "        use_delta_ts = 'normalized'\n",
    "        # Set it as not a custom model\n",
    "        is_custom = False\n",
    "        # Set as a traditional ML model\n",
    "        ml_core = 'deep learning'\n",
    "    elif model_name == 'RNN':\n",
    "        # Set the model file and class names, then load the model\n",
    "        model_filename = 'rnn_one_hot_encoded_90dayswindow_0.5445valloss_21_08_2020_04_34.pth'\n",
    "        model_class = 'VanillaRNN'\n",
    "        model = du.deep_learning.load_checkpoint(f'{models_path}{model_filename}', getattr(Models, model_class))\n",
    "        if test_mode == 'aggregate':\n",
    "            model_filename2 = 'rnn_one_hot_encoded_90dayswindow_0.5497valloss_30_06_2020_18_25.pth'\n",
    "            model_filename3 = 'rnn_one_hot_encoded_90dayswindow_0.5409valloss_21_08_2020_04_30.pth'\n",
    "            model2 = du.deep_learning.load_checkpoint(f'{models_path}{model_filename2}', getattr(Models, model_class))\n",
    "            model3 = du.deep_learning.load_checkpoint(f'{models_path}{model_filename3}', getattr(Models, model_class))\n",
    "            # Set the secondary random seed\n",
    "            random_seed_2 = 42\n",
    "            random_seed_3 = 100\n",
    "        # Set the main random seed\n",
    "        random_seed_1 = 0\n",
    "        # Set the use of an embedding layer\n",
    "        dataset_mode = 'one hot encoded'\n",
    "        # Set the use of delta_ts\n",
    "        use_delta_ts = False\n",
    "        # Set it as not a custom model\n",
    "        is_custom = False\n",
    "        # Set as a traditional ML model\n",
    "        ml_core = 'deep learning'\n",
    "    elif model_name == 'MF1-LSTM with embedding layer':\n",
    "        # Set the model file and class names, then load the model\n",
    "        model_filename = 'mf1lstm_pre_embedded_90dayswindow_0.6516valloss_07_07_2020_03_35.pth'\n",
    "        model_class = 'MF1LSTM'\n",
    "        model = du.deep_learning.load_checkpoint(f'{models_path}{model_filename}', getattr(Models, model_class))\n",
    "        if test_mode == 'aggregate':\n",
    "            model_filename2 = 'mf1lstm_pre_embedded_90dayswindow_0.6351valloss_21_08_2020_15_53.pth'\n",
    "            model_filename3 = 'mf1lstm_pre_embedded_90dayswindow_0.6449valloss_21_08_2020_15_27.pth'\n",
    "            model2 = du.deep_learning.load_checkpoint(f'{models_path}{model_filename2}', getattr(Models, model_class))\n",
    "            model3 = du.deep_learning.load_checkpoint(f'{models_path}{model_filename3}', getattr(Models, model_class))\n",
    "            # Set the secondary random seed\n",
    "            random_seed_2 = 0\n",
    "            random_seed_3 = 100\n",
    "        # Set the main random seed\n",
    "        random_seed_1 = 42\n",
    "        # Set the use of an embedding layer\n",
    "        dataset_mode = 'pre-embedded'\n",
    "        # Set the use of delta_ts\n",
    "        use_delta_ts = False\n",
    "        # Set it as a custom model\n",
    "        is_custom = True\n",
    "        # Set as a traditional ML model\n",
    "        ml_core = 'deep learning'\n",
    "    elif model_name == 'MF1-LSTM':\n",
    "        # Set the model file and class names, then load the model\n",
    "        model_filename = 'mf1lstm_one_hot_encoded_90dayswindow_0.6009valloss_07_07_2020_03_46.pth'\n",
    "        model_class = 'MF1LSTM'\n",
    "        model = du.deep_learning.load_checkpoint(f'{models_path}{model_filename}', getattr(Models, model_class))\n",
    "        if test_mode == 'aggregate':\n",
    "            model_filename2 = 'mf1lstm_one_hot_encoded_90dayswindow_0.6135valloss_21_08_2020_16_09.pth'\n",
    "            model_filename3 = 'mf1lstm_one_hot_encoded_90dayswindow_0.6200valloss_21_08_2020_16_16.pth'\n",
    "            model2 = du.deep_learning.load_checkpoint(f'{models_path}{model_filename2}', getattr(Models, model_class))\n",
    "            model3 = du.deep_learning.load_checkpoint(f'{models_path}{model_filename3}', getattr(Models, model_class))\n",
    "            # Set the secondary random seed\n",
    "            random_seed_2 = 0\n",
    "            random_seed_3 = 100\n",
    "        # Set the main random seed\n",
    "        random_seed_1 = 42\n",
    "        # Set the use of an embedding layer\n",
    "        dataset_mode = 'one hot encoded'\n",
    "        # Set the use of delta_ts\n",
    "        use_delta_ts = 'normalized'\n",
    "        # Set it as a custom model\n",
    "        is_custom = True\n",
    "        # Set as a traditional ML model\n",
    "        ml_core = 'deep learning'\n",
    "    elif model_name == 'MF2-LSTM with embedding layer':\n",
    "        # Set the model file and class names, then load the model\n",
    "        model_filename = 'mf2lstm_pre_embedded_90dayswindow_0.6388valloss_07_07_2020_03_54.pth'\n",
    "        model_class = 'MF2LSTM'\n",
    "        model = du.deep_learning.load_checkpoint(f'{models_path}{model_filename}', getattr(Models, model_class))\n",
    "        if test_mode == 'aggregate':\n",
    "            model_filename2 = 'mf2lstm_pre_embedded_90dayswindow_0.6341valloss_21_08_2020_16_40.pth'\n",
    "            model_filename3 = 'mf2lstm_pre_embedded_90dayswindow_0.6405valloss_21_08_2020_17_56.pth'\n",
    "            model2 = du.deep_learning.load_checkpoint(f'{models_path}{model_filename2}', getattr(Models, model_class))\n",
    "            model3 = du.deep_learning.load_checkpoint(f'{models_path}{model_filename3}', getattr(Models, model_class))\n",
    "            # Set the secondary random seed\n",
    "            random_seed_2 = 0\n",
    "            random_seed_3 = 100\n",
    "        # Set the main random seed\n",
    "        random_seed_1 = 42\n",
    "        # Set the use of an embedding layer\n",
    "        dataset_mode = 'pre-embedded'\n",
    "        # Set the use of delta_ts\n",
    "        use_delta_ts = False\n",
    "        # Set it as a custom model\n",
    "        is_custom = True\n",
    "        # Set as a traditional ML model\n",
    "        ml_core = 'deep learning'\n",
    "    elif model_name == 'MF2-LSTM':\n",
    "        # Set the model file and class names, then load the model\n",
    "        model_filename = 'mf2lstm_one_hot_encoded_90dayswindow_0.5918valloss_07_07_2020_03_58.pth'\n",
    "        model_class = 'MF2LSTM'\n",
    "        model = du.deep_learning.load_checkpoint(f'{models_path}{model_filename}', getattr(Models, model_class))\n",
    "        if test_mode == 'aggregate':\n",
    "            model_filename2 = 'mf2lstm_one_hot_encoded_90dayswindow_0.6145valloss_21_08_2020_16_33.pth'\n",
    "            model_filename3 = 'mf2lstm_one_hot_encoded_90dayswindow_0.6200valloss_21_08_2020_16_25.pth'\n",
    "            model2 = du.deep_learning.load_checkpoint(f'{models_path}{model_filename2}', getattr(Models, model_class))\n",
    "            model3 = du.deep_learning.load_checkpoint(f'{models_path}{model_filename3}', getattr(Models, model_class))\n",
    "            # Set the secondary random seed\n",
    "            random_seed_2 = 0\n",
    "            random_seed_3 = 100\n",
    "        # Set the main random seed\n",
    "        random_seed_1 = 42\n",
    "        # Set the use of an embedding layer\n",
    "        dataset_mode = 'one hot encoded'\n",
    "        # Set the use of delta_ts\n",
    "        use_delta_ts = 'raw'\n",
    "        # Set it as a custom model\n",
    "        is_custom = True\n",
    "        # Set as a traditional ML model\n",
    "        ml_core = 'deep learning'\n",
    "    elif model_name == 'TLSTM with embedding layer':\n",
    "        # Set the model file and class names, then load the model\n",
    "        model_filename = 'tlstm_pre_embedded_90dayswindow_0.6503valloss_07_07_2020_03_03.pth'\n",
    "        model_class = 'TLSTM'\n",
    "        model = du.deep_learning.load_checkpoint(f'{models_path}{model_filename}', getattr(Models, model_class))\n",
    "        if test_mode == 'aggregate':\n",
    "            model_filename2 = 'tlstm_pre_embedded_90dayswindow_0.6173valloss_21_08_2020_15_12.pth'\n",
    "            model_filename3 = 'tlstm_pre_embedded_90dayswindow_0.6402valloss_21_08_2020_15_19.pth'\n",
    "            model2 = du.deep_learning.load_checkpoint(f'{models_path}{model_filename2}', getattr(Models, model_class))\n",
    "            model3 = du.deep_learning.load_checkpoint(f'{models_path}{model_filename3}', getattr(Models, model_class))\n",
    "            # Set the secondary random seed\n",
    "            random_seed_2 = 0\n",
    "            random_seed_3 = 100\n",
    "        # Set the main random seed\n",
    "        random_seed_1 = 42\n",
    "        # Set the use of an embedding layer\n",
    "        dataset_mode = 'pre-embedded'\n",
    "        # Set the use of delta_ts\n",
    "        use_delta_ts = False\n",
    "        # Set it as a custom model\n",
    "        is_custom = True\n",
    "        # Set as a traditional ML model\n",
    "        ml_core = 'deep learning'\n",
    "    elif model_name == 'TLSTM':\n",
    "        # Set the model file and class names, then load the model\n",
    "        model_filename = 'tlstm_one_hot_encoded_90dayswindow_0.6153valloss_07_07_2020_03_13.pth'\n",
    "        model_class = 'TLSTM'\n",
    "        model = du.deep_learning.load_checkpoint(f'{models_path}{model_filename}', getattr(Models, model_class))\n",
    "        if test_mode == 'aggregate':\n",
    "            model_filename2 = 'tlstm_one_hot_encoded_90dayswindow_0.6197valloss_21_08_2020_14_52.pth'\n",
    "            model_filename3 = 'tlstm_one_hot_encoded_90dayswindow_0.6516valloss_21_08_2020_15_03.pth'\n",
    "            model2 = du.deep_learning.load_checkpoint(f'{models_path}{model_filename2}', getattr(Models, model_class))\n",
    "            model3 = du.deep_learning.load_checkpoint(f'{models_path}{model_filename3}', getattr(Models, model_class))\n",
    "            # Set the secondary random seed\n",
    "            random_seed_2 = 0\n",
    "            random_seed_3 = 100\n",
    "        # Set the main random seed\n",
    "        random_seed_1 = 42\n",
    "        # Set the use of an embedding layer\n",
    "        dataset_mode = 'one hot encoded'\n",
    "        # Set the use of delta_ts\n",
    "        use_delta_ts = 'normalized'\n",
    "        # Set it as a custom model\n",
    "        is_custom = True\n",
    "        # Set as a traditional ML model\n",
    "        ml_core = 'deep learning'\n",
    "    elif model_name == 'XGBoost':\n",
    "        # Set the model file and class names, then load the model\n",
    "        model_filename = 'xgb_0.5926valloss_09_07_2020_02_40.pth'\n",
    "        model_class = 'XGBoost'\n",
    "        model = xgb.XGBClassifier()\n",
    "        model.load_model(f'{models_path}{model_filename}')\n",
    "        if test_mode == 'aggregate':\n",
    "            model_filename2 = 'xgb_0.5694valloss_21_08_2020_18_03.pth'\n",
    "            model_filename3 = 'xgb_0.5601valloss_21_08_2020_18_01.pth'\n",
    "            model2 = xgb.XGBClassifier()\n",
    "            model2.load_model(f'{models_path}{model_filename2}')\n",
    "            model3 = xgb.XGBClassifier()\n",
    "            model3.load_model(f'{models_path}{model_filename3}')\n",
    "            # Set the secondary random seed\n",
    "            random_seed_2 = 0\n",
    "            random_seed_3 = 100\n",
    "        # Set the main random seed\n",
    "        random_seed_1 = 42\n",
    "        # Set the use of an embedding layer\n",
    "        dataset_mode = 'one hot encoded'\n",
    "        # Set the use of delta_ts\n",
    "        use_delta_ts = False\n",
    "        # Set as a traditional ML model\n",
    "        ml_core = 'machine learning'\n",
    "    elif model_name == 'Logistic regression':\n",
    "        # Set the model file and class names, then load the model\n",
    "        model_filename = 'logreg_0.6210valloss_09_07_2020_02_54.pth'\n",
    "        model_class = 'LogReg'\n",
    "        model = joblib.load(f'{models_path}{model_filename}')\n",
    "        if test_mode == 'aggregate':\n",
    "            model_filename2 = 'logreg_0.5932valloss_21_08_2020_18_05.pth'\n",
    "            model_filename3 = 'logreg_0.5644valloss_21_08_2020_18_07.pth'\n",
    "            model2 = joblib.load(f'{models_path}{model_filename2}')\n",
    "            model3 = joblib.load(f'{models_path}{model_filename3}')\n",
    "            # Set the secondary random seed\n",
    "            random_seed_2 = 0\n",
    "            random_seed_3 = 100\n",
    "        # Set the main random seed\n",
    "        random_seed_1 = 42\n",
    "        # Set the use of an embedding layer\n",
    "        dataset_mode = 'one hot encoded'\n",
    "        # Set the use of delta_ts\n",
    "        use_delta_ts = False\n",
    "        # Set as a traditional ML model\n",
    "        ml_core = 'machine learning'\n",
    "    elif model_name == 'SVM':\n",
    "        # Set the model file and class names, then load the model\n",
    "        model_filename = 'svm_0.9078valloss_09_07_2020_02_55.pth'\n",
    "        model_class = 'SVM'\n",
    "        model = joblib.load(f'{models_path}{model_filename}')\n",
    "        if test_mode == 'aggregate':\n",
    "            model_filename2 = 'svm_0.8402valloss_21_08_2020_18_06.pth'\n",
    "            model_filename3 = 'svm_0.7443valloss_21_08_2020_18_07.pth'\n",
    "            model2 = joblib.load(f'{models_path}{model_filename2}')\n",
    "            model3 = joblib.load(f'{models_path}{model_filename3}')\n",
    "            # Set the secondary random seed\n",
    "            random_seed_2 = 0\n",
    "            random_seed_3 = 100\n",
    "        # Set the main random seed\n",
    "        random_seed_1 = 42\n",
    "        # Set the use of an embedding layer\n",
    "        dataset_mode = 'one hot encoded'\n",
    "        # Set the use of delta_ts\n",
    "        use_delta_ts = False\n",
    "        # Set as a traditional ML model\n",
    "        ml_core = 'machine learning'\n",
    "    print(model)\n",
    "    if test_mode == 'aggregate':\n",
    "        print(model2)\n",
    "        print(model3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the random seed for reproducibility:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "du.set_random_seed(random_seed_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessed data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALS_df = pd.read_csv(f'{data_path}FCUL_ALS_cleaned.csv')\n",
    "ALS_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove the `Unnamed: 0` column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALS_df.drop(columns=['Unnamed: 0'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALS_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ALS_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the maximum sequence length, so that the ML models and their related methods can handle all sequences, which have varying sequence lengths:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_length = ALS_df.groupby(id_column)[ts_column].count().max()\n",
    "total_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the label column, in case we're using a time window different than 90 days:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "if time_window_days is not 90:\n",
    "    # Recalculate the NIV label, based on the chosen time window\n",
    "    ALS_df[label_column] = utils.set_niv_label(ALS_df, time_window_days)\n",
    "    display(ALS_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove the `niv` column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALS_df.drop(columns=['niv'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the `delta_ts` (time variation between samples) if required:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pixiedust": {
     "displayParams": {
      "handlerId": "tableView"
     }
    }
   },
   "outputs": [],
   "source": [
    "if use_delta_ts is not False:\n",
    "    # Create a time variation column\n",
    "    ALS_df['delta_ts'] = ALS_df.groupby(id_column).ts.diff()\n",
    "    # Fill all the delta_ts missing values (the first value in a time series) with zeros\n",
    "    ALS_df['delta_ts'] = ALS_df['delta_ts'].fillna(0)\n",
    "if use_delta_ts == 'normalized':\n",
    "    # Normalize the time variation data\n",
    "    # NOTE: When using the MF2-LSTM model, since it assumes that the time\n",
    "    # variation is in days, we shouldn't normalize `delta_ts` with this model.\n",
    "    ALS_df['delta_ts'] = (ALS_df['delta_ts'] - ALS_df['delta_ts'].mean()) / ALS_df['delta_ts'].std()\n",
    "if use_delta_ts is not False:\n",
    "    ALS_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert into a padded tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = du.padding.dataframe_to_padded_tensor(ALS_df, padding_value=padding_value,\n",
    "                                             label_column=label_column, inplace=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the embedding configuration, if needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indices of the ID, timestamp and label columns\n",
    "id_column_idx = du.search_explore.find_col_idx(ALS_df, id_column)\n",
    "ts_column_idx = du.search_explore.find_col_idx(ALS_df, ts_column)\n",
    "label_column_idx = du.search_explore.find_col_idx(ALS_df, label_column)\n",
    "print(\n",
    "f'''ID index: {id_column_idx}\n",
    "Timestamp index: {ts_column_idx}\n",
    "Label index: {label_column_idx}'''\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset_mode == 'one hot encoded':\n",
    "    embed_features = None\n",
    "else:\n",
    "    embed_features = list()\n",
    "    if len(categ_feat_ohe.keys()) == 1:\n",
    "        for ohe_feature in list(categ_feat_ohe.values())[0]:\n",
    "            # Find the current feature's index so as to be able to use it as a tensor\n",
    "            feature_idx = du.search_explore.find_col_idx(ALS_df, ohe_feature)\n",
    "            # Decrease the index number if it's larger than the label column (which will be removed)\n",
    "            if feature_idx > label_column_idx:\n",
    "                feature_idx = feature_idx - 1\n",
    "            embed_features.append(feature_idx)\n",
    "    else:\n",
    "        for i in range(len(categ_feat_ohe.keys())):\n",
    "            tmp_list = list()\n",
    "            for ohe_feature in list(categ_feat_ohe.values())[i]:\n",
    "                # Find the current feature's index so as to be able to use it as a tensor\n",
    "                feature_idx = du.search_explore.find_col_idx(ALS_df, ohe_feature)\n",
    "                # Decrease the index number if it's larger than the label column (which will be removed)\n",
    "                if feature_idx > label_column_idx:\n",
    "                    feature_idx = feature_idx - 1\n",
    "                tmp_list.append(feature_idx)\n",
    "            # Add the current feature's list of one hot encoded columns\n",
    "            embed_features.append(tmp_list)\n",
    "print(f'Embedding features: {embed_features}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gather the feature names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = list(ALS_df.columns)\n",
    "feature_columns.remove('niv_label')\n",
    "if ml_core == 'machine learning':\n",
    "    feature_columns.remove('subject_id')\n",
    "    feature_columns.remove('ts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the dataset object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = du.datasets.Time_Series_Dataset(ALS_df, data, padding_value=padding_value,\n",
    "                                          label_name=label_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "dataset.__len__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separating into train and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure that we are using the right random seed\n",
    "du.set_random_seed(random_seed_1)\n",
    "(train_dataloader, val_dataloader, test_dataloader,\n",
    "train_indeces, val_indeces, test_indeces) = du.machine_learning.create_train_sets(dataset,\n",
    "                                                                                  test_train_ratio=test_train_ratio,\n",
    "                                                                                  validation_ratio=validation_ratio,\n",
    "                                                                                  batch_size=batch_size,\n",
    "                                                                                  get_indices=True)\n",
    "if test_mode == 'aggregate':\n",
    "    # Temporarily change the random seed to the one used in the second model\n",
    "    du.set_random_seed(random_seed_2)\n",
    "    # Create a separate data division\n",
    "    (train_dataloader, val_dataloader, test_dataloader,\n",
    "    train_indeces2, val_indeces2, test_indeces2) = du.machine_learning.create_train_sets(dataset,\n",
    "                                                                                         test_train_ratio=test_train_ratio,\n",
    "                                                                                         validation_ratio=validation_ratio,\n",
    "                                                                                         batch_size=batch_size,\n",
    "                                                                                         get_indices=True)\n",
    "    # Temporarily change the random seed to the one used in the second model\n",
    "    du.set_random_seed(random_seed_3)\n",
    "    # Create a separate data division\n",
    "    (train_dataloader, val_dataloader, test_dataloader,\n",
    "    train_indeces3, val_indeces3, test_indeces3) = du.machine_learning.create_train_sets(dataset,\n",
    "                                                                                         test_train_ratio=test_train_ratio,\n",
    "                                                                                         validation_ratio=validation_ratio,\n",
    "                                                                                         batch_size=batch_size,\n",
    "                                                                                         get_indices=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the full arrays of each set\n",
    "train_features, train_labels = dataset.X[train_indeces], dataset.y[train_indeces]\n",
    "val_features, val_labels = dataset.X[val_indeces], dataset.y[val_indeces]\n",
    "test_features, test_labels = dataset.X[test_indeces], dataset.y[test_indeces]\n",
    "all_features, all_labels = dataset.X, dataset.y\n",
    "if test_mode == 'aggregate':\n",
    "    train_features2, train_labels2 = dataset.X[train_indeces2], dataset.y[train_indeces2]\n",
    "    val_features2, val_labels2 = dataset.X[val_indeces2], dataset.y[val_indeces2]\n",
    "    test_features2, test_labels2 = dataset.X[test_indeces2], dataset.y[test_indeces2]\n",
    "    train_features3, train_labels3 = dataset.X[train_indeces3], dataset.y[train_indeces3]\n",
    "    val_features3, val_labels3 = dataset.X[val_indeces3], dataset.y[val_indeces3]\n",
    "    test_features3, test_labels3 = dataset.X[test_indeces3], dataset.y[test_indeces3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore the dataloaders, we only care about the full arrays when using scikit-learn or XGBoost\n",
    "del train_dataloader\n",
    "del val_dataloader\n",
    "del test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ml_core == 'machine learning':\n",
    "    # Remove the ID and timestamp columns from the data arrays\n",
    "    train_features = train_features[:, :, 2:]\n",
    "    val_features = val_features[:, :, 2:]\n",
    "    test_features = test_features[:, :, 2:]\n",
    "    # Reshape the data into a 2D format\n",
    "    train_features = train_features.reshape(-1, train_features.shape[-1])\n",
    "    val_features = val_features.reshape(-1, val_features.shape[-1])\n",
    "    test_features = test_features.reshape(-1, test_features.shape[-1])\n",
    "    all_features = all_features.reshape(-1, all_features.shape[-1])\n",
    "    train_labels = train_labels.reshape(-1)\n",
    "    val_labels = val_labels.reshape(-1)\n",
    "    test_labels = test_labels.reshape(-1)\n",
    "    all_labels = all_labels.reshape(-1)\n",
    "    # Remove padding samples from the data\n",
    "    train_features = train_features[[padding_value not in row for row in train_features]]\n",
    "    val_features = val_features[[padding_value not in row for row in val_features]]\n",
    "    test_features = test_features[[padding_value not in row for row in test_features]]\n",
    "    all_features = all_features[[padding_value not in row for row in all_features]]\n",
    "    train_labels = train_labels[[padding_value not in row for row in train_labels]]\n",
    "    val_labels = val_labels[[padding_value not in row for row in val_labels]]\n",
    "    test_labels = test_labels[[padding_value not in row for row in test_labels]]\n",
    "    all_labels = all_labels[[padding_value not in row for row in all_labels]]\n",
    "    # Convert from PyTorch tensor to NumPy array\n",
    "    train_features = train_features.numpy()\n",
    "    val_features = val_features.numpy()\n",
    "    test_features = test_features.numpy()\n",
    "    all_features = all_features.numpy()\n",
    "    train_labels = train_labels.numpy()\n",
    "    val_labels = val_labels.numpy()\n",
    "    test_labels = test_labels.numpy()\n",
    "    all_labels = all_labels.numpy()\n",
    "    if test_mode == 'aggregate':\n",
    "        # Model 2\n",
    "        # Remove the ID and timestamp columns from the data arrays\n",
    "        train_features2 = train_features2[:, :, 2:]\n",
    "        val_features2 = val_features2[:, :, 2:]\n",
    "        test_features2 = test_features2[:, :, 2:]\n",
    "        # Reshape the data into a 2D format\n",
    "        train_features2 = train_features2.reshape(-1, train_features2.shape[-1])\n",
    "        val_features2 = val_features2.reshape(-1, val_features2.shape[-1])\n",
    "        test_features2 = test_features2.reshape(-1, test_features2.shape[-1])\n",
    "        train_labels2 = train_labels2.reshape(-1)\n",
    "        val_labels2 = val_labels2.reshape(-1)\n",
    "        test_labels2 = test_labels2.reshape(-1)\n",
    "        # Remove padding samples from the data\n",
    "        train_features2 = train_features2[[padding_value not in row for row in train_features2]]\n",
    "        val_features2 = val_features2[[padding_value not in row for row in val_features2]]\n",
    "        test_features2 = test_features2[[padding_value not in row for row in test_features2]]\n",
    "        train_labels2 = train_labels2[[padding_value not in row for row in train_labels2]]\n",
    "        val_labels2 = val_labels2[[padding_value not in row for row in val_labels2]]\n",
    "        test_labels2 = test_labels2[[padding_value not in row for row in test_labels2]]\n",
    "        # Convert from PyTorch tensor to NumPy array\n",
    "        train_features2 = train_features2.numpy()\n",
    "        val_features2 = val_features2.numpy()\n",
    "        test_features2 = test_features2.numpy()\n",
    "        train_labels2 = train_labels2.numpy()\n",
    "        val_labels2 = val_labels2.numpy()\n",
    "        test_labels2 = test_labels2.numpy()\n",
    "        # Model 3\n",
    "        # Remove the ID and timestamp columns from the data arrays\n",
    "        train_features3 = train_features3[:, :, 2:]\n",
    "        val_features3 = val_features3[:, :, 2:]\n",
    "        test_features3 = test_features3[:, :, 2:]\n",
    "        # Reshape the data into a 2D format\n",
    "        train_features3 = train_features3.reshape(-1, train_features3.shape[-1])\n",
    "        val_features3 = val_features3.reshape(-1, val_features3.shape[-1])\n",
    "        test_features3 = test_features3.reshape(-1, test_features3.shape[-1])\n",
    "        train_labels3 = train_labels3.reshape(-1)\n",
    "        val_labels3 = val_labels3.reshape(-1)\n",
    "        test_labels3 = test_labels3.reshape(-1)\n",
    "        # Remove padding samples from the data\n",
    "        train_features3 = train_features3[[padding_value not in row for row in train_features3]]\n",
    "        val_features3 = val_features3[[padding_value not in row for row in val_features3]]\n",
    "        test_features3 = test_features3[[padding_value not in row for row in test_features3]]\n",
    "        train_labels3 = train_labels3[[padding_value not in row for row in train_labels3]]\n",
    "        val_labels3 = val_labels3[[padding_value not in row for row in val_labels3]]\n",
    "        test_labels3 = test_labels3[[padding_value not in row for row in test_labels3]]\n",
    "        # Convert from PyTorch tensor to NumPy array\n",
    "        train_features3 = train_features3.numpy()\n",
    "        val_features3 = val_features3.numpy()\n",
    "        test_features3 = test_features3.numpy()\n",
    "        train_labels3 = train_labels3.numpy()\n",
    "        val_labels3 = val_labels3.numpy()\n",
    "        test_labels3 = test_labels3.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "if ml_core == 'deep learning':\n",
    "    _, train_metrics = du.deep_learning.model_inference(model, data=(train_features, train_labels), \n",
    "                                                        metrics=['loss', 'accuracy', 'precision',\n",
    "                                                                'recall', 'F1', 'AUC'], \n",
    "                                                        model_type='multivariate_rnn', is_custom=is_custom, \n",
    "                                                        padding_value=padding_value)\n",
    "else:\n",
    "    train_metrics = dict()\n",
    "    train_pred_proba = model.predict_proba(train_features)\n",
    "    train_pred = model.predict(train_features)\n",
    "    train_metrics['AUC'] = roc_auc_score(train_labels, train_pred_proba[:, 1])\n",
    "    train_metrics['F1'] = f1_score(train_labels, train_pred, average='weighted')\n",
    "    train_metrics['accuracy'] = accuracy_score(train_labels, train_pred)\n",
    "    train_metrics['loss'] = log_loss(train_labels, train_pred_proba)\n",
    "    train_metrics['precision'] = precision_score(train_labels, train_pred)\n",
    "    train_metrics['recall'] = recall_score(train_labels, train_pred)\n",
    "train_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If doing an aggregate test, do inference on the other similar models and combine the metrics in mean and standard deviation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "if test_mode == 'aggregate':\n",
    "    if ml_core == 'deep learning':\n",
    "        _, train_metrics2 = du.deep_learning.model_inference(model2, data=(train_features2, train_labels2), \n",
    "                                                            metrics=['loss', 'accuracy', 'precision',\n",
    "                                                                    'recall', 'F1', 'AUC'], \n",
    "                                                            model_type='multivariate_rnn', is_custom=is_custom, \n",
    "                                                            padding_value=padding_value)\n",
    "        _, train_metrics3 = du.deep_learning.model_inference(model3, data=(train_features3, train_labels3), \n",
    "                                                            metrics=['loss', 'accuracy', 'precision',\n",
    "                                                                    'recall', 'F1', 'AUC'], \n",
    "                                                            model_type='multivariate_rnn', is_custom=is_custom, \n",
    "                                                            padding_value=padding_value)\n",
    "    else:\n",
    "        # Model 2\n",
    "        train_metrics2 = dict()\n",
    "        train_pred_proba2 = model.predict_proba(train_features2)\n",
    "        train_pred2 = model.predict(train_features2)\n",
    "        train_metrics2['AUC'] = roc_auc_score(train_labels2, train_pred_proba2[:, 1])\n",
    "        train_metrics2['F1'] = f1_score(train_labels2, train_pred2, average='weighted')\n",
    "        train_metrics2['accuracy'] = accuracy_score(train_labels2, train_pred2)\n",
    "        train_metrics2['loss'] = log_loss(train_labels2, train_pred_proba2)\n",
    "        train_metrics2['precision'] = precision_score(train_labels2, train_pred2)\n",
    "        train_metrics2['recall'] = recall_score(train_labels2, train_pred2)\n",
    "        # Model 3\n",
    "        train_metrics3 = dict()\n",
    "        train_pred_proba3 = model.predict_proba(train_features3)\n",
    "        train_pred3 = model.predict(train_features3)\n",
    "        train_metrics3['AUC'] = roc_auc_score(train_labels3, train_pred_proba3[:, 1])\n",
    "        train_metrics3['F1'] = f1_score(train_labels3, train_pred3, average='weighted')\n",
    "        train_metrics3['accuracy'] = accuracy_score(train_labels3, train_pred3)\n",
    "        train_metrics3['loss'] = log_loss(train_labels3, train_pred_proba3)\n",
    "        train_metrics3['precision'] = precision_score(train_labels3, train_pred3)\n",
    "        train_metrics3['recall'] = recall_score(train_labels3, train_pred3)\n",
    "    train_metrics2\n",
    "    train_metrics3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_mode == 'aggregate':\n",
    "    train_metrics_agg = dict()\n",
    "    for key in train_metrics.keys():\n",
    "        # Create the current key\n",
    "        train_metrics_agg[key] = dict()\n",
    "        # Add the mean\n",
    "        train_metrics_agg[key]['mean'] = np.mean([train_metrics[key], train_metrics2[key], train_metrics3[key]]).item()\n",
    "        # Add the standard deviation\n",
    "        train_metrics_agg[key]['std'] = np.std([train_metrics[key], train_metrics2[key], train_metrics3[key]]).item()\n",
    "    train_metrics_agg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ml_core == 'deep learning':\n",
    "    _, val_metrics = du.deep_learning.model_inference(model, data=(val_features, val_labels), \n",
    "                                                        metrics=['loss', 'accuracy', 'precision',\n",
    "                                                                'recall', 'F1', 'AUC'], \n",
    "                                                        model_type='multivariate_rnn', is_custom=is_custom, \n",
    "                                                        padding_value=padding_value)\n",
    "else:\n",
    "    val_metrics = dict()\n",
    "    val_pred_proba = model.predict_proba(val_features)\n",
    "    val_pred = model.predict(val_features)\n",
    "    val_metrics['AUC'] = roc_auc_score(val_labels, val_pred_proba[:, 1])\n",
    "    val_metrics['F1'] = f1_score(val_labels, val_pred, average='weighted')\n",
    "    val_metrics['accuracy'] = accuracy_score(val_labels, val_pred)\n",
    "    val_metrics['loss'] = log_loss(val_labels, val_pred_proba)\n",
    "    val_metrics['precision'] = precision_score(val_labels, val_pred)\n",
    "    val_metrics['recall'] = recall_score(val_labels, val_pred)\n",
    "val_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If doing an aggregate test, do inference on the other similar models and combine the metrics in mean and standard deviation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "if test_mode == 'aggregate':\n",
    "    if ml_core == 'deep learning':\n",
    "        _, val_metrics2 = du.deep_learning.model_inference(model2, data=(val_features2, val_labels2), \n",
    "                                                           metrics=['loss', 'accuracy', 'precision',\n",
    "                                                                   'recall', 'F1', 'AUC'], \n",
    "                                                           model_type='multivariate_rnn', is_custom=is_custom, \n",
    "                                                           padding_value=padding_value)\n",
    "        _, val_metrics3 = du.deep_learning.model_inference(model3, data=(val_features3, val_labels3), \n",
    "                                                           metrics=['loss', 'accuracy', 'precision',\n",
    "                                                                   'recall', 'F1', 'AUC'], \n",
    "                                                           model_type='multivariate_rnn', is_custom=is_custom, \n",
    "                                                           padding_value=padding_value)\n",
    "    else:\n",
    "        # Model 2\n",
    "        val_metrics2 = dict()\n",
    "        val_pred_proba2 = model.predict_proba(val_features2)\n",
    "        val_pred2 = model.predict(val_features2)\n",
    "        val_metrics2['AUC'] = roc_auc_score(val_labels2, val_pred_proba2[:, 1])\n",
    "        val_metrics2['F1'] = f1_score(val_labels2, val_pred2, average='weighted')\n",
    "        val_metrics2['accuracy'] = accuracy_score(val_labels2, val_pred2)\n",
    "        val_metrics2['loss'] = log_loss(val_labels2, val_pred_proba2)\n",
    "        val_metrics2['precision'] = precision_score(val_labels2, val_pred2)\n",
    "        val_metrics2['recall'] = recall_score(val_labels2, val_pred2)\n",
    "        # Model 3\n",
    "        val_metrics3 = dict()\n",
    "        val_pred_proba3 = model.predict_proba(val_features3)\n",
    "        val_pred3 = model.predict(val_features3)\n",
    "        val_metrics3['AUC'] = roc_auc_score(val_labels3, val_pred_proba3[:, 1])\n",
    "        val_metrics3['F1'] = f1_score(val_labels3, val_pred3, average='weighted')\n",
    "        val_metrics3['accuracy'] = accuracy_score(val_labels3, val_pred3)\n",
    "        val_metrics3['loss'] = log_loss(val_labels3, val_pred_proba3)\n",
    "        val_metrics3['precision'] = precision_score(val_labels3, val_pred3)\n",
    "        val_metrics3['recall'] = recall_score(val_labels3, val_pred3)\n",
    "    val_metrics2\n",
    "    val_metrics3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_mode == 'aggregate':\n",
    "    val_metrics_agg = dict()\n",
    "    for key in val_metrics.keys():\n",
    "        # Create the current key\n",
    "        val_metrics_agg[key] = dict()\n",
    "        # Add the mean\n",
    "        val_metrics_agg[key]['mean'] = np.mean([val_metrics[key], val_metrics2[key], val_metrics3[key]]).item()\n",
    "        # Add the standard deviation\n",
    "        val_metrics_agg[key]['std'] = np.std([val_metrics[key], val_metrics2[key], val_metrics3[key]]).item()\n",
    "    val_metrics_agg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ml_core == 'deep learning':\n",
    "    _, test_metrics = du.deep_learning.model_inference(model, data=(test_features, test_labels), \n",
    "                                                        metrics=['loss', 'accuracy', 'precision',\n",
    "                                                                'recall', 'F1', 'AUC'], \n",
    "                                                        model_type='multivariate_rnn', is_custom=is_custom, \n",
    "                                                        padding_value=padding_value)\n",
    "else:\n",
    "    test_metrics = dict()\n",
    "    test_pred_proba = model.predict_proba(test_features)\n",
    "    test_pred = model.predict(test_features)\n",
    "    test_metrics['AUC'] = roc_auc_score(test_labels, test_pred_proba[:, 1])\n",
    "    test_metrics['F1'] = f1_score(test_labels, test_pred, average='weighted')\n",
    "    test_metrics['accuracy'] = accuracy_score(test_labels, test_pred)\n",
    "    test_metrics['loss'] = log_loss(test_labels, test_pred_proba)\n",
    "    test_metrics['precision'] = precision_score(test_labels, test_pred)\n",
    "    test_metrics['recall'] = recall_score(test_labels, test_pred)\n",
    "test_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If doing an aggregate test, do inference on the other similar models and combine the metrics in mean and standard deviation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "if test_mode == 'aggregate':\n",
    "    if ml_core == 'deep learning':\n",
    "        _, test_metrics2 = du.deep_learning.model_inference(model2, data=(test_features2, test_labels2), \n",
    "                                                            metrics=['loss', 'accuracy', 'precision',\n",
    "                                                                    'recall', 'F1', 'AUC'], \n",
    "                                                            model_type='multivariate_rnn', is_custom=is_custom, \n",
    "                                                            padding_value=padding_value)\n",
    "        _, test_metrics3 = du.deep_learning.model_inference(model3, data=(test_features3, test_labels3), \n",
    "                                                            metrics=['loss', 'accuracy', 'precision',\n",
    "                                                                    'recall', 'F1', 'AUC'], \n",
    "                                                            model_type='multivariate_rnn', is_custom=is_custom, \n",
    "                                                            padding_value=padding_value)\n",
    "    else:\n",
    "        # Model 2\n",
    "        test_metrics2 = dict()\n",
    "        test_pred_proba2 = model.predict_proba(test_features2)\n",
    "        test_pred2 = model.predict(test_features2)\n",
    "        test_metrics2['AUC'] = roc_auc_score(test_labels2, test_pred_proba2[:, 1])\n",
    "        test_metrics2['F1'] = f1_score(test_labels2, test_pred2, average='weighted')\n",
    "        test_metrics2['accuracy'] = accuracy_score(test_labels2, test_pred2)\n",
    "        test_metrics2['loss'] = log_loss(test_labels2, test_pred_proba2)\n",
    "        test_metrics2['precision'] = precision_score(test_labels2, test_pred2)\n",
    "        test_metrics2['recall'] = recall_score(test_labels2, test_pred2)\n",
    "        # Model 3\n",
    "        test_metrics3 = dict()\n",
    "        test_pred_proba3 = model.predict_proba(test_features3)\n",
    "        test_pred3 = model.predict(test_features3)\n",
    "        test_metrics3['AUC'] = roc_auc_score(test_labels3, test_pred_proba3[:, 1])\n",
    "        test_metrics3['F1'] = f1_score(test_labels3, test_pred3, average='weighted')\n",
    "        test_metrics3['accuracy'] = accuracy_score(test_labels3, test_pred3)\n",
    "        test_metrics3['loss'] = log_loss(test_labels3, test_pred_proba3)\n",
    "        test_metrics3['precision'] = precision_score(test_labels3, test_pred3)\n",
    "        test_metrics3['recall'] = recall_score(test_labels3, test_pred3)\n",
    "    test_metrics2\n",
    "    test_metrics3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_mode == 'aggregate':\n",
    "    test_metrics_agg = dict()\n",
    "    for key in test_metrics.keys():\n",
    "        # Create the current key\n",
    "        test_metrics_agg[key] = dict()\n",
    "        # Add the mean\n",
    "        test_metrics_agg[key]['mean'] = np.mean([test_metrics[key], test_metrics2[key], test_metrics3[key]]).item()\n",
    "        # Add the standard deviation\n",
    "        test_metrics_agg[key]['std'] = np.std([test_metrics[key], test_metrics2[key], test_metrics3[key]]).item()\n",
    "    test_metrics_agg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving a dictionary with all the metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join all the sets' metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = dict()\n",
    "if test_mode == 'aggregate':\n",
    "    metrics['train'] = train_metrics_agg\n",
    "    metrics['val'] = val_metrics_agg\n",
    "    metrics['test'] = test_metrics_agg\n",
    "else:\n",
    "    metrics['train'] = train_metrics\n",
    "    metrics['val'] = val_metrics\n",
    "    metrics['test'] = test_metrics\n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save in a YAML file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file_name_no_ext = model_filename.split('.pth')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_mode == 'aggregate':\n",
    "    file_path = f'{metrics_path}aggregate/'\n",
    "    model_file_name_yml = model_file_name_no_ext.split('.')[0][:-2]\n",
    "else:\n",
    "    file_path = f'{metrics_path}individual_models/'\n",
    "    model_file_name_yml = model_file_name_no_ext\n",
    "model_file_name_yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_stream = open(f'{file_path}{model_file_name_yml}.yml', 'w')\n",
    "yaml.dump(metrics, metrics_stream, default_flow_style=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-4.m49",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-4:m49"
  },
  "jupytext": {
   "cell_metadata_filter": "-all",
   "cell_metadata_json": true,
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "ipynb,py:light",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "fcul_als_disease_progression",
   "language": "python",
   "name": "fcul_als_disease_progression"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
