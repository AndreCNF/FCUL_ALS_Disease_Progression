{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FCUL ALS LaTeX Metrics\n",
    "---\n",
    "\n",
    "Joining the metrics of the models trained on the ALS dataset from Faculdade de CiÃªncias da Universidade de Lisboa (FCUL) with the data from over 1000 patients collected in Portugal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KOdmFzXqF7nq"
   },
   "source": [
    "## Importing the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os                                  # os handles directory/workspace changes\n",
    "import yaml                                # Save and load YAML files\n",
    "import plotly.graph_objs as go             # Plotly for interactive and pretty plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pixiedust                           # Debugging in Jupyter Notebook cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the metrics\n",
    "metrics_path = 'GitHub/FCUL_ALS_Disease_Progression/metrics/aggregate/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to parent directory (presumably \"Documents\")\n",
    "os.chdir(\"../../..\")\n",
    "import pandas as pd                        # Pandas to load and handle the data\n",
    "import data_utils as du                    # Data science and machine learning relevant methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "du.set_pandas_library(lib='pandas')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Allow pandas to show more columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 3000)\n",
    "pd.set_option('display.max_rows', 3000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the random seed for reproducibility:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "du.set_random_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Allow Jupyter Lab to display all outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_files = os.listdir(metrics_path)\n",
    "try:\n",
    "    metrics_files.remove('.DS_Store')\n",
    "except:\n",
    "    pass\n",
    "metrics_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dictionary with all the metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = dict()\n",
    "for file_name in metrics_files:\n",
    "    # Load the current metrics file\n",
    "    stream = open(f'{metrics_path}{file_name}', 'r')\n",
    "    model_metrics = yaml.load(stream, Loader=yaml.FullLoader)\n",
    "    # Remove the extension from the name\n",
    "    file_name = file_name.split('.yml')[0]\n",
    "    # Define the model name which will appear in the table\n",
    "    model_name = ''\n",
    "    if 'bidir' in file_name:\n",
    "        model_name = 'Bidirectional '\n",
    "    if 'tlstm' in file_name:\n",
    "        model_name += 'TLSTM'\n",
    "    elif 'mf1lstm' in file_name:\n",
    "        model_name += 'MF1-LSTM'\n",
    "    elif 'mf2lstm' in file_name:\n",
    "        model_name += 'MF2-LSTM'\n",
    "    elif 'lstm' in file_name:\n",
    "        model_name += 'LSTM'\n",
    "    elif 'rnn' in file_name:\n",
    "        model_name += 'RNN'\n",
    "    elif 'xgb' in file_name:\n",
    "        model_name += 'XGBoost'\n",
    "    elif 'logreg' in file_name:\n",
    "        model_name += 'Logistic Regression'\n",
    "    elif 'svm' in file_name:\n",
    "        model_name += 'SVM'\n",
    "    if 'embed' in file_name:\n",
    "        model_name += ', embedded'\n",
    "    if 'delta_ts' in file_name:\n",
    "        model_name += ', time aware'\n",
    "    # Create a dictionary entry for the current model\n",
    "    metrics[model_name] = dict()\n",
    "    metrics[model_name]['Avg. Test AUC'] = model_metrics['test']['AUC']['mean']\n",
    "    metrics[model_name]['Std. Test AUC'] = model_metrics['test']['AUC']['std']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert to a dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df = pd.DataFrame(metrics)\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transpose to have a row per model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df = metrics_df.transpose()\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sort by a descending order of performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df = metrics_df.sort_values('Avg. Test AUC', ascending=False)\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert to a LaTeX table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df.to_latex()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Component impact\n",
    "\n",
    "Measuring the average gain in performance that we get from the components of bidirectionality, embedding layer and time awareness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = list(metrics_df.index)\n",
    "model_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "component_gains = dict()\n",
    "components_str = dict(bidirectionality='Bidirectional ', \n",
    "                      embedding=', embedded', \n",
    "                      time_awareness=', time aware')\n",
    "for component in components_str.keys():\n",
    "    # Find and match the names of the models with and without the component\n",
    "    models_without_comp = [model_name.replace(components_str[component], '') \n",
    "                           for model_name in model_names \n",
    "                           if components_str[component] in model_name]\n",
    "    models_with_comp = [model_name \n",
    "                        for model_name in model_names \n",
    "                        if components_str[component] in model_name]\n",
    "    model_comp_names_match = dict(zip(models_without_comp, models_with_comp))\n",
    "    curr_component_gains = list()\n",
    "    for model_name in models_without_comp:\n",
    "        # Calculate the difference in model performance with and without the component\n",
    "        component_gain = (metrics_df.loc[model_comp_names_match[model_name], 'Avg. Test AUC'] \n",
    "                          - metrics_df.loc[model_name, 'Avg. Test AUC'])\n",
    "        curr_component_gains.append(component_gain)\n",
    "    # Average the component's effect\n",
    "    component_gains[component] = sum(curr_component_gains) / len(curr_component_gains)\n",
    "component_gains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find and match the names of the models with LSTM and with RNN\n",
    "models_with_lstm = [model_name.replace('RNN', 'LSTM')\n",
    "                    for model_name in model_names \n",
    "                    if 'RNN' in model_name]\n",
    "models_with_rnn = [model_name \n",
    "                   for model_name in model_names \n",
    "                   if 'RNN' in model_name]\n",
    "model_comp_names_match = dict(zip(models_with_rnn, models_with_lstm))\n",
    "curr_component_gains = list()\n",
    "for model_name in models_with_rnn:\n",
    "    # Calculate the difference in model performance with LSTM and with RNN\n",
    "    component_gain = (metrics_df.loc[model_comp_names_match[model_name], 'Avg. Test AUC'] \n",
    "                      - metrics_df.loc[model_name, 'Avg. Test AUC'])\n",
    "    curr_component_gains.append(component_gain)\n",
    "# Average LSTM's effect\n",
    "component_gains['LSTM'] = sum(curr_component_gains) / len(curr_component_gains)\n",
    "component_gains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert to a dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gain_df = pd.Series(component_gains, name='Avg. Impact on Test AUC')\n",
    "gain_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gain_df.index = ['Bidirectionality', 'Embedding', 'Time Awareness', 'LSTM']\n",
    "gain_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gain_df.index.rename('Component')\n",
    "gain_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sort by a descending order of performance gain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gain_df = gain_df.sort_values(ascending=False)\n",
    "gain_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert to a LaTeX table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gain_df.to_latex()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a bar plot, similar to SHAP's summary plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gain_plot_df = gain_df.copy()\n",
    "gain_plot_df = gain_plot_df.sort_values(ascending=True)\n",
    "# Define the colors based on the value\n",
    "marker_color = ['rgba(255,13,87,1)' \n",
    "                if gain_plot_df[comp] > 0\n",
    "                else 'rgba(30,136,229,1)'\n",
    "                for comp in gain_plot_df.index ]\n",
    "# Create the figure\n",
    "figure=dict(\n",
    "    data=[dict(\n",
    "        type='bar',\n",
    "        x=gain_plot_df,\n",
    "        y=gain_plot_df.index,\n",
    "        orientation='h',\n",
    "        marker=dict(color=marker_color)\n",
    "    )],\n",
    "    layout=dict(\n",
    "        paper_bgcolor='white',\n",
    "        plot_bgcolor='white',\n",
    "        title='Average impact on model\\'s test AUC',\n",
    "        yaxis_title=gain_plot_df.index.name,\n",
    "        font=dict(\n",
    "            family='Roboto',\n",
    "            size=14,\n",
    "            color='black'\n",
    "        )\n",
    "    )\n",
    ")\n",
    "go.Figure(figure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "ipynb,py:light",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "fcul_als_disease_progression",
   "language": "python",
   "name": "fcul_als_disease_progression"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
