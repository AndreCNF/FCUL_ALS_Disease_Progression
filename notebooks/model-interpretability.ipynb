{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FCUL ALS Model Interpretability\n",
    "---\n",
    "\n",
    "Exploring the ALS dataset from Faculdade de CiÃªncias da Universidade de Lisboa (FCUL) with the data from over 1000 patients collected in Portugal.\n",
    "\n",
    "Using different interpretability approaches so as to understand the outputs of the models trained on FCUL's ALS dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KOdmFzXqF7nq"
   },
   "source": [
    "## Importing the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G5RrWE9R_Nkl"
   },
   "outputs": [],
   "source": [
    "import os                                  # os handles directory/workspace changes\n",
    "import torch                               # PyTorch to create and apply deep learning models\n",
    "import xgboost as xgb                      # Gradient boosting trees models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "import joblib                              # Save and load scikit-learn models in disk\n",
    "import pickle                              # Save python objects in files\n",
    "import yaml                                # Save and load YAML files\n",
    "from datetime import datetime              # datetime to use proper date and time formats\n",
    "from ipywidgets import interact            # Display selectors and sliders\n",
    "import shap                                # Model-agnostic interpretability package inspired on Shapley values\n",
    "import plotly.graph_objs as go             # Plotly for interactive and pretty plots\n",
    "from model_interpreter.model_interpreter import ModelInterpreter # Class that enables the interpretation of models that handle variable sequence length input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pixiedust                           # Debugging in Jupyter Notebook cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the parquet dataset files\n",
    "data_path = 'Datasets/Thesis/FCUL_ALS/cleaned/'\n",
    "# Path to the data + SHAP values dataframes\n",
    "data_n_shap_path = 'Datasets/Thesis/FCUL_ALS/interpreted/'\n",
    "# Path to the code files\n",
    "project_path = 'GitHub/FCUL_ALS_Disease_Progression/'\n",
    "# Path to the models\n",
    "models_path = f'{project_path}models/'\n",
    "# Path to the model interpreters\n",
    "interpreters_path = f'{project_path}interpreters/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to the scripts directory\n",
    "os.chdir(\"../scripts/\")\n",
    "import utils                               # Context specific (in this case, for the ALS data) methods\n",
    "import Models                              # Deep learning models\n",
    "# Change to parent directory (presumably \"Documents\")\n",
    "os.chdir(\"../../..\")\n",
    "import pandas as pd                        # Pandas to load and handle the data\n",
    "import data_utils as du                    # Data science and machine learning relevant methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "du.set_pandas_library(lib='pandas')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Allow pandas to show more columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 3000)\n",
    "pd.set_option('display.max_rows', 3000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the random seed for reproducibility:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "du.set_random_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_column = 'subject_id'                   # Name of the sequence ID column\n",
    "ts_column = 'ts'                           # Name of the timestamp column\n",
    "label_column = 'niv_label'                 # Name of the label column\n",
    "padding_value = 999999                     # Padding value used to fill in sequences up to the maximum sequence length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One hot encoding columns categorization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_categ_feat_ohe = open(f'{data_path}categ_feat_ohe.yml', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categ_feat_ohe = yaml.load(stream_categ_feat_ohe, Loader=yaml.FullLoader)\n",
    "categ_feat_ohe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(categ_feat_ohe.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Dataloaders parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_train_ratio = 0.25                    # Percentage of the data which will be used as a test set\n",
    "validation_ratio = 0.1                     # Percentage of the data from the training set which is used for validation purposes\n",
    "batch_size = 32                            # Number of unit stays in a mini batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model to interpret:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_filename = None                      # Name of the file containing the model that will be loaded\n",
    "model_class = None                         # Python class name that corresponds to the chosen model's type\n",
    "model = None                               # Machine learning model object\n",
    "dataset_mode = 'one hot encoded'           # The mode in which we'll use the data, either one hot encoded or pre-embedded\n",
    "ml_core = 'deep learning'                  # The core machine learning type we'll use; either traditional ML or DL\n",
    "use_delta_ts = False                       # Indicates if we'll use time variation info\n",
    "time_window_days = 90                      # Number of days on which we want to predict NIV\n",
    "is_custom = False                          # Indicates if the model being used is a custom built one\n",
    "@interact\n",
    "def get_dataset_mode(model_name=['Bidirectional LSTM with embedding layer',\n",
    "                                 'Bidirectional RNN with embedding layer and delta_ts',\n",
    "                                 'Bidirectional LSTM with delta_ts',\n",
    "                                 'Regular LSTM',\n",
    "                                 'MF1-LSTM',\n",
    "                                 'XGBoost',\n",
    "                                 'Logistic regression']):\n",
    "    global model_filename, model_class, model, dataset_mode, ml_core, use_delta_ts, time_window_days, is_custom\n",
    "    if model_name == 'Bidirectional LSTM with embedding layer':\n",
    "        # Set the model file and class names, then load the model\n",
    "        model_filename = 'lstm_bidir_pre_embedded_90dayswindow_0.2490valloss_06_07_2020_03_47.pth'\n",
    "        model_class = 'VanillaLSTM'\n",
    "        model = du.deep_learning.load_checkpoint(f'{models_path}{model_filename}', getattr(Models, model_class))\n",
    "        # Set the use of an embedding layer\n",
    "        dataset_mode = 'pre-embedded'\n",
    "        # Set it as a custom model\n",
    "        is_custom = True\n",
    "    elif model_name == 'Bidirectional RNN with embedding layer and delta_ts':\n",
    "        # Set the model file and class names, then load the model\n",
    "        model_filename = 'rnn_bidir_pre_embedded_delta_ts_90dayswindow_0.3059valloss_06_07_2020_03_10.pth'\n",
    "        model_class = 'VanillaRNN'\n",
    "        model = du.deep_learning.load_checkpoint(f'{models_path}{model_filename}', getattr(Models, model_class))\n",
    "        # Set the use of an embedding layer\n",
    "        dataset_mode = 'pre-embedded'\n",
    "        # Set the use of delta_ts\n",
    "        use_delta_ts = 'normalized'\n",
    "        # Set it as a custom model\n",
    "        is_custom = True\n",
    "    elif model_name == 'Bidirectional LSTM with delta_ts':\n",
    "        # Set the model file and class names, then load the model\n",
    "        model_filename = 'lstm_bidir_pre_embedded_delta_ts_90dayswindow_0.3481valloss_06_07_2020_04_15.pth'\n",
    "        model_class = 'VanillaLSTM'\n",
    "        model = du.deep_learning.load_checkpoint(f'{models_path}{model_filename}', getattr(Models, model_class))\n",
    "        # Set the use of delta_ts\n",
    "        use_delta_ts = 'normalized'\n",
    "        # Set it as a custom model\n",
    "        is_custom = True\n",
    "    elif model_name == 'Regular LSTM':\n",
    "        # Set the model file and class names, then load the model\n",
    "        model_filename = 'lstm_one_hot_encoded_90dayswindow_0.4363valloss_06_07_2020_03_28.pth'\n",
    "        model_class = 'VanillaLSTM'\n",
    "        model = du.deep_learning.load_checkpoint(f'{models_path}{model_filename}', getattr(Models, model_class))\n",
    "        # Set it as a custom model\n",
    "        is_custom = True\n",
    "    elif model_name == 'MF1-LSTM':\n",
    "        # Set the model file and class names, then load the model\n",
    "        model_filename = 'mf1lstm_one_hot_encoded_90dayswindow_0.6009valloss_07_07_2020_03_46.pth'\n",
    "        model_class = 'MF1LSTM'\n",
    "        model = du.deep_learning.load_checkpoint(f'{models_path}{model_filename}', getattr(Models, model_class))\n",
    "        # Set the use of delta_ts\n",
    "        use_delta_ts = 'normalized'\n",
    "        # Set it as a custom model\n",
    "        is_custom = True\n",
    "    elif model_name == 'XGBoost':\n",
    "        # Set the model file and class names, then load the model\n",
    "        model_filename = 'xgb_0.5926valloss_09_07_2020_02_40.pth'\n",
    "        model_class = 'XGBoost'\n",
    "        model = xgb.XGBClassifier()\n",
    "        model.load_model(f'{models_path}{model_filename}')\n",
    "        # Set as a traditional ML model\n",
    "        ml_core = 'machine learning'\n",
    "    elif model_name == 'Logistic regression':\n",
    "        # Set the model file and class names, then load the model\n",
    "        model_filename = 'logreg_0.6210valloss_09_07_2020_02_54.pth'\n",
    "        model_class = 'logreg'\n",
    "        model = joblib.load(f'{models_path}{model_filename}')\n",
    "        # Set as a traditional ML model\n",
    "        ml_core = 'machine learning'\n",
    "    print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALS_df = pd.read_csv(f'{data_path}FCUL_ALS_cleaned.csv')\n",
    "ALS_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove the `Unnamed: 0` column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALS_df.drop(columns=['Unnamed: 0'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALS_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ALS_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the maximum sequence length, so that the ML models and their related methods can handle all sequences, which have varying sequence lengths:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_length = ALS_df.groupby(id_column)[ts_column].count().max()\n",
    "total_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the label column, in case we're using a time window different than 90 days:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "if time_window_days is not 90:\n",
    "    # Recalculate the NIV label, based on the chosen time window\n",
    "    ALS_df[label_column] = utils.set_niv_label(ALS_df, time_window_days)\n",
    "    display(ALS_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove the `niv` column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALS_df.drop(columns=['niv'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the `delta_ts` (time variation between samples) if required:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pixiedust": {
     "displayParams": {
      "handlerId": "tableView"
     }
    }
   },
   "outputs": [],
   "source": [
    "if use_delta_ts is not False:\n",
    "    # Create a time variation column\n",
    "    ALS_df['delta_ts'] = ALS_df.groupby(id_column).ts.diff()\n",
    "    # Fill all the delta_ts missing values (the first value in a time series) with zeros\n",
    "    ALS_df['delta_ts'] = ALS_df['delta_ts'].fillna(0)\n",
    "if use_delta_ts == 'normalized':\n",
    "    # Normalize the time variation data\n",
    "    # NOTE: When using the MF2-LSTM model, since it assumes that the time\n",
    "    # variation is in days, we shouldn't normalize `delta_ts` with this model.\n",
    "    ALS_df['delta_ts'] = (ALS_df['delta_ts'] - ALS_df['delta_ts'].mean()) / ALS_df['delta_ts'].std()\n",
    "if use_delta_ts is not False:\n",
    "    display(ALS_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert into a padded tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = du.padding.dataframe_to_padded_tensor(ALS_df, padding_value=padding_value,\n",
    "                                             label_column=label_column, inplace=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the embedding configuration, if needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indices of the ID, timestamp and label columns\n",
    "id_column_idx = du.search_explore.find_col_idx(ALS_df, id_column)\n",
    "ts_column_idx = du.search_explore.find_col_idx(ALS_df, ts_column)\n",
    "label_column_idx = du.search_explore.find_col_idx(ALS_df, label_column)\n",
    "print(\n",
    "f'''ID index: {id_column_idx}\n",
    "Timestamp index: {ts_column_idx}\n",
    "Label index: {label_column_idx}'''\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset_mode == 'one hot encoded':\n",
    "    embed_features = None\n",
    "else:\n",
    "    embed_features = list()\n",
    "    if len(categ_feat_ohe.keys()) == 1:\n",
    "        for ohe_feature in list(categ_feat_ohe.values())[0]:\n",
    "            # Find the current feature's index so as to be able to use it as a tensor\n",
    "            feature_idx = du.search_explore.find_col_idx(ALS_df, ohe_feature)\n",
    "            # Decrease the index number if it's larger than the label column (which will be removed)\n",
    "            if feature_idx > label_column_idx:\n",
    "                feature_idx = feature_idx - 1\n",
    "            embed_features.append(feature_idx)\n",
    "    else:\n",
    "        for i in range(len(categ_feat_ohe.keys())):\n",
    "            tmp_list = list()\n",
    "            for ohe_feature in list(categ_feat_ohe.values())[i]:\n",
    "                # Find the current feature's index so as to be able to use it as a tensor\n",
    "                feature_idx = du.search_explore.find_col_idx(ALS_df, ohe_feature)\n",
    "                # Decrease the index number if it's larger than the label column (which will be removed)\n",
    "                if feature_idx > label_column_idx:\n",
    "                    feature_idx = feature_idx - 1\n",
    "                tmp_list.append(feature_idx)\n",
    "            # Add the current feature's list of one hot encoded columns\n",
    "            embed_features.append(tmp_list)\n",
    "print(f'Embedding features: {embed_features}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the dataset object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = du.datasets.Time_Series_Dataset(ALS_df, data, padding_value=padding_value,\n",
    "                                          label_name=label_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "dataset.__len__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separating into train and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_dataloader, val_dataloader, test_dataloader,\n",
    "train_indeces, val_indeces, test_indeces) = du.machine_learning.create_train_sets(dataset,\n",
    "                                                                                  test_train_ratio=test_train_ratio,\n",
    "                                                                                  validation_ratio=validation_ratio,\n",
    "                                                                                  batch_size=batch_size,\n",
    "                                                                                  get_indices=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the full arrays of each set\n",
    "train_features, train_labels = dataset.X[train_indeces], dataset.y[train_indeces]\n",
    "val_features, val_labels = dataset.X[val_indeces], dataset.y[val_indeces]\n",
    "test_features, test_labels = dataset.X[test_indeces], dataset.y[test_indeces]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore the dataloaders, we only care about the full arrays when using scikit-learn or XGBoost\n",
    "del train_dataloader\n",
    "del val_dataloader\n",
    "del test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ml_core == 'machine learning':\n",
    "    # Remove the ID and timestamp columns from the data arrays\n",
    "    train_features = train_features[:, :, 2:]\n",
    "    val_features = val_features[:, :, 2:]\n",
    "    test_features = test_features[:, :, 2:]\n",
    "    # Reshape the data into a 2D format\n",
    "    train_features = train_features.reshape(-1, train_features.shape[-1])\n",
    "    val_features = val_features.reshape(-1, val_features.shape[-1])\n",
    "    test_features = test_features.reshape(-1, test_features.shape[-1])\n",
    "    train_labels = train_labels.reshape(-1)\n",
    "    val_labels = val_labels.reshape(-1)\n",
    "    test_labels = test_labels.reshape(-1)\n",
    "    # Remove padding samples from the data\n",
    "    train_features = train_features[[padding_value not in row for row in train_features]]\n",
    "    val_features = val_features[[padding_value not in row for row in val_features]]\n",
    "    test_features = test_features[[padding_value not in row for row in test_features]]\n",
    "    train_labels = train_labels[[padding_value not in row for row in train_labels]]\n",
    "    val_labels = val_labels[[padding_value not in row for row in val_labels]]\n",
    "    test_labels = test_labels[[padding_value not in row for row in test_labels]]\n",
    "    # Convert from PyTorch tensor to NumPy array\n",
    "    train_features = train_features.numpy()\n",
    "    val_features = val_features.numpy()\n",
    "    test_features = test_features.numpy()\n",
    "    train_labels = train_labels.numpy()\n",
    "    val_labels = val_labels.numpy()\n",
    "    test_labels = test_labels.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the interpreter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ml_core == 'deep learning':\n",
    "    # Calculating the number of times to re-evaluate the model when explaining each prediction,\n",
    "    # based on SHAP's formula of nsamples = 2 * n_features + 2048\n",
    "    SHAP_bkgnd_samples = 2 * test_features.shape[-1] + 2048\n",
    "    print(SHAP_bkgnd_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "if ml_core == 'deep learning':\n",
    "    interpreter = ModelInterpreter(model, ALS_df, model_type='multivariate_rnn', id_column=0, \n",
    "                                   inst_column=1, fast_calc=True, SHAP_bkgnd_samples=SHAP_bkgnd_samples,\n",
    "                                   random_seed=du.random_seed, padding_value=padding_value,\n",
    "                                   is_custom=is_custom, total_length=total_length)\n",
    "# else:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the feature importance scores (through SHAP values):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "if ml_core == 'deep learning':\n",
    "    _ = interpreter.interpret_model(test_data=test_features[:5],\n",
    "                                    test_labels=test_labels[:5],\n",
    "                                    instance_importance=False, \n",
    "                                    feature_importance='shap')\n",
    "# else:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ml_core == 'deep learning':\n",
    "    print(interpreter.explainer.expected_value)\n",
    "else:\n",
    "    print(explainer.expected_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving a dataframe with the resulting SHAP values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter.feat_scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "du.visualization.shap_summary_plot(interpreter.feat_scores, interpreter.feat_names, max_display=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "du.visualization.shap_waterfall_plot(interpreter.explainer.expected_value[0], interpreter.feat_scores[0, 2, :], \n",
    "                                     interpreter.test_data[0, 2, 2:], interpreter.feat_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "if ml_core == 'deep learning':\n",
    "    data_n_shap_df = interpreter.shap_values_df()\n",
    "    data_n_shap_df.head()\n",
    "# else:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_n_shap_df.to_csv(f'{data_n_shap_path}fcul_als_with_shap_for_{model_filename}.csv')"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_json": true,
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "fcul_als_disease_progression",
   "language": "python",
   "name": "fcul_als_disease_progression"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
