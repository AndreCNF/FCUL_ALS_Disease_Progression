{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FCUL ALS Model Interpretability\n",
    "---\n",
    "\n",
    "Interpreting models trained on the ALS dataset from Faculdade de CiÃªncias da Universidade de Lisboa (FCUL) with the data from over 1000 patients collected in Portugal.\n",
    "\n",
    "Using different interpretability approaches so as to understand the outputs of the models trained on FCUL's ALS dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KOdmFzXqF7nq"
   },
   "source": [
    "## Importing the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G5RrWE9R_Nkl"
   },
   "outputs": [],
   "source": [
    "import os                                  # os handles directory/workspace changes\n",
    "import numpy as np                         # NumPy to handle numeric and NaN operations\n",
    "import torch                               # PyTorch to create and apply deep learning models\n",
    "import xgboost as xgb                      # Gradient boosting trees models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "import joblib                              # Save and load scikit-learn models in disk\n",
    "import pickle                              # Save python objects in files\n",
    "import yaml                                # Save and load YAML files\n",
    "from datetime import datetime              # datetime to use proper date and time formats\n",
    "from ipywidgets import interact            # Display selectors and sliders\n",
    "import shap                                # Model-agnostic interpretability package inspired on Shapley values\n",
    "import plotly.graph_objs as go             # Plotly for interactive and pretty plots\n",
    "from model_interpreter.model_interpreter import ModelInterpreter # Class that enables the interpretation of models that handle variable sequence length input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pixiedust                           # Debugging in Jupyter Notebook cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the parquet dataset files\n",
    "data_path = 'Datasets/Thesis/FCUL_ALS/cleaned/'\n",
    "# Path to the data + SHAP values dataframes\n",
    "data_n_shap_path = 'Datasets/Thesis/FCUL_ALS/interpreted/'\n",
    "# Path to the code files\n",
    "project_path = 'GitHub/FCUL_ALS_Disease_Progression/'\n",
    "# Path to the models\n",
    "models_path = f'{project_path}models/'\n",
    "# Path to the model interpreters\n",
    "interpreters_path = f'{project_path}interpreters/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to the scripts directory\n",
    "os.chdir(\"../scripts/\")\n",
    "import utils                               # Context specific (in this case, for the ALS data) methods\n",
    "import Models                              # Deep learning models\n",
    "# Change to parent directory (presumably \"Documents\")\n",
    "os.chdir(\"../../..\")\n",
    "import pandas as pd                        # Pandas to load and handle the data\n",
    "import data_utils as du                    # Data science and machine learning relevant methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "du.set_pandas_library(lib='pandas')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Allow pandas to show more columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 3000)\n",
    "pd.set_option('display.max_rows', 3000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the random seed for reproducibility:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "du.set_random_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Allow Jupyter Lab to display all outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_column = 'subject_id'                   # Name of the sequence ID column\n",
    "ts_column = 'ts'                           # Name of the timestamp column\n",
    "label_column = 'niv_label'                 # Name of the label column\n",
    "padding_value = 999999                     # Padding value used to fill in sequences up to the maximum sequence length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One hot encoding columns categorization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_categ_feat_ohe = open(f'{data_path}categ_feat_ohe.yml', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categ_feat_ohe = yaml.load(stream_categ_feat_ohe, Loader=yaml.FullLoader)\n",
    "categ_feat_ohe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(categ_feat_ohe.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalization stats (allow us to get back the original values, to display in the interpretability plots):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_norm_stats = open(f'{data_path}norm_stats.yml', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_stats = yaml.load(stream_norm_stats, Loader=yaml.FullLoader)\n",
    "norm_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list(norm_stats.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "means = dict([(feat, norm_stats[feat]['mean']) for feat in norm_stats])\n",
    "means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stds = dict([(feat, norm_stats[feat]['std']) for feat in norm_stats])\n",
    "stds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Dataloaders parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_train_ratio = 0.25                    # Percentage of the data which will be used as a test set\n",
    "validation_ratio = 0.1                     # Percentage of the data from the training set which is used for validation purposes\n",
    "batch_size = 32                            # Number of unit stays in a mini batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model to interpret:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_filename = None                      # Name of the file containing the model that will be loaded\n",
    "model_class = None                         # Python class name that corresponds to the chosen model's type\n",
    "model = None                               # Machine learning model object\n",
    "dataset_mode = 'one hot encoded'           # The mode in which we'll use the data, either one hot encoded or pre-embedded\n",
    "ml_core = 'deep learning'                  # The core machine learning type we'll use; either traditional ML or DL\n",
    "use_delta_ts = False                       # Indicates if we'll use time variation info\n",
    "time_window_days = 90                      # Number of days on which we want to predict NIV\n",
    "is_custom = False                          # Indicates if the model being used is a custom built one\n",
    "random_seed = None                         # Model random seed\n",
    "@interact\n",
    "def get_dataset_mode(model_name=['Bidirectional LSTM with embedding layer and delta_ts',\n",
    "                                 'Bidirectional LSTM with embedding layer',\n",
    "                                 'Bidirectional LSTM with delta_ts',\n",
    "                                 'Bidirectional LSTM',\n",
    "                                 'LSTM with embedding layer and delta_ts',\n",
    "                                 'LSTM with embedding layer',\n",
    "                                 'LSTM with delta_ts',\n",
    "                                 'LSTM',\n",
    "                                 'Bidirectional RNN with embedding layer and delta_ts',\n",
    "                                 'Bidirectional RNN with embedding layer',\n",
    "                                 'Bidirectional RNN with delta_ts',\n",
    "                                 'Bidirectional RNN',\n",
    "                                 'RNN with embedding layer and delta_ts',\n",
    "                                 'RNN with embedding layer',\n",
    "                                 'RNN with delta_ts',\n",
    "                                 'RNN',\n",
    "                                 'MF1-LSTM with embedding layer',\n",
    "                                 'MF1-LSTM',\n",
    "                                 'MF2-LSTM with embedding layer',\n",
    "                                 'MF2-LSTM',\n",
    "                                 'TLSTM with embedding layer',\n",
    "                                 'TLSTM',\n",
    "                                 'XGBoost',\n",
    "                                 'Logistic regression',\n",
    "                                 'SVM']):\n",
    "    global model_filename, model_class, model, dataset_mode, ml_core, use_delta_ts \n",
    "    global time_window_days, is_custom, random_seed\n",
    "    if model_name == 'Bidirectional LSTM with embedding layer and delta_ts':\n",
    "        # Set the model file and class names, then load the model\n",
    "        model_filename = 'lstm_bidir_pre_embedded_delta_ts_90dayswindow_0.3705valloss_08_07_2020_04_04.pth'\n",
    "        model_class = 'VanillaLSTM'\n",
    "        model = du.deep_learning.load_checkpoint(f'{models_path}{model_filename}', getattr(Models, model_class))\n",
    "        # Set the main random seed\n",
    "        random_seed = 0\n",
    "        # Set the use of an embedding layer\n",
    "        dataset_mode = 'pre-embedded'\n",
    "        # Set the use of delta_ts\n",
    "        use_delta_ts = 'normalized'\n",
    "        # Set it as not a custom model\n",
    "        is_custom = False\n",
    "        # Set as a traditional ML model\n",
    "        ml_core = 'deep learning'\n",
    "    elif model_name == 'Bidirectional LSTM with embedding layer':\n",
    "        # Set the model file and class names, then load the model\n",
    "        model_filename = 'lstm_bidir_pre_embedded_90dayswindow_0.2490valloss_06_07_2020_03_47.pth'\n",
    "        model_class = 'VanillaLSTM'\n",
    "        model = du.deep_learning.load_checkpoint(f'{models_path}{model_filename}', getattr(Models, model_class))\n",
    "        # Set the main random seed\n",
    "        random_seed = 42\n",
    "        # Set the use of an embedding layer\n",
    "        dataset_mode = 'pre-embedded'\n",
    "        # Set the use of delta_ts\n",
    "        use_delta_ts = False\n",
    "        # Set it as not a custom model\n",
    "        is_custom = False\n",
    "        # Set as a traditional ML model\n",
    "        ml_core = 'deep learning'\n",
    "    elif model_name == 'Bidirectional LSTM with delta_ts':\n",
    "        # Set the model file and class names, then load the model\n",
    "        model_filename = 'lstm_bidir_one_hot_encoded_delta_ts_90dayswindow_0.3784valloss_08_07_2020_04_14.pth'\n",
    "        model_class = 'VanillaLSTM'\n",
    "        model = du.deep_learning.load_checkpoint(f'{models_path}{model_filename}', getattr(Models, model_class))\n",
    "        # Set the main random seed\n",
    "        random_seed = 0\n",
    "        # Set the use of an embedding layer\n",
    "        dataset_mode = 'one hot encoded'\n",
    "        # Set the use of delta_ts\n",
    "        use_delta_ts = 'normalized'\n",
    "        # Set it as not a custom model\n",
    "        is_custom = False\n",
    "        # Set as a traditional ML model\n",
    "        ml_core = 'deep learning'\n",
    "    elif model_name == 'Bidirectional LSTM':\n",
    "        # Set the model file and class names, then load the model\n",
    "        model_filename = 'lstm_bidir_one_hot_encoded_90dayswindow_0.4497valloss_08_07_2020_04_31.pth'\n",
    "        model_class = 'VanillaLSTM'\n",
    "        model = du.deep_learning.load_checkpoint(f'{models_path}{model_filename}', getattr(Models, model_class))\n",
    "        # Set the main random seed\n",
    "        random_seed = 0\n",
    "        # Set the use of an embedding layer\n",
    "        dataset_mode = 'one hot encoded'\n",
    "        # Set the use of delta_ts\n",
    "        use_delta_ts = False\n",
    "        # Set it as not a custom model\n",
    "        is_custom = False\n",
    "        # Set as a traditional ML model\n",
    "        ml_core = 'deep learning'\n",
    "    elif model_name == 'LSTM with embedding layer and delta_ts':\n",
    "        # Set the model file and class names, then load the model\n",
    "        model_filename = 'lstm_pre_embedded_delta_ts_90dayswindow_0.5071valloss_21_08_2020_05_03.pth'\n",
    "        model_class = 'VanillaLSTM'\n",
    "        model = du.deep_learning.load_checkpoint(f'{models_path}{model_filename}', getattr(Models, model_class))\n",
    "        # Set the main random seed\n",
    "        random_seed = 100\n",
    "        # Set the use of an embedding layer\n",
    "        dataset_mode = 'pre-embedded'\n",
    "        # Set the use of delta_ts\n",
    "        use_delta_ts = 'normalized'\n",
    "        # Set it as not a custom model\n",
    "        is_custom = False\n",
    "        # Set as a traditional ML model\n",
    "        ml_core = 'deep learning'\n",
    "    elif model_name == 'LSTM with embedding layer':\n",
    "        # Set the model file and class names, then load the model\n",
    "        model_filename = 'lstm_pre_embedded_90dayswindow_0.5898valloss_06_07_2020_03_21.pth'\n",
    "        model_class = 'VanillaLSTM'\n",
    "        model = du.deep_learning.load_checkpoint(f'{models_path}{model_filename}', getattr(Models, model_class))\n",
    "        # Set the main random seed\n",
    "        random_seed = 42\n",
    "        # Set the use of an embedding layer\n",
    "        dataset_mode = 'pre-embedded'\n",
    "        # Set the use of delta_ts\n",
    "        use_delta_ts = False\n",
    "        # Set it as not a custom model\n",
    "        is_custom = False\n",
    "        # Set as a traditional ML model\n",
    "        ml_core = 'deep learning'\n",
    "    elif model_name == 'LSTM with delta_ts':\n",
    "        # Set the model file and class names, then load the model\n",
    "        model_filename = 'lstm_one_hot_encoded_delta_ts_90dayswindow_0.5178valloss_06_07_2020_04_02.pth'\n",
    "        model_class = 'VanillaLSTM'\n",
    "        model = du.deep_learning.load_checkpoint(f'{models_path}{model_filename}', getattr(Models, model_class))\n",
    "        # Set the main random seed\n",
    "        random_seed = 42\n",
    "        # Set the use of an embedding layer\n",
    "        dataset_mode = 'one hot encoded'\n",
    "        # Set the use of delta_ts\n",
    "        use_delta_ts = 'normalized'\n",
    "        # Set it as not a custom model\n",
    "        is_custom = False\n",
    "        # Set as a traditional ML model\n",
    "        ml_core = 'deep learning'\n",
    "    elif model_name == 'LSTM':\n",
    "        # Set the model file and class names, then load the model\n",
    "        model_filename = 'lstm_one_hot_encoded_90dayswindow_0.5125valloss_08_07_2020_04_41.pth'\n",
    "        model_class = 'VanillaLSTM'\n",
    "        model = du.deep_learning.load_checkpoint(f'{models_path}{model_filename}', getattr(Models, model_class))\n",
    "        # Set the main random seed\n",
    "        random_seed = 100\n",
    "        # Set the use of an embedding layer\n",
    "        dataset_mode = 'one hot encoded'\n",
    "        # Set the use of delta_ts\n",
    "        use_delta_ts = False\n",
    "        # Set it as not a custom model\n",
    "        is_custom = False\n",
    "        # Set as a traditional ML model\n",
    "        ml_core = 'deep learning'\n",
    "    elif model_name == 'Bidirectional RNN with embedding layer and delta_ts':\n",
    "        # Set the model file and class names, then load the model\n",
    "        model_filename = 'rnn_bidir_pre_embedded_delta_ts_90dayswindow_0.3579valloss_08_07_2020_03_55.pth'\n",
    "        model_class = 'VanillaRNN'\n",
    "        model = du.deep_learning.load_checkpoint(f'{models_path}{model_filename}', getattr(Models, model_class))\n",
    "        # Set the main random seed\n",
    "        random_seed = 100\n",
    "        # Set the use of an embedding layer\n",
    "        dataset_mode = 'pre-embedded'\n",
    "        # Set the use of delta_ts\n",
    "        use_delta_ts = 'normalized'\n",
    "        # Set it as not a custom model\n",
    "        is_custom = False\n",
    "        # Set as a traditional ML model\n",
    "        ml_core = 'deep learning'\n",
    "    elif model_name == 'Bidirectional RNN with embedding layer':\n",
    "        # Set the model file and class names, then load the model\n",
    "        model_filename = 'rnn_bidir_pre_embedded_90dayswindow_0.4005valloss_08_07_2020_03_43.pth'\n",
    "        model_class = 'VanillaRNN'\n",
    "        model = du.deep_learning.load_checkpoint(f'{models_path}{model_filename}', getattr(Models, model_class))\n",
    "        # Set the main random seed\n",
    "        random_seed = 0\n",
    "        # Set the use of an embedding layer\n",
    "        dataset_mode = 'pre-embedded'\n",
    "        # Set the use of delta_ts\n",
    "        use_delta_ts = False\n",
    "        # Set it as not a custom model\n",
    "        is_custom = False\n",
    "        # Set as a traditional ML model\n",
    "        ml_core = 'deep learning'\n",
    "    elif model_name == 'Bidirectional RNN with delta_ts':\n",
    "        # Set the model file and class names, then load the model\n",
    "        model_filename = 'rnn_bidir_one_hot_encoded_delta_ts_90dayswindow_0.3631valloss_08_07_2020_04_21.pth'\n",
    "        model_class = 'VanillaRNN'\n",
    "        model = du.deep_learning.load_checkpoint(f'{models_path}{model_filename}', getattr(Models, model_class))\n",
    "        # Set the main random seed\n",
    "        random_seed = 100\n",
    "        # Set the use of an embedding layer\n",
    "        dataset_mode = 'one hot encoded'\n",
    "        # Set the use of delta_ts\n",
    "        use_delta_ts = 'normalized'\n",
    "        # Set it as not a custom model\n",
    "        is_custom = False\n",
    "        # Set as a traditional ML model\n",
    "        ml_core = 'deep learning'\n",
    "    elif model_name == 'Bidirectional RNN':\n",
    "        # Set the model file and class names, then load the model\n",
    "        model_filename = 'rnn_bidir_one_hot_encoded_90dayswindow_0.3713valloss_08_07_2020_04_49.pth'\n",
    "        model_class = 'VanillaRNN'\n",
    "        model = du.deep_learning.load_checkpoint(f'{models_path}{model_filename}', getattr(Models, model_class))\n",
    "        # Set the main random seed\n",
    "        random_seed = 100\n",
    "        # Set the use of an embedding layer\n",
    "        dataset_mode = 'one hot encoded'\n",
    "        # Set the use of delta_ts\n",
    "        use_delta_ts = False\n",
    "        # Set it as not a custom model\n",
    "        is_custom = False\n",
    "        # Set as a traditional ML model\n",
    "        ml_core = 'deep learning'\n",
    "    elif model_name == 'RNN with embedding layer and delta_ts':\n",
    "        # Set the model file and class names, then load the model\n",
    "        model_filename = 'rnn_pre_embedded_delta_ts_90dayswindow_0.5267valloss_21_08_2020_04_19.pth'\n",
    "        model_class = 'VanillaRNN'\n",
    "        model = du.deep_learning.load_checkpoint(f'{models_path}{model_filename}', getattr(Models, model_class))\n",
    "        # Set the main random seed\n",
    "        random_seed = 0\n",
    "        # Set the use of an embedding layer\n",
    "        dataset_mode = 'pre-embedded'\n",
    "        # Set the use of delta_ts\n",
    "        use_delta_ts = 'normalized'\n",
    "        # Set it as not a custom model\n",
    "        is_custom = False\n",
    "        # Set as a traditional ML model\n",
    "        ml_core = 'deep learning'\n",
    "    elif model_name == 'RNN with embedding layer':\n",
    "        # Set the model file and class names, then load the model\n",
    "        model_filename = 'rnn_pre_embedded_90dayswindow_0.5238valloss_21_08_2020_04_39.pth'\n",
    "        model_class = 'VanillaRNN'\n",
    "        model = du.deep_learning.load_checkpoint(f'{models_path}{model_filename}', getattr(Models, model_class))\n",
    "        # Set the main random seed\n",
    "        random_seed = 0\n",
    "        # Set the use of an embedding layer\n",
    "        dataset_mode = 'pre-embedded'\n",
    "        # Set the use of delta_ts\n",
    "        use_delta_ts = False\n",
    "        # Set it as not a custom model\n",
    "        is_custom = False\n",
    "        # Set as a traditional ML model\n",
    "        ml_core = 'deep learning'\n",
    "    elif model_name == 'RNN with delta_ts':\n",
    "        # Set the model file and class names, then load the model\n",
    "        model_filename = 'rnn_one_hot_encoded_delta_ts_90dayswindow_0.5354valloss_21_08_2020_04_24.pth'\n",
    "        model_class = 'VanillaRNN'\n",
    "        model = du.deep_learning.load_checkpoint(f'{models_path}{model_filename}', getattr(Models, model_class))\n",
    "        # Set the main random seed\n",
    "        random_seed = 0\n",
    "        # Set the use of an embedding layer\n",
    "        dataset_mode = 'one hot encoded'\n",
    "        # Set the use of delta_ts\n",
    "        use_delta_ts = 'normalized'\n",
    "        # Set it as not a custom model\n",
    "        is_custom = False\n",
    "        # Set as a traditional ML model\n",
    "        ml_core = 'deep learning'\n",
    "    elif model_name == 'RNN':\n",
    "        # Set the model file and class names, then load the model\n",
    "        model_filename = 'rnn_one_hot_encoded_90dayswindow_0.5445valloss_21_08_2020_04_34.pth'\n",
    "        model_class = 'VanillaRNN'\n",
    "        model = du.deep_learning.load_checkpoint(f'{models_path}{model_filename}', getattr(Models, model_class))\n",
    "        # Set the main random seed\n",
    "        random_seed = 0\n",
    "        # Set the use of an embedding layer\n",
    "        dataset_mode = 'one hot encoded'\n",
    "        # Set the use of delta_ts\n",
    "        use_delta_ts = False\n",
    "        # Set it as not a custom model\n",
    "        is_custom = False\n",
    "        # Set as a traditional ML model\n",
    "        ml_core = 'deep learning'\n",
    "    elif model_name == 'MF1-LSTM with embedding layer':\n",
    "        # Set the model file and class names, then load the model\n",
    "        model_filename = 'mf1lstm_pre_embedded_90dayswindow_0.6516valloss_07_07_2020_03_35.pth'\n",
    "        model_class = 'MF1LSTM'\n",
    "        model = du.deep_learning.load_checkpoint(f'{models_path}{model_filename}', getattr(Models, model_class))\n",
    "        # Set the main random seed\n",
    "        random_seed = 42\n",
    "        # Set the use of an embedding layer\n",
    "        dataset_mode = 'pre-embedded'\n",
    "        # Set the use of delta_ts\n",
    "        use_delta_ts = False\n",
    "        # Set it as a custom model\n",
    "        is_custom = True\n",
    "        # Set as a traditional ML model\n",
    "        ml_core = 'deep learning'\n",
    "    elif model_name == 'MF1-LSTM':\n",
    "        # Set the model file and class names, then load the model\n",
    "        model_filename = 'mf1lstm_one_hot_encoded_90dayswindow_0.6009valloss_07_07_2020_03_46.pth'\n",
    "        model_class = 'MF1LSTM'\n",
    "        model = du.deep_learning.load_checkpoint(f'{models_path}{model_filename}', getattr(Models, model_class))\n",
    "        # Set the main random seed\n",
    "        random_seed = 42\n",
    "        # Set the use of an embedding layer\n",
    "        dataset_mode = 'one hot encoded'\n",
    "        # Set the use of delta_ts\n",
    "        use_delta_ts = 'normalized'\n",
    "        # Set it as a custom model\n",
    "        is_custom = True\n",
    "        # Set as a traditional ML model\n",
    "        ml_core = 'deep learning'\n",
    "    elif model_name == 'MF2-LSTM with embedding layer':\n",
    "        # Set the model file and class names, then load the model\n",
    "        model_filename = 'mf2lstm_pre_embedded_90dayswindow_0.6388valloss_07_07_2020_03_54.pth'\n",
    "        model_class = 'MF2LSTM'\n",
    "        model = du.deep_learning.load_checkpoint(f'{models_path}{model_filename}', getattr(Models, model_class))\n",
    "        # Set the main random seed\n",
    "        random_seed = 42\n",
    "        # Set the use of an embedding layer\n",
    "        dataset_mode = 'pre-embedded'\n",
    "        # Set the use of delta_ts\n",
    "        use_delta_ts = False\n",
    "        # Set it as a custom model\n",
    "        is_custom = True\n",
    "        # Set as a traditional ML model\n",
    "        ml_core = 'deep learning'\n",
    "    elif model_name == 'MF2-LSTM':\n",
    "        # Set the model file and class names, then load the model\n",
    "        model_filename = 'mf2lstm_one_hot_encoded_90dayswindow_0.5918valloss_07_07_2020_03_58.pth'\n",
    "        model_class = 'MF2LSTM'\n",
    "        model = du.deep_learning.load_checkpoint(f'{models_path}{model_filename}', getattr(Models, model_class))\n",
    "        # Set the main random seed\n",
    "        random_seed = 42\n",
    "        # Set the use of an embedding layer\n",
    "        dataset_mode = 'one hot encoded'\n",
    "        # Set the use of delta_ts\n",
    "        use_delta_ts = 'raw'\n",
    "        # Set it as a custom model\n",
    "        is_custom = True\n",
    "        # Set as a traditional ML model\n",
    "        ml_core = 'deep learning'\n",
    "    elif model_name == 'TLSTM with embedding layer':\n",
    "        # Set the model file and class names, then load the model\n",
    "        model_filename = 'tlstm_pre_embedded_90dayswindow_0.6503valloss_07_07_2020_03_03.pth'\n",
    "        model_class = 'TLSTM'\n",
    "        model = du.deep_learning.load_checkpoint(f'{models_path}{model_filename}', getattr(Models, model_class))\n",
    "        # Set the main random seed\n",
    "        random_seed = 42\n",
    "        # Set the use of an embedding layer\n",
    "        dataset_mode = 'pre-embedded'\n",
    "        # Set the use of delta_ts\n",
    "        use_delta_ts = False\n",
    "        # Set it as a custom model\n",
    "        is_custom = True\n",
    "        # Set as a traditional ML model\n",
    "        ml_core = 'deep learning'\n",
    "    elif model_name == 'TLSTM':\n",
    "        # Set the model file and class names, then load the model\n",
    "        model_filename = 'tlstm_one_hot_encoded_90dayswindow_0.6153valloss_07_07_2020_03_13.pth'\n",
    "        model_class = 'TLSTM'\n",
    "        model = du.deep_learning.load_checkpoint(f'{models_path}{model_filename}', getattr(Models, model_class))\n",
    "        # Set the main random seed\n",
    "        random_seed = 42\n",
    "        # Set the use of an embedding layer\n",
    "        dataset_mode = 'one hot encoded'\n",
    "        # Set the use of delta_ts\n",
    "        use_delta_ts = 'normalized'\n",
    "        # Set it as a custom model\n",
    "        is_custom = True\n",
    "        # Set as a traditional ML model\n",
    "        ml_core = 'deep learning'\n",
    "    elif model_name == 'XGBoost':\n",
    "        # Set the model file and class names, then load the model\n",
    "        model_filename = 'xgb_0.5926valloss_09_07_2020_02_40.pth'\n",
    "        model_class = 'XGBoost'\n",
    "        model = xgb.XGBClassifier()\n",
    "        model.load_model(f'{models_path}{model_filename}')\n",
    "        # Set the main random seed\n",
    "        random_seed = 42\n",
    "        # Set the use of an embedding layer\n",
    "        dataset_mode = 'one hot encoded'\n",
    "        # Set the use of delta_ts\n",
    "        use_delta_ts = False\n",
    "        # Set as a traditional ML model\n",
    "        ml_core = 'machine learning'\n",
    "    elif model_name == 'Logistic regression':\n",
    "        # Set the model file and class names, then load the model\n",
    "        model_filename = 'logreg_0.6210valloss_09_07_2020_02_54.pth'\n",
    "        model_class = 'LogReg'\n",
    "        model = joblib.load(f'{models_path}{model_filename}')\n",
    "        # Set the main random seed\n",
    "        random_seed = 42\n",
    "        # Set the use of an embedding layer\n",
    "        dataset_mode = 'one hot encoded'\n",
    "        # Set the use of delta_ts\n",
    "        use_delta_ts = False\n",
    "        # Set as a traditional ML model\n",
    "        ml_core = 'machine learning'\n",
    "    elif model_name == 'SVM':\n",
    "        # Set the model file and class names, then load the model\n",
    "        model_filename = 'svm_0.9078valloss_09_07_2020_02_55.pth'\n",
    "        model_class = 'SVM'\n",
    "        model = joblib.load(f'{models_path}{model_filename}')\n",
    "        random_seed = 42\n",
    "        # Set the use of an embedding layer\n",
    "        dataset_mode = 'one hot encoded'\n",
    "        # Set the use of delta_ts\n",
    "        use_delta_ts = False\n",
    "        # Set as a traditional ML model\n",
    "        ml_core = 'machine learning'\n",
    "    print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_ALS_df = pd.read_csv(f'{data_path}FCUL_ALS_cleaned_denorm.csv')\n",
    "orig_ALS_df.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "orig_ALS_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessed data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALS_df = pd.read_csv(f'{data_path}FCUL_ALS_cleaned.csv')\n",
    "ALS_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove the `Unnamed: 0` column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALS_df.drop(columns=['Unnamed: 0'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALS_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ALS_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the maximum sequence length, so that the ML models and their related methods can handle all sequences, which have varying sequence lengths:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_length = ALS_df.groupby(id_column)[ts_column].count().max()\n",
    "total_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the label column, in case we're using a time window different than 90 days:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "if time_window_days is not 90:\n",
    "    # Recalculate the NIV label, based on the chosen time window\n",
    "    ALS_df[label_column] = utils.set_niv_label(ALS_df, time_window_days)\n",
    "    display(ALS_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove the `niv` column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALS_df.drop(columns=['niv'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the `delta_ts` (time variation between samples) if required:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pixiedust": {
     "displayParams": {
      "handlerId": "tableView"
     }
    }
   },
   "outputs": [],
   "source": [
    "if use_delta_ts is not False:\n",
    "    # Create a time variation column\n",
    "    ALS_df['delta_ts'] = ALS_df.groupby(id_column).ts.diff()\n",
    "    # Fill all the delta_ts missing values (the first value in a time series) with zeros\n",
    "    ALS_df['delta_ts'] = ALS_df['delta_ts'].fillna(0)\n",
    "if use_delta_ts == 'normalized':\n",
    "    # Add delta_ts' normalization stats to the dictionaries\n",
    "    means['delta_ts'] = ALS_df['delta_ts'].mean()\n",
    "    stds['delta_ts'] = ALS_df['delta_ts'].std()\n",
    "    # Normalize the time variation data\n",
    "    # NOTE: When using the MF2-LSTM model, since it assumes that the time\n",
    "    # variation is in days, we shouldn't normalize `delta_ts` with this model.\n",
    "    ALS_df['delta_ts'] = (ALS_df['delta_ts'] - means['delta_ts']) / stds['delta_ts']\n",
    "else:\n",
    "    # Ignore delta_ts' normalization stats to the dictionaries\n",
    "    means['delta_ts'] = 0\n",
    "    stds['delta_ts'] = 1\n",
    "if use_delta_ts is not False:\n",
    "    ALS_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert into a padded tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = du.padding.dataframe_to_padded_tensor(ALS_df, padding_value=padding_value,\n",
    "                                             label_column=label_column, inplace=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the embedding configuration, if needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indices of the ID, timestamp and label columns\n",
    "id_column_idx = du.search_explore.find_col_idx(ALS_df, id_column)\n",
    "ts_column_idx = du.search_explore.find_col_idx(ALS_df, ts_column)\n",
    "label_column_idx = du.search_explore.find_col_idx(ALS_df, label_column)\n",
    "print(\n",
    "f'''ID index: {id_column_idx}\n",
    "Timestamp index: {ts_column_idx}\n",
    "Label index: {label_column_idx}'''\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset_mode == 'one hot encoded':\n",
    "    embed_features = None\n",
    "else:\n",
    "    embed_features = list()\n",
    "    if len(categ_feat_ohe.keys()) == 1:\n",
    "        for ohe_feature in list(categ_feat_ohe.values())[0]:\n",
    "            # Find the current feature's index so as to be able to use it as a tensor\n",
    "            feature_idx = du.search_explore.find_col_idx(ALS_df, ohe_feature)\n",
    "            # Decrease the index number if it's larger than the label column (which will be removed)\n",
    "            if feature_idx > label_column_idx:\n",
    "                feature_idx = feature_idx - 1\n",
    "            embed_features.append(feature_idx)\n",
    "    else:\n",
    "        for i in range(len(categ_feat_ohe.keys())):\n",
    "            tmp_list = list()\n",
    "            for ohe_feature in list(categ_feat_ohe.values())[i]:\n",
    "                # Find the current feature's index so as to be able to use it as a tensor\n",
    "                feature_idx = du.search_explore.find_col_idx(ALS_df, ohe_feature)\n",
    "                # Decrease the index number if it's larger than the label column (which will be removed)\n",
    "                if feature_idx > label_column_idx:\n",
    "                    feature_idx = feature_idx - 1\n",
    "                tmp_list.append(feature_idx)\n",
    "            # Add the current feature's list of one hot encoded columns\n",
    "            embed_features.append(tmp_list)\n",
    "print(f'Embedding features: {embed_features}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gather the feature names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = list(ALS_df.columns)\n",
    "feature_columns.remove('niv_label')\n",
    "if ml_core == 'machine learning':\n",
    "    feature_columns.remove('subject_id')\n",
    "    feature_columns.remove('ts')\n",
    "    model_feature_columns = feature_columns\n",
    "else:\n",
    "    model_feature_columns = feature_columns.copy()\n",
    "    model_feature_columns.remove('subject_id')\n",
    "    model_feature_columns.remove('ts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the dataset object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = du.datasets.Time_Series_Dataset(ALS_df, data, padding_value=padding_value,\n",
    "                                          label_name=label_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "dataset.__len__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separating into train and validation sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the random seed for reproducibility:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "du.set_random_seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_dataloader, val_dataloader, test_dataloader,\n",
    "train_indeces, val_indeces, test_indeces) = du.machine_learning.create_train_sets(dataset,\n",
    "                                                                                  test_train_ratio=test_train_ratio,\n",
    "                                                                                  validation_ratio=validation_ratio,\n",
    "                                                                                  batch_size=batch_size,\n",
    "                                                                                  get_indices=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the full arrays of each set\n",
    "train_features, train_labels = dataset.X[train_indeces], dataset.y[train_indeces]\n",
    "val_features, val_labels = dataset.X[val_indeces], dataset.y[val_indeces]\n",
    "test_features, test_labels = dataset.X[test_indeces], dataset.y[test_indeces]\n",
    "all_features, all_labels = dataset.X, dataset.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore the dataloaders, we only care about the full arrays when using scikit-learn or XGBoost\n",
    "del train_dataloader\n",
    "del val_dataloader\n",
    "del test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ml_core == 'machine learning':\n",
    "    # Reshape the data into a 2D format\n",
    "    train_features = train_features.reshape(-1, train_features.shape[-1])\n",
    "    val_features = val_features.reshape(-1, val_features.shape[-1])\n",
    "    test_features = test_features.reshape(-1, test_features.shape[-1])\n",
    "    all_features = all_features.reshape(-1, all_features.shape[-1])\n",
    "    train_labels = train_labels.reshape(-1)\n",
    "    val_labels = val_labels.reshape(-1)\n",
    "    test_labels = test_labels.reshape(-1)\n",
    "    all_labels = all_labels.reshape(-1)\n",
    "    # Remove padding samples from the data\n",
    "    train_features = train_features[[padding_value not in row for row in train_features]]\n",
    "    val_features = val_features[[padding_value not in row for row in val_features]]\n",
    "    test_features = test_features[[padding_value not in row for row in test_features]]\n",
    "    all_features = all_features[[padding_value not in row for row in all_features]]\n",
    "    train_labels = train_labels[[padding_value not in row for row in train_labels]]\n",
    "    val_labels = val_labels[[padding_value not in row for row in val_labels]]\n",
    "    test_labels = test_labels[[padding_value not in row for row in test_labels]]\n",
    "    all_labels = all_labels[[padding_value not in row for row in all_labels]]\n",
    "    # Convert from PyTorch tensor to NumPy array\n",
    "    train_features = train_features.numpy()\n",
    "    val_features = val_features.numpy()\n",
    "    test_features = test_features.numpy()\n",
    "    all_features = all_features.numpy()\n",
    "    train_labels = train_labels.numpy()\n",
    "    val_labels = val_labels.numpy()\n",
    "    test_labels = test_labels.numpy()\n",
    "    all_labels = all_labels.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the original, denormalized test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_delta_ts is False:\n",
    "    # Prevent the identifier columns from being denormalized\n",
    "    columns_to_remove = [id_column, ts_column, 'delta_ts']\n",
    "else:\n",
    "    # Also prevent the time variation column from being denormalized\n",
    "    columns_to_remove = [id_column, ts_column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [],
   "source": [
    "if ml_core == 'deep learning':\n",
    "    denorm_data = du.data_processing.denormalize_data(ALS_df, data=all_features, \n",
    "                                                      id_columns=[id_column, ts_column],\n",
    "                                                      feature_columns=feature_columns,\n",
    "                                                      means=means, stds=stds,\n",
    "                                                      see_progress=False)\n",
    "else:\n",
    "    denorm_data = du.data_processing.denormalize_data(ALS_df, data=all_features[:, 2:], \n",
    "                                                      id_columns=None,\n",
    "                                                      feature_columns=feature_columns,\n",
    "                                                      means=means, stds=stds,\n",
    "                                                      see_progress=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the denormalization (getting the original values back):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pixiedust": {
     "displayParams": {}
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if ml_core == 'deep learning':\n",
    "    print(denorm_data[0, 0])\n",
    "else:\n",
    "    print(denorm_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pixiedust": {
     "displayParams": {
      "handlerId": "tableView"
     }
    }
   },
   "outputs": [],
   "source": [
    "if ml_core == 'deep learning':\n",
    "    orig_ALS_df[(orig_ALS_df.subject_id == int(all_features[0, 0, 0])) & (orig_ALS_df.ts == int(all_features[0, 0, 1]))]\n",
    "else:\n",
    "    orig_ALS_df[(orig_ALS_df.subject_id == 2) & (orig_ALS_df.ts == 27)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if ml_core == 'deep learning':\n",
    "    print(orig_ALS_df[\n",
    "            (orig_ALS_df.subject_id == int(all_features[0, 0, 0])) \n",
    "            & (orig_ALS_df.ts == int(all_features[0, 0, 1]))\n",
    "        ].drop(columns=['niv', 'niv_label']).values == denorm_data[0, 0].numpy())\n",
    "else:\n",
    "    print(orig_ALS_df[\n",
    "            (orig_ALS_df.subject_id == 2) \n",
    "            & (orig_ALS_df.ts == 27)\n",
    "        ].drop(columns=['subject_id', 'ts', 'niv', 'niv_label']).values == denorm_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the interpreter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ml_core == 'deep learning':\n",
    "    # Calculating the number of times to re-evaluate the model when explaining each prediction,\n",
    "    # based on SHAP's formula of nsamples = 2 * n_features + 2048\n",
    "    SHAP_bkgnd_samples = 2 * all_features.shape[-1] + 2048\n",
    "#     SHAP_bkgnd_samples = 2 * test_features.shape[-1]\n",
    "#     SHAP_bkgnd_samples = 200\n",
    "    print(SHAP_bkgnd_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2,
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [],
   "source": [
    "if ml_core == 'deep learning':\n",
    "    interpreter = ModelInterpreter(model, ALS_df, model_type='multivariate_rnn', id_column_name=id_column, \n",
    "                                   inst_column_name=ts_column, label_column_name=label_column, \n",
    "                                   fast_calc=True, SHAP_bkgnd_samples=SHAP_bkgnd_samples, \n",
    "                                   random_seed=du.random_seed, padding_value=padding_value, \n",
    "                                   is_custom=is_custom, total_length=total_length, occlusion_wgt=0.7)\n",
    "elif model_class == 'XGBoost':\n",
    "    interpreter = shap.TreeExplainer(model)\n",
    "interpreter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instance importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate instance importance scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2,
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [],
   "source": [
    "if ml_core == 'deep learning':\n",
    "    inst_scores = interpreter.interpret_model(test_data=all_features,\n",
    "                                              test_labels=all_labels,\n",
    "                                              instance_importance=True, \n",
    "                                              feature_importance=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inst_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the instance importance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter.instance_importance_plot(orig_data=interpreter.test_data, inst_scores=inst_scores,\n",
    "                                     pred_prob=None, uniform_spacing=False,\n",
    "                                     show_pred_prob=False, show_title=True,\n",
    "                                     show_colorbar=True, click_mode='event+select',\n",
    "                                     labels=interpreter.test_labels, seq_len=None, threshold=0,\n",
    "                                     get_fig_obj=False, tensor_idx=True,\n",
    "                                     max_seq=10, background_color='black',\n",
    "                                     font_family='Roboto', font_size=14,\n",
    "                                     font_color='white')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check a patient's outputs to see if the instance importance scores make sense:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects = list(interpreter.test_data[:, :, 0].unique().int().numpy())\n",
    "subjects.remove(padding_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact\n",
    "def get_outputs_of_patient(patient=subjects):\n",
    "    global interpreter, id_column_idx, ts_column_idx\n",
    "    # Find the sequence length of the current patient\n",
    "    seq_len = interpreter.seq_len_dict[patient]\n",
    "    # Get the data corresponding to the current patient\n",
    "    data = interpreter.test_data[interpreter.test_data[:, :, 0] == patient]\n",
    "    # Add a third dimension for the data to be readable by the model\n",
    "    data = data.unsqueeze(0)\n",
    "    # Remove identifier columns from the data\n",
    "    data = du.deep_learning.remove_tensor_column(data, [id_column_idx, ts_column_idx], inplace=True)\n",
    "    # Run the model on the patient's data\n",
    "    new_output = model(data[:, :seq_len, :])\n",
    "    return new_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the feature importance scores (through SHAP values):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2,
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [],
   "source": [
    "if ml_core == 'deep learning':\n",
    "    feat_scores = interpreter.interpret_model(test_data=all_features,\n",
    "                                              test_labels=all_labels,\n",
    "                                              instance_importance=False, \n",
    "                                              feature_importance='shap')\n",
    "elif model_class == 'XGBoost':\n",
    "    feat_scores = interpreter.shap_values(all_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the expected value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ml_core == 'deep learning':\n",
    "    expected_value = interpreter.explainer.expected_value[0]\n",
    "else:\n",
    "    expected_value = interpreter.expected_value\n",
    "print(f'Expected value: {expected_value}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test if the SHAP values match the model output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ml_core == 'deep learning':\n",
    "    feature_columns.remove('subject_id')\n",
    "    feature_columns.remove('ts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter.test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(all_features[:2] == interpreter.test_data).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_labels = interpreter.test_labels.flatten()\n",
    "no_pad_mask = flat_labels != padding_value\n",
    "no_pad_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_0, hidden_state_0 = model(interpreter.test_data[0, 0, 2:].unsqueeze(0).unsqueeze(0), get_hidden_state=True)\n",
    "output_0\n",
    "hidden_state_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_1, hidden_state_1 = model(interpreter.test_data[0, 1, 2:].unsqueeze(0).unsqueeze(0), get_hidden_state=True,\n",
    "                                 hidden_state=hidden_state_0)\n",
    "output_1\n",
    "hidden_state_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_test, _ = model(interpreter.test_data[0, :6, 2:].unsqueeze(0), get_hidden_state=True)\n",
    "output_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_lengths = du.search_explore.find_seq_len(interpreter.test_labels, padding_value)\n",
    "seq_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ref_output = model(interpreter.test_data[:, :, 2:], seq_lengths=seq_lengths).squeeze().detach().numpy()\n",
    "ref_output = ref_output[no_pad_mask]\n",
    "ref_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_output = np.sum(feat_scores, axis=2) + expected_value\n",
    "shap_output = shap_output.flatten()\n",
    "shap_output = shap_output[no_pad_mask]\n",
    "shap_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all(shap_output == ref_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_output_s = pd.Series(ref_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_output_s = pd.Series(shap_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects = list(interpreter.test_data[:, :, 0].unique().int().numpy())\n",
    "subjects.remove(padding_value)\n",
    "subjects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get an overview of the important features and model output for the current patient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALS_df[ALS_df.subject_id.isin(subjects)].reset_index().drop(columns='index').assign(real_output=ref_output_s) \\\n",
    "                                                                            .assign(shap_output=shap_output_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_output_s == shap_output_s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test some interpretation plots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "du.visualization.shap_summary_plot(feat_scores, model_feature_columns, max_display=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ml_core == 'deep learning':\n",
    "    du.visualization.shap_waterfall_plot(expected_value, feat_scores[0, 2, :], \n",
    "                                         all_features[0, 2, 2:], model_feature_columns)\n",
    "else:\n",
    "    du.visualization.shap_waterfall_plot(expected_value, feat_scores[2, :], \n",
    "                                         all_features[2, :], model_feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ml_core == 'deep learning':\n",
    "    du.visualization.shap_waterfall_plot(expected_value, feat_scores[0, 2, :], \n",
    "                                         denorm_data[0, 2, 2:], model_feature_columns)\n",
    "else:\n",
    "    du.visualization.shap_waterfall_plot(expected_value, feat_scores[2, :], \n",
    "                                         denorm_data[2, :], model_feature_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving a dataframe with the resulting SHAP values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "if ml_core == 'deep learning':\n",
    "    data_n_shap_df = interpreter.shap_values_df()\n",
    "else:\n",
    "    # Join the original data and the features' SHAP values\n",
    "    data_n_shap = np.concatenate([all_features, all_labels.reshape(-1, 1), feat_scores], axis=1)\n",
    "    # Reshape into a 2D format\n",
    "    data_n_shap = data_n_shap.reshape(-1, data_n_shap.shape[-1])\n",
    "    # Remove padding samples\n",
    "    data_n_shap = data_n_shap[[padding_value not in row for row in data_n_shap]]\n",
    "    # Define the column names list\n",
    "    shap_column_names = [f'{feature}_shap' for feature in feature_columns]\n",
    "    column_names = ([id_column] + [ts_column] + feature_columns\n",
    "                    + [label_column] + shap_column_names)\n",
    "    # Create the dataframe\n",
    "    data_n_shap_df = pd.DataFrame(data=data_n_shap, columns=column_names)\n",
    "data_n_shap_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_n_shap_file_name = f'fcul_als_with_shap_for_{model_filename.split(\".pth\")[0]}'\n",
    "data_n_shap_file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_n_shap_df.to_csv(f'{data_n_shap_path}{data_n_shap_file_name}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-4.m49",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-4:m49"
  },
  "jupytext": {
   "cell_metadata_filter": "-all",
   "cell_metadata_json": true,
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "ipynb,py:light",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "fcul_als_disease_progression",
   "language": "python",
   "name": "fcul_als_disease_progression"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
